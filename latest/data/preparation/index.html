<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../../img/favicon.ico">
  <title>Preparation - OpenNMT</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../../css/highlight.css">
  <link href="../../css/extra.css" rel="stylesheet">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Preparation";
    var mkdocs_page_input_path = "data/preparation.md";
    var mkdocs_page_url = "/data/preparation/";
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js"></script>
  <script src="../../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../../js/highlight.pack.js"></script> 
  
  <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-89222039-1', 'opennmt.net');
      ga('send', 'pageview');
  </script>
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> OpenNMT</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="../..">Overview</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../installation/">Installation</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../quickstart/">Quickstart</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../applications/">Applications</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Data</span>
    <ul class="subnav">
                <li class=" current">
                    
    <a class="current" href="./">Preparation</a>
    <ul class="subnav">
            
    <li class="toctree-l3"><a href="#data-type">Data type</a></li>
    

    <li class="toctree-l3"><a href="#delimiters">Delimiters</a></li>
    

    <li class="toctree-l3"><a href="#index-files">Index files</a></li>
    

    <li class="toctree-l3"><a href="#input-vectors">Input vectors</a></li>
    

    <li class="toctree-l3"><a href="#vocabularies">Vocabularies</a></li>
    

    <li class="toctree-l3"><a href="#shuffling-and-sorting">Shuffling and sorting</a></li>
    

    <li class="toctree-l3"><a href="#sentence-length">Sentence length</a></li>
    

    </ul>
                </li>
                <li class="">
                    
    <a class="" href="../word_features/">Word features</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Training</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../training/models/">Models</a>
                </li>
                <li class="">
                    
    <a class="" href="../../training/embeddings/">Embeddings</a>
                </li>
                <li class="">
                    
    <a class="" href="../../training/logs/">Logs</a>
                </li>
                <li class="">
                    
    <a class="" href="../../training/multi_gpu/">Multi GPU</a>
                </li>
                <li class="">
                    
    <a class="" href="../../training/retraining/">Retraining</a>
                </li>
                <li class="">
                    
    <a class="" href="../../training/regularization/">Regularization</a>
                </li>
                <li class="">
                    
    <a class="" href="../../training/decay/">Decay strategies</a>
                </li>
                <li class="">
                    
    <a class="" href="../../training/sampling/">Data sampling</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Translation</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../translation/inference/">Inference</a>
                </li>
                <li class="">
                    
    <a class="" href="../../translation/beam_search/">Beam search</a>
                </li>
                <li class="">
                    
    <a class="" href="../../translation/unknowns/">Unknown words</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Tools</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../tools/tokenization/">Tokenization</a>
                </li>
                <li class="">
                    
    <a class="" href="../../tools/scorer/">Scorer</a>
                </li>
                <li class="">
                    
    <a class="" href="../../tools/servers/">Servers</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Reference: Options</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../options/usage/">Scripts usage</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/preprocess/">preprocess.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/train/">train.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/translate/">translate.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/tag/">tag.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/lm/">lm.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/build_vocab/">tools/build_vocab.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/release_model/">tools/release_model.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/tokenize/">tools/tokenize.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/learn_bpe/">tools/learn_bpe.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/server/">tools/translation_server.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/rest_server/">tools/rest_translation_server.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/embeddings/">tools/embeddings.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/average_models/">tools/average_models.lua</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../lua_python_comparison/">Lua and Python OpenNMT</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../references/">References</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../issues/">Common issues</a>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">OpenNMT</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
        
          <li>Data &raquo;</li>
        
      
    
    <li>Preparation</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/OpenNMT/OpenNMT/edit/master/docs/data/preparation.md"> Edit on OpenNMT/OpenNMT</a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <p>The data preparation (or preprocessing) passes over the data to generate word vocabularies and sequences of indices used by the training.</p>
<p>Generally the global process includes several steps:
<em> tokenization (for text files): is splitting the corpus into space-separated tokens, possibly associated to features. See <a href="../../../tools/tokenization/">tokenization tool here</a>.
</em> preprocessing: is building a <code>data file</code> from tokenized source training and validation corpus, optionally shuffling the sentences, and sorting by sentence length.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is possible to perform tokenization and preprocessing dynamically during the training using so-called "Dynamic Datasets" as documented <a href="../../../training/sampling/#dynamic-dataset">here</a></p>
</div>
<h2 id="data-type">Data type<a class="headerlink" href="#data-type" title="Permanent link">&para;</a></h2>
<p>By default, the data type is <code>bitext</code> which are aligned source and target files. Alignment is by default done at the line level, but can also be done through aligned index (see <a href="#index-files">Index files</a>).</p>
<p>For training language models, data type is <code>monotext</code> which is only one language file.</p>
<p>Finally, you can also manipulate the <code>feattext</code> data type (see <a href="#input-vectors">Input vectors</a>) which allows to code sequences of vectors (e.g. sequence of features generated by a device).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Input vectors can only be used for the source.</p>
</div>
<h2 id="delimiters">Delimiters<a class="headerlink" href="#delimiters" title="Permanent link">&para;</a></h2>
<p>Training data (for <code>bitext</code> and <code>monotext</code> data types) are expected to follow the following format:</p>
<ul>
<li>sentences are newline-separated</li>
<li>tokens are space-separated</li>
</ul>
<h2 id="index-files">Index files<a class="headerlink" href="#index-files" title="Permanent link">&para;</a></h2>
<p>Index files are aligning different files by index and not by line. For instance the following files are aligned by index:</p>
<div class="codehilite"><pre><span></span>line1 First line
line2 Second line
</pre></div>


<div class="codehilite"><pre><span></span>line2 Deuxième ligne
line1 Première ligne
</pre></div>


<p>where the first token of each line is an index which must have an equivalent (at any position) in aligned files.</p>
<p>The option <code>-idx_files</code> is used (in <code>preprocess.lua</code> or <code>translate.lua</code>) to enable this feature.</p>
<h2 id="input-vectors">Input vectors<a class="headerlink" href="#input-vectors" title="Permanent link">&para;</a></h2>
<p>OpenNMT supports the use of vector sequence instead of word sequence on the source side.</p>
<p>The data type is <code>feattext</code> and is using the <a href="http://kaldi-asr.org">Kaldi</a> text format (<code>.ark</code> files). For instance the following entry, indexed by <code>KEY</code> is representing a sequence
of <code>m</code> vectors of <code>n</code> values:</p>
<div class="codehilite"><pre><span></span>KEY [
FEAT1.1 FEAT1.2 FEAT1.3 ... FEAT1.n
...
FEATm.1 FEATm.2 FEATm.3 ... FEATm.n ]
</pre></div>


<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Note that you need to use index files for representing input vectors.</p>
</div>
<h2 id="vocabularies">Vocabularies<a class="headerlink" href="#vocabularies" title="Permanent link">&para;</a></h2>
<p>The main goal of the preprocessing is to build the word and features vocabularies and assign each word to an index within these dictionaries.</p>
<p>By default, word vocabularies are limited to 50,000. You can change this value with the <code>-src_vocab_size</code> and <code>-tgt_vocab_size</code>. Alternatively, you can prune the vocabulary size by setting the minimum frequency of words with the <code>-src_words_min_frequency</code> and <code>-tgt_words_min_frequency</code> options.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When pruning vocabularies to 50,000, the preprocessing will actually report a vocabulary size of 50,004 because of 4 special tokens that are automatically added.</p>
</div>
<p>The preprocessing script will generate <code>*.dict</code> files containing the vocabularies: source and target token vocabularies are named <code>PREFIX.src.dict</code> and <code>PREFIX.tgt.dict</code>, while features' vocabulary files are named <code>PREFIX.{source,target}_feature_N.dict</code>.</p>
<p>These files are optional for the rest of the workflow. However, it is common to reuse vocabularies across dataset using the <code>-src_vocab</code> and <code>-tgt_vocab</code> options. This is particularly needed when retraining a model on new data: the vocabulary has to be the same.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Vocabularies can be generated beforehand with the <code>tools/build_vocab.lua</code> script.</p>
</div>
<p>Each line of dictionary files is space-separated fields:</p>
<ul>
<li><code>token</code> the vocab entry.</li>
<li><code>ID</code> its index used internally to map tokens to integer as an entry of lookup tables.</li>
<li>(optional) the vocab frequency in the corpus it was extracted form. This field is generated.</li>
<li>other fields are ignored</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>if you provide your own vocabulary - be sure to integrate the 4 special tokens: <code>&lt;blank&gt; &lt;unk&gt; &lt;s&gt; &lt;/s&gt;</code>. A good practice is to keep them at the beginning of the file with the respective index 1, 2, 3, 4</p>
</div>
<h2 id="shuffling-and-sorting">Shuffling and sorting<a class="headerlink" href="#shuffling-and-sorting" title="Permanent link">&para;</a></h2>
<p>By default, OpenNMT both shuffles and sorts the data before the training. This process comes from 2 constraints of batch training:</p>
<ul>
<li><strong>shuffling</strong>: sentences within a batch should come from different parts of the corpus</li>
<li><strong>sorting</strong>: sentences within a batch should have the same source length (i.e. without padding to maximize efficiency)</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>During the training, batches are also randomly selected unless the <code>-curriculum</code> option is used.</p>
</div>
<h2 id="sentence-length">Sentence length<a class="headerlink" href="#sentence-length" title="Permanent link">&para;</a></h2>
<p>During preprocessing, too long sentences (with source longer than <code>-src_seq_length</code> or target longer than <code>-tgt_seq_length</code>) are discarded from the corpus. You can have an idea of the distribution of sentence length in your training corpus by looking at the preprocess log where a table gives percent of sentences with length 1-10, 11-20, 21-30, ..., 90+:</p>
<div class="codehilite"><pre><span></span>[04/14/17 00:40:10 INFO]  * Source Sentence Length (range of 10): [ 7% ; 35% ; 32% ; 16% ; 7% ; 0% ; 0% ; 0% ; 0% ; 0% ]
[04/14/17 00:40:10 INFO]  * Target Sentence Length (range of 10): [ 9% ; 38% ; 30% ; 15% ; 5% ; 0% ; 0% ; 0% ; 0% ; 0% ]
</pre></div>


<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Limiting maximal sentence length is a key parameter to reduce the GPU memory footprint used during training: indeed the memory grows linearly with maximal sentence length.</p>
</div>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../word_features/" class="btn btn-neutral float-right" title="Word features">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../../applications/" class="btn btn-neutral" title="Applications"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>
    
  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../../applications/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../word_features/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script src="../../js/theme.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_HTMLorMML"></script>

</body>
</html>
