{
    "docs": [
        {
            "location": "/", 
            "text": "OpenNMT project\n\n\nOpenNMT project is composed of 3 main repositories:\n\n\n\n\nOpenNMT-Lua\n (a.k.a. OpenNMT): the main project developed with \nLuaTorch\n.\nOptimized and stable code for production and large scale experiments.\n\n\nOpenNMT-py\n: light version of OpenNMT using \nPyTorch\n.\nInitially created by the Facebook AI research team as a sample project for PyTorch, this version is easier to extend and is suited for research purpose but does not include all features.\n\n\nOpenNMT-C\n (a.k.a. CTranslate): C++ inference engine for OpenNMT models.\n\n\n\n\nOpenNMT is a generic deep learning framework mainly specialized in sequence-to-sequence models covering a variety of tasks such as \nmachine translation\n, \nsummarization\n, \nimage to text\n, and \nspeech recognition\n. The framework has also been extended for other non sequence-to-sequence tasks like \nlanguage modelling\n and \nsequence tagging\n.\n\n\nAll these applications are reusing and sometimes extending a collection of easy-to-reuse modules: \nencoders\n, \ndecoders\n, \nembeddings layers\n, \nattention layers\n, and more.\n\n\nThe framework is implemented to be as generic as possible and can be used either via command line applications, client-server, or libraries.\n\n\nThe project is self-contained and ready to use for both research and production.\n\n\nOpenNMT project is an open-source initiative derivated from \nseq2seq-attn\n, initially created by Kim Yoon at HarvardNLP group.\n\n\nAdditional resources\n\n\nYou can find additional help or tutorials in the following resources:\n\n\n\n\nForum\n\n\nGitter channel\n\n\n\n\n\n\nNote\n\n\nIf you find an error in this documentation, please consider \nopening an issue\n or directly submitting a modification by clicking on the edit button at the top of a page.", 
            "title": "Overview"
        }, 
        {
            "location": "/#opennmt-project", 
            "text": "OpenNMT project is composed of 3 main repositories:   OpenNMT-Lua  (a.k.a. OpenNMT): the main project developed with  LuaTorch . Optimized and stable code for production and large scale experiments.  OpenNMT-py : light version of OpenNMT using  PyTorch . Initially created by the Facebook AI research team as a sample project for PyTorch, this version is easier to extend and is suited for research purpose but does not include all features.  OpenNMT-C  (a.k.a. CTranslate): C++ inference engine for OpenNMT models.   OpenNMT is a generic deep learning framework mainly specialized in sequence-to-sequence models covering a variety of tasks such as  machine translation ,  summarization ,  image to text , and  speech recognition . The framework has also been extended for other non sequence-to-sequence tasks like  language modelling  and  sequence tagging .  All these applications are reusing and sometimes extending a collection of easy-to-reuse modules:  encoders ,  decoders ,  embeddings layers ,  attention layers , and more.  The framework is implemented to be as generic as possible and can be used either via command line applications, client-server, or libraries.  The project is self-contained and ready to use for both research and production.  OpenNMT project is an open-source initiative derivated from  seq2seq-attn , initially created by Kim Yoon at HarvardNLP group.", 
            "title": "OpenNMT project"
        }, 
        {
            "location": "/#additional-resources", 
            "text": "You can find additional help or tutorials in the following resources:   Forum  Gitter channel    Note  If you find an error in this documentation, please consider  opening an issue  or directly submitting a modification by clicking on the edit button at the top of a page.", 
            "title": "Additional resources"
        }, 
        {
            "location": "/installation/", 
            "text": "Standard\n\n\n1. \nInstall Torch\n\n\n2. Install additional dependencies:\n\n\nluarocks install tds\nluarocks install bit32 \n# if using LuaJIT\n\n\n\n\n\n\n3. Clone the OpenNMT repository:\n\n\ngit clone https://github.com/OpenNMT/OpenNMT\n\ncd\n OpenNMT\n\n\n\n\n\nAnd you are ready to go! Take a look at the \nquickstart\n to familiarize yourself with the main training workflow.\n\n\nDocker\n\n\n1. \nInstall \nnvidia-docker\n if using GPUs\n\n\n2. Pull and run the latest OpenNMT image:\n\n\nsudo nvidia-docker run -it opennmt/opennmt:latest\n\n\n\n\n\n3. Clone the OpenNMT repository:\n\n\ngit clone https://github.com/OpenNMT/OpenNMT ~/OpenNMT\n\ncd\n ~/OpenNMT\n\n\n\n\n\nAmazon EC2\n\n\nThe best way to do this is through Docker. We have a public AMI with the preliminary CUDA drivers installed: \nami-c12f86a1\n. Start a P2/G2 GPU instance with this AMI and run the \nnvidia-docker\n command above to get started.", 
            "title": "Installation"
        }, 
        {
            "location": "/installation/#standard", 
            "text": "1.  Install Torch  2. Install additional dependencies:  luarocks install tds\nluarocks install bit32  # if using LuaJIT   3. Clone the OpenNMT repository:  git clone https://github.com/OpenNMT/OpenNMT cd  OpenNMT  And you are ready to go! Take a look at the  quickstart  to familiarize yourself with the main training workflow.", 
            "title": "Standard"
        }, 
        {
            "location": "/installation/#docker", 
            "text": "1.  Install  nvidia-docker  if using GPUs  2. Pull and run the latest OpenNMT image:  sudo nvidia-docker run -it opennmt/opennmt:latest  3. Clone the OpenNMT repository:  git clone https://github.com/OpenNMT/OpenNMT ~/OpenNMT cd  ~/OpenNMT", 
            "title": "Docker"
        }, 
        {
            "location": "/installation/#amazon-ec2", 
            "text": "The best way to do this is through Docker. We have a public AMI with the preliminary CUDA drivers installed:  ami-c12f86a1 . Start a P2/G2 GPU instance with this AMI and run the  nvidia-docker  command above to get started.", 
            "title": "Amazon EC2"
        }, 
        {
            "location": "/quickstart/", 
            "text": "Step 1: Preprocess the data\n\n\nth preprocess.lua -train_src data/src-train.txt -train_tgt data/tgt-train.txt -valid_src data/src-val.txt -valid_tgt data/tgt-val.txt -save_data data/demo\n\n\n\n\n\nWe will be working with some example data in \ndata/\n folder.\n\n\nThe data consists of parallel source (\nsrc\n) and target (\ntgt\n) data containing one sentence per line with tokens separated by a space:\n\n\n\n\nsrc-train.txt\n\n\ntgt-train.txt\n\n\nsrc-val.txt\n\n\ntgt-val.txt\n\n\n\n\nValidation files are required and used to evaluate the convergence of the training. It usually contains no more than 5000 sentences.\n\n\n$ head -n 3 data/src-train.txt\nIt is not acceptable that , with the help of the national bureaucracies , Parliament \napos;s legislative prerogative should be made null and void by means of implementing provisions whose content , purpose and extent are not laid down in advance .\nFederal Master Trainer and Senior Instructor of the Italian Federation of Aerobic Fitness , Group Fitness , Postural Gym , Stretching and Pilates; from 2004 , he has been collaborating with Antiche Terme as personal Trainer and Instructor of Stretching , Pilates and Postural Gym .\n\nquot; Two soldiers came up to me and told me that if I refuse to sleep with them , they will kill me . They beat me and ripped my clothes .\n\n\n\n\n\nAfter running the preprocessing, the following files are generated:\n\n\n\n\ndemo.src.dict\n: Dictionary of source vocab to index mappings.\n\n\ndemo.tgt.dict\n: Dictionary of target vocab to index mappings.\n\n\ndemo-train.t7\n: serialized Torch file containing vocabulary, training and validation data\n\n\n\n\nThe \n*.dict\n files are needed to check or reuse the vocabularies. These files are simple human-readable dictionaries.\n\n\n$ head -n 10 data/demo.src.dict\n\nblank\n 1\n\nunk\n 2\n\ns\n 3\n\n/s\n 4\nIt 5\nis 6\nnot 7\nacceptable 8\nthat 9\n, 10\nwith 11\n\n\n\n\n\nInternally the system never touches the words themselves, but uses these indices.\n\n\n\n\nNote\n\n\nIf the corpus is not tokenized, you can use \nOpenNMT's tokenizer\n.\n\n\n\n\nStep 2: Train the model\n\n\nth train.lua -data data/demo-train.t7 -save_model demo-model\n\n\n\n\n\nThe main train command is quite simple. Minimally it takes a data file\nand a save file.  This will run the default model, which consists of a\n2-layer LSTM with 500 hidden units on both the encoder/decoder. You\ncan also add \n-gpuid 1\n to use (say) GPU 1.\n\n\nStep 3: Translate\n\n\nth translate.lua -model demo-model_epochX_PPL.t7 -src data/src-test.txt -output pred.txt\n\n\n\n\n\nNow you have a model which you can use to predict on new data. We do this by running beam search. This will output predictions into \npred.txt\n.\n\n\n\n\nNote\n\n\nThe predictions are going to be quite terrible, as the demo dataset is small. Try running on some larger datasets! For example you can download millions of parallel sentences for \ntranslation\n or \nsummarization\n.", 
            "title": "Quickstart"
        }, 
        {
            "location": "/quickstart/#step-1-preprocess-the-data", 
            "text": "th preprocess.lua -train_src data/src-train.txt -train_tgt data/tgt-train.txt -valid_src data/src-val.txt -valid_tgt data/tgt-val.txt -save_data data/demo  We will be working with some example data in  data/  folder.  The data consists of parallel source ( src ) and target ( tgt ) data containing one sentence per line with tokens separated by a space:   src-train.txt  tgt-train.txt  src-val.txt  tgt-val.txt   Validation files are required and used to evaluate the convergence of the training. It usually contains no more than 5000 sentences.  $ head -n 3 data/src-train.txt\nIt is not acceptable that , with the help of the national bureaucracies , Parliament  apos;s legislative prerogative should be made null and void by means of implementing provisions whose content , purpose and extent are not laid down in advance .\nFederal Master Trainer and Senior Instructor of the Italian Federation of Aerobic Fitness , Group Fitness , Postural Gym , Stretching and Pilates; from 2004 , he has been collaborating with Antiche Terme as personal Trainer and Instructor of Stretching , Pilates and Postural Gym . quot; Two soldiers came up to me and told me that if I refuse to sleep with them , they will kill me . They beat me and ripped my clothes .  After running the preprocessing, the following files are generated:   demo.src.dict : Dictionary of source vocab to index mappings.  demo.tgt.dict : Dictionary of target vocab to index mappings.  demo-train.t7 : serialized Torch file containing vocabulary, training and validation data   The  *.dict  files are needed to check or reuse the vocabularies. These files are simple human-readable dictionaries.  $ head -n 10 data/demo.src.dict blank  1 unk  2 s  3 /s  4\nIt 5\nis 6\nnot 7\nacceptable 8\nthat 9\n, 10\nwith 11  Internally the system never touches the words themselves, but uses these indices.   Note  If the corpus is not tokenized, you can use  OpenNMT's tokenizer .", 
            "title": "Step 1: Preprocess the data"
        }, 
        {
            "location": "/quickstart/#step-2-train-the-model", 
            "text": "th train.lua -data data/demo-train.t7 -save_model demo-model  The main train command is quite simple. Minimally it takes a data file\nand a save file.  This will run the default model, which consists of a\n2-layer LSTM with 500 hidden units on both the encoder/decoder. You\ncan also add  -gpuid 1  to use (say) GPU 1.", 
            "title": "Step 2: Train the model"
        }, 
        {
            "location": "/quickstart/#step-3-translate", 
            "text": "th translate.lua -model demo-model_epochX_PPL.t7 -src data/src-test.txt -output pred.txt  Now you have a model which you can use to predict on new data. We do this by running beam search. This will output predictions into  pred.txt .   Note  The predictions are going to be quite terrible, as the demo dataset is small. Try running on some larger datasets! For example you can download millions of parallel sentences for  translation  or  summarization .", 
            "title": "Step 3: Translate"
        }, 
        {
            "location": "/applications/", 
            "text": "Machine Translation\n\n\nNeural Machine Translation (NMT) is the default and original task for OpenNMT. It requires a corpus of bilingual sentences for instance as available on \nOpus\n for a very large variety of domains and language pairs.\n\n\nTraining a NMT engine is a 3 steps process:\n\n\n\n\nTokenization\n\n\nPreprocessing\n\n\nTraining\n\n\n\n\nStep by step process is described on the \nquickstart page\n and full process to train large system is described on the forum \nhere\n.\n\n\n\n\nSummarization\n\n\nSummarization models are trained exactly like NMT models. However, the nature of the training data is different: source corpus are full length document or articles, and target are summaries.\n\n\nThis forum post\n details how to train and evaluate a summarization model.\n\n\nImage to text\n\n\nIm2Text\n, developed by Yuntian Deng from the Harvard NLP group, is implementing a generic image-to-text application on top of OpenNMT libraries for \nvisual markup decompilation\n. The main modification to the vanilla OpenNMT is an encoder introducing CNN layers in combination with RNN.\n\n\nSpeech recognition\n\n\nWhile OpenNMT is not primarily targetting speech recognition applications, its ability to support \ninput vectors\n and \npyramidal RNN\n makes possible end-to-end experiments on speech to text applications as described for instance in \nListen, Attend and Spell\n.\n\n\nSee for instance \nmonophone speech recognition with OpenNMT\n on the forum.\n\n\n\n\nSequence tagging\n\n\nA sequence tagger is available in OpenNMT. It shares the same encoder architecture as a sequence-to-sequence model but does not need a decoder since each input is synced with an output. A sequence tagger just needs an encoder and a generation layer. Sequence tagging can be used for any annotation tasks such as part of speech tagging.\n\n\n\n\nTo train a sequence tagger:\n\n\n\n\npreprocess the parallel data with source and target sequence having the same length (you can use the \n-check_plength\n option).\n\n\ntrain the model with \n-model_type seqtagger\n\n\nuse the model with \ntag.lua\n\n\n\n\nLanguage modelling\n\n\nA language model is very similar to a sequence tagger. The main difference is that the output \"tag\" for each token is the following word in source sentence.\n\n\n\n\n\n\npreprocess the data with \n-data_type monotext\n\n\ntrain the model with \n-model_type lm\n\n\nuse the model with \nlm.lua", 
            "title": "Applications"
        }, 
        {
            "location": "/applications/#machine-translation", 
            "text": "Neural Machine Translation (NMT) is the default and original task for OpenNMT. It requires a corpus of bilingual sentences for instance as available on  Opus  for a very large variety of domains and language pairs.  Training a NMT engine is a 3 steps process:   Tokenization  Preprocessing  Training   Step by step process is described on the  quickstart page  and full process to train large system is described on the forum  here .", 
            "title": "Machine Translation"
        }, 
        {
            "location": "/applications/#summarization", 
            "text": "Summarization models are trained exactly like NMT models. However, the nature of the training data is different: source corpus are full length document or articles, and target are summaries.  This forum post  details how to train and evaluate a summarization model.", 
            "title": "Summarization"
        }, 
        {
            "location": "/applications/#image-to-text", 
            "text": "Im2Text , developed by Yuntian Deng from the Harvard NLP group, is implementing a generic image-to-text application on top of OpenNMT libraries for  visual markup decompilation . The main modification to the vanilla OpenNMT is an encoder introducing CNN layers in combination with RNN.", 
            "title": "Image to text"
        }, 
        {
            "location": "/applications/#speech-recognition", 
            "text": "While OpenNMT is not primarily targetting speech recognition applications, its ability to support  input vectors  and  pyramidal RNN  makes possible end-to-end experiments on speech to text applications as described for instance in  Listen, Attend and Spell .  See for instance  monophone speech recognition with OpenNMT  on the forum.", 
            "title": "Speech recognition"
        }, 
        {
            "location": "/applications/#sequence-tagging", 
            "text": "A sequence tagger is available in OpenNMT. It shares the same encoder architecture as a sequence-to-sequence model but does not need a decoder since each input is synced with an output. A sequence tagger just needs an encoder and a generation layer. Sequence tagging can be used for any annotation tasks such as part of speech tagging.   To train a sequence tagger:   preprocess the parallel data with source and target sequence having the same length (you can use the  -check_plength  option).  train the model with  -model_type seqtagger  use the model with  tag.lua", 
            "title": "Sequence tagging"
        }, 
        {
            "location": "/applications/#language-modelling", 
            "text": "A language model is very similar to a sequence tagger. The main difference is that the output \"tag\" for each token is the following word in source sentence.    preprocess the data with  -data_type monotext  train the model with  -model_type lm  use the model with  lm.lua", 
            "title": "Language modelling"
        }, 
        {
            "location": "/data/preparation/", 
            "text": "The data preparation (or preprocessing) passes over the data to generate word vocabularies and sequences of indices used by the training.\n\n\nGenerally the global process includes several steps:\n\n tokenization (for text files): is splitting the corpus into space-separated tokens, possibly associated to features. See \ntokenization tool here\n.\n\n preprocessing: is building a \ndata file\n from tokenized source training and validation corpus, optionally shuffling the sentences, and sorting by sentence length.\n\n\n\n\nNote\n\n\nIt is possible to perform tokenization and preprocessing dynamically during the training using so-called \"Dynamic Datasets\" as documented \nhere\n\n\n\n\nData type\n\n\nBy default, the data type is \nbitext\n which are aligned source and target files. Alignment is by default done at the line level, but can also be done through aligned index (see \nIndex files\n).\n\n\nFor training language models, data type is \nmonotext\n which is only one language file.\n\n\nFinally, you can also manipulate the \nfeattext\n data type (see \nInput vectors\n) which allows to code sequences of vectors (e.g. sequence of features generated by a device).\n\n\n\n\nNote\n\n\nInput vectors can only be used for the source.\n\n\n\n\nDelimiters\n\n\nTraining data (for \nbitext\n and \nmonotext\n data types) are expected to follow the following format:\n\n\n\n\nsentences are newline-separated\n\n\ntokens are space-separated\n\n\n\n\nIndex files\n\n\nIndex files are aligning different files by index and not by line. For instance the following files are aligned by index:\n\n\nline1 First line\nline2 Second line\n\n\n\n\n\nline2 Deuxi\u00e8me ligne\nline1 Premi\u00e8re ligne\n\n\n\n\n\nwhere the first token of each line is an index which must have an equivalent (at any position) in aligned files.\n\n\nThe option \n-idx_files\n is used (in \npreprocess.lua\n or \ntranslate.lua\n) to enable this feature.\n\n\nInput vectors\n\n\nOpenNMT supports the use of vector sequence instead of word sequence on the source side.\n\n\nThe data type is \nfeattext\n and is using the \nKaldi\n text format (\n.ark\n files). For instance the following entry, indexed by \nKEY\n is representing a sequence\nof \nm\n vectors of \nn\n values:\n\n\nKEY [\nFEAT1.1 FEAT1.2 FEAT1.3 ... FEAT1.n\n...\nFEATm.1 FEATm.2 FEATm.3 ... FEATm.n ]\n\n\n\n\n\n\n\nWarning\n\n\nNote that you need to use index files for representing input vectors.\n\n\n\n\nVocabularies\n\n\nThe main goal of the preprocessing is to build the word and features vocabularies and assign each word to an index within these dictionaries.\n\n\nBy default, word vocabularies are limited to 50,000. You can change this value with the \n-src_vocab_size\n and \n-tgt_vocab_size\n. Alternatively, you can prune the vocabulary size by setting the minimum frequency of words with the \n-src_words_min_frequency\n and \n-tgt_words_min_frequency\n options.\n\n\n\n\nNote\n\n\nWhen pruning vocabularies to 50,000, the preprocessing will actually report a vocabulary size of 50,004 because of 4 special tokens that are automatically added.\n\n\n\n\nThe preprocessing script will generate \n*.dict\n files containing the vocabularies: source and target token vocabularies are named \nPREFIX.src.dict\n and \nPREFIX.tgt.dict\n, while features' vocabulary files are named \nPREFIX.{source,target}_feature_N.dict\n.\n\n\nThese files are optional for the rest of the workflow. However, it is common to reuse vocabularies across dataset using the \n-src_vocab\n and \n-tgt_vocab\n options. This is particularly needed when retraining a model on new data: the vocabulary has to be the same.\n\n\n\n\nTip\n\n\nVocabularies can be generated beforehand with the \ntools/build_vocab.lua\n script.\n\n\n\n\nEach line of dictionary files is space-separated fields:\n\n\n\n\ntoken\n the vocab entry.\n\n\nID\n its index used internally to map tokens to integer as an entry of lookup tables.\n\n\n(optional) the vocab frequency in the corpus it was extracted form. This field is generated.\n\n\nother fields are ignored\n\n\n\n\n\n\nNote\n\n\nif you provide your own vocabulary - be sure to integrate the 4 special tokens: \nblank\n \nunk\n \ns\n \n/s\n. A good practice is to keep them at the beginning of the file with the respective index 1, 2, 3, 4\n\n\n\n\nShuffling and sorting\n\n\nBy default, OpenNMT both shuffles and sorts the data before the training. This process comes from 2 constraints of batch training:\n\n\n\n\nshuffling\n: sentences within a batch should come from different parts of the corpus\n\n\nsorting\n: sentences within a batch should have the same source length (i.e. without padding to maximize efficiency)\n\n\n\n\n\n\nNote\n\n\nDuring the training, batches are also randomly selected unless the \n-curriculum\n option is used.\n\n\n\n\nSentence length\n\n\nDuring preprocessing, too long sentences (with source longer than \n-src_seq_length\n or target longer than \n-tgt_seq_length\n) are discarded from the corpus. You can have an idea of the distribution of sentence length in your training corpus by looking at the preprocess log where a table gives percent of sentences with length 1-10, 11-20, 21-30, ..., 90+:\n\n\n[04/14/17 00:40:10 INFO]  * Source Sentence Length (range of 10): [ 7% ; 35% ; 32% ; 16% ; 7% ; 0% ; 0% ; 0% ; 0% ; 0% ]\n[04/14/17 00:40:10 INFO]  * Target Sentence Length (range of 10): [ 9% ; 38% ; 30% ; 15% ; 5% ; 0% ; 0% ; 0% ; 0% ; 0% ]\n\n\n\n\n\n\n\nNote\n\n\nLimiting maximal sentence length is a key parameter to reduce the GPU memory footprint used during training: indeed the memory grows linearly with maximal sentence length.", 
            "title": "Preparation"
        }, 
        {
            "location": "/data/preparation/#data-type", 
            "text": "By default, the data type is  bitext  which are aligned source and target files. Alignment is by default done at the line level, but can also be done through aligned index (see  Index files ).  For training language models, data type is  monotext  which is only one language file.  Finally, you can also manipulate the  feattext  data type (see  Input vectors ) which allows to code sequences of vectors (e.g. sequence of features generated by a device).   Note  Input vectors can only be used for the source.", 
            "title": "Data type"
        }, 
        {
            "location": "/data/preparation/#delimiters", 
            "text": "Training data (for  bitext  and  monotext  data types) are expected to follow the following format:   sentences are newline-separated  tokens are space-separated", 
            "title": "Delimiters"
        }, 
        {
            "location": "/data/preparation/#index-files", 
            "text": "Index files are aligning different files by index and not by line. For instance the following files are aligned by index:  line1 First line\nline2 Second line  line2 Deuxi\u00e8me ligne\nline1 Premi\u00e8re ligne  where the first token of each line is an index which must have an equivalent (at any position) in aligned files.  The option  -idx_files  is used (in  preprocess.lua  or  translate.lua ) to enable this feature.", 
            "title": "Index files"
        }, 
        {
            "location": "/data/preparation/#input-vectors", 
            "text": "OpenNMT supports the use of vector sequence instead of word sequence on the source side.  The data type is  feattext  and is using the  Kaldi  text format ( .ark  files). For instance the following entry, indexed by  KEY  is representing a sequence\nof  m  vectors of  n  values:  KEY [\nFEAT1.1 FEAT1.2 FEAT1.3 ... FEAT1.n\n...\nFEATm.1 FEATm.2 FEATm.3 ... FEATm.n ]   Warning  Note that you need to use index files for representing input vectors.", 
            "title": "Input vectors"
        }, 
        {
            "location": "/data/preparation/#vocabularies", 
            "text": "The main goal of the preprocessing is to build the word and features vocabularies and assign each word to an index within these dictionaries.  By default, word vocabularies are limited to 50,000. You can change this value with the  -src_vocab_size  and  -tgt_vocab_size . Alternatively, you can prune the vocabulary size by setting the minimum frequency of words with the  -src_words_min_frequency  and  -tgt_words_min_frequency  options.   Note  When pruning vocabularies to 50,000, the preprocessing will actually report a vocabulary size of 50,004 because of 4 special tokens that are automatically added.   The preprocessing script will generate  *.dict  files containing the vocabularies: source and target token vocabularies are named  PREFIX.src.dict  and  PREFIX.tgt.dict , while features' vocabulary files are named  PREFIX.{source,target}_feature_N.dict .  These files are optional for the rest of the workflow. However, it is common to reuse vocabularies across dataset using the  -src_vocab  and  -tgt_vocab  options. This is particularly needed when retraining a model on new data: the vocabulary has to be the same.   Tip  Vocabularies can be generated beforehand with the  tools/build_vocab.lua  script.   Each line of dictionary files is space-separated fields:   token  the vocab entry.  ID  its index used internally to map tokens to integer as an entry of lookup tables.  (optional) the vocab frequency in the corpus it was extracted form. This field is generated.  other fields are ignored    Note  if you provide your own vocabulary - be sure to integrate the 4 special tokens:  blank   unk   s   /s . A good practice is to keep them at the beginning of the file with the respective index 1, 2, 3, 4", 
            "title": "Vocabularies"
        }, 
        {
            "location": "/data/preparation/#shuffling-and-sorting", 
            "text": "By default, OpenNMT both shuffles and sorts the data before the training. This process comes from 2 constraints of batch training:   shuffling : sentences within a batch should come from different parts of the corpus  sorting : sentences within a batch should have the same source length (i.e. without padding to maximize efficiency)    Note  During the training, batches are also randomly selected unless the  -curriculum  option is used.", 
            "title": "Shuffling and sorting"
        }, 
        {
            "location": "/data/preparation/#sentence-length", 
            "text": "During preprocessing, too long sentences (with source longer than  -src_seq_length  or target longer than  -tgt_seq_length ) are discarded from the corpus. You can have an idea of the distribution of sentence length in your training corpus by looking at the preprocess log where a table gives percent of sentences with length 1-10, 11-20, 21-30, ..., 90+:  [04/14/17 00:40:10 INFO]  * Source Sentence Length (range of 10): [ 7% ; 35% ; 32% ; 16% ; 7% ; 0% ; 0% ; 0% ; 0% ; 0% ]\n[04/14/17 00:40:10 INFO]  * Target Sentence Length (range of 10): [ 9% ; 38% ; 30% ; 15% ; 5% ; 0% ; 0% ; 0% ; 0% ; 0% ]   Note  Limiting maximal sentence length is a key parameter to reduce the GPU memory footprint used during training: indeed the memory grows linearly with maximal sentence length.", 
            "title": "Sentence length"
        }, 
        {
            "location": "/data/word_features/", 
            "text": "OpenNMT supports additional features on source and target words in the form of \ndiscrete labels\n.\n\n\n\n\nOn the source side, these features act as \nadditional information\n to the encoder. An\nembedding will be optimized for each label and then fed as additional source input\nalongside the word it annotates.\n\n\nOn the target side, these features will be \npredicted\n by the network. The\ndecoder is then able to decode a sentence and annotate each word.\n\n\n\n\nTo use additional features, directly modify your data by appending labels to each word with\nthe special character \n\uffe8\n (unicode character FFE8). There can be an \narbitrary number\n of additional\nfeatures in the form \nword\uffe8feat1\uffe8feat2\uffe8...\uffe8featN\n but each word must have the same number of\nfeatures and in the same order. Source and target data can have a different number of additional features.\n\n\nAs an example, see \ndata/src-train-case.txt\n which uses a separate feature\nto represent the case of each word. Using case as a feature is a way to optimize the word\ndictionary (no duplicated words like \"the\" and \"The\") and gives the system an additional\ninformation that can be useful to optimize its objective function.\n\n\nit\uffe8C is\uffe8l not\uffe8l acceptable\uffe8l that\uffe8l ,\uffe8n with\uffe8l the\uffe8l help\uffe8l of\uffe8l the\uffe8l national\uffe8l bureaucracies\uffe8l ,\uffe8n parliament\uffe8C \napos;s\uffe8l legislative\uffe8l prerogative\uffe8l should\uffe8l be\uffe8l made\uffe8l null\uffe8l and\uffe8l void\uffe8l by\uffe8l means\uffe8l of\uffe8l implementing\uffe8l provisions\uffe8l whose\uffe8l content\uffe8l ,\uffe8n purpose\uffe8l and\uffe8l extent\uffe8l are\uffe8l not\uffe8l laid\uffe8l down\uffe8l in\uffe8l advance\uffe8l .\uffe8n\n\n\n\n\n\nYou can generate this case feature with OpenNMT's tokenization script and the \n-case_feature\n flag.\n\n\nTime-shifting\n\n\nBy default, word features on the target side are automatically shifted compared to the words so that their prediction directly depends on the word they annotate. This way, the decoder architecture is similar to a RNN-based sequence tagger with the output of a timestep being the tag of the input.\n\n\nMore precisely at timestep (t):\n\n\n\n\nthe inputs are (words^{(t)}) and (features^{(t-1)})\n\n\nthe outputs are (words^{(t+1)}) and (features^{(t)})\n\n\n\n\nTo reuse available vocabulary, (features^{(-1)}) is set to the end of sentence token.\n\n\nVocabularies\n\n\nBy default, features vocabulary size is unlimited. Depending on the type of features you are using, you may want to limit their vocabulary during the preprocessing with the \n-src_vocab_size\n and \n-tgt_vocab_size\n options in the format \nword_vocab_size[ feat1_vocab_size[ feat2_vocab_size[ ...]]]\n. For example:\n\n\n# unlimited source features vocabulary size\n\n-src_vocab_size \n50000\n\n\n\n# first feature vocabulary is limited to 60, others are unlimited\n\n-src_vocab_size \n50000\n \n60\n\n\n\n# second feature vocabulary is limited to 100, others are unlimited\n\n-src_vocab_size \n50000\n \n0\n \n100\n\n\n\n# limit vocabulary size of the first and second feature\n\n-src_vocab_size \n50000\n \n60\n \n100\n\n\n\n\n\n\nYou can similarly use \n-src_words_min_frequency\n and \n-tgt_words_min_frequency\n to limit vocabulary by frequency instead of absolute size.\n\n\nLike words, word features vocabularies can be reused across datasets with the \n-features_vocabs_prefix\n. For example, if the processing generates theses features dictionaries:\n\n\n\n\ndata/demo.source_feature_1.dict\n\n\ndata/demo.source_feature_2.dict\n\n\ndata/demo.source_feature_3.dict\n\n\n\n\nyou have to set \n-features_vocabs_prefix data/demo\n as command line option.\n\n\nEmbeddings\n\n\nThe feature embedding size is automatically computed based on the number of values the feature takes. This default size reduction works well for features with few values like the case or POS.\n\n\nFor other features, you may want to manually choose the embedding size with the \n-src_word_vec_size\n and \n-tgt_word_vec_size\n options. They behave similarly to \n-src_vocab_size\n with a list of embedding size: \nword_vec_size[ feat1_vec_size[ feat2_vec_size[ ...]]]\n.\n\n\nThen, each feature embedding is concatenated to each other by default. You can instead choose to sum them by setting \n-feat_merge sum\n. Finally, the resulting merged embedding is concatenated to the word embedding.\n\n\n\n\nWarning\n\n\nIn the \nsum\n case, each feature embedding must have the same dimension. You can set the common embedding size with \n-feat_vec_size\n.\n\n\n\n\nBeam search\n\n\nDuring decoding, the beam search is only applied on the target words space and not on the word features. When the beam path is complete, the associated features are selected along this path.", 
            "title": "Word features"
        }, 
        {
            "location": "/data/word_features/#time-shifting", 
            "text": "By default, word features on the target side are automatically shifted compared to the words so that their prediction directly depends on the word they annotate. This way, the decoder architecture is similar to a RNN-based sequence tagger with the output of a timestep being the tag of the input.  More precisely at timestep (t):   the inputs are (words^{(t)}) and (features^{(t-1)})  the outputs are (words^{(t+1)}) and (features^{(t)})   To reuse available vocabulary, (features^{(-1)}) is set to the end of sentence token.", 
            "title": "Time-shifting"
        }, 
        {
            "location": "/data/word_features/#vocabularies", 
            "text": "By default, features vocabulary size is unlimited. Depending on the type of features you are using, you may want to limit their vocabulary during the preprocessing with the  -src_vocab_size  and  -tgt_vocab_size  options in the format  word_vocab_size[ feat1_vocab_size[ feat2_vocab_size[ ...]]] . For example:  # unlimited source features vocabulary size \n-src_vocab_size  50000  # first feature vocabulary is limited to 60, others are unlimited \n-src_vocab_size  50000   60  # second feature vocabulary is limited to 100, others are unlimited \n-src_vocab_size  50000   0   100  # limit vocabulary size of the first and second feature \n-src_vocab_size  50000   60   100   You can similarly use  -src_words_min_frequency  and  -tgt_words_min_frequency  to limit vocabulary by frequency instead of absolute size.  Like words, word features vocabularies can be reused across datasets with the  -features_vocabs_prefix . For example, if the processing generates theses features dictionaries:   data/demo.source_feature_1.dict  data/demo.source_feature_2.dict  data/demo.source_feature_3.dict   you have to set  -features_vocabs_prefix data/demo  as command line option.", 
            "title": "Vocabularies"
        }, 
        {
            "location": "/data/word_features/#embeddings", 
            "text": "The feature embedding size is automatically computed based on the number of values the feature takes. This default size reduction works well for features with few values like the case or POS.  For other features, you may want to manually choose the embedding size with the  -src_word_vec_size  and  -tgt_word_vec_size  options. They behave similarly to  -src_vocab_size  with a list of embedding size:  word_vec_size[ feat1_vec_size[ feat2_vec_size[ ...]]] .  Then, each feature embedding is concatenated to each other by default. You can instead choose to sum them by setting  -feat_merge sum . Finally, the resulting merged embedding is concatenated to the word embedding.   Warning  In the  sum  case, each feature embedding must have the same dimension. You can set the common embedding size with  -feat_vec_size .", 
            "title": "Embeddings"
        }, 
        {
            "location": "/data/word_features/#beam-search", 
            "text": "During decoding, the beam search is only applied on the target words space and not on the word features. When the beam path is complete, the associated features are selected along this path.", 
            "title": "Beam search"
        }, 
        {
            "location": "/training/models/", 
            "text": "In addition to standard dimension settings like the number of layers, the hidden dimension size, etc., OpenNMT also provides various model architecture.\n\n\nEncoders\n\n\nDefault encoder\n\n\nThe default encoder is a simple recurrent neural network (LSTM or GRU).\n\n\nBidirectional encoder\n\n\nThe bidirectional encoder (\n-encoder_type brnn\n) consists of two independent encoders: one encoding the normal sequence and the other the reversed sequence. The output and final states are concatenated or summed depending on the \n-brnn_merge\n option.\n\n\n\n\nPyramidal deep bidirectional encoder\n\n\nThe pyramidal deep bidirectional encoder (\n-encoder_type pdbrnn\n) is an alternative bidirectional encoder that reduces the time dimension after \neach\n layer based on the \n-pdbrnn_reduction\n factor and using \n-pdbrnn_merge\n as the reduction action (sum or concatenation).\n\n\n\n\nDeep bidirectional encoder\n\n\nThe deep bidirectional encoder (\n-encoder_type dbrnn\n) is an alternative bidirectional encoder where the outputs of every layers are summed (or concatenated) prior feeding to the next layer. It is a special case of a pyramidal deep bidirectional encoder without time reduction (i.e. \n-pdbrnn_reduction = 1\n).\n\n\n\n\nGoogle's NMT encoder\n\n\nThe Google encoder (\n-encoder_type gnmt\n) is an encoder with a single bidirectional layer as described in \nWu et al. (2016)\n. The bidirectional states are concatenated and residual connections are enabled by default.\n\n\n\n\nConvolutional encoder\n\n\nThe convolutional encoder (\n-encoder_type cnn\n) is an encoder based on several convolutional layers as described in \nGehring et al. (2017)\n.\n\n\nIn sequence-to-sequence models, it should be used either without a bridge or with a dense bridge (options \n-bridge dense\n, \n-bridge dense_nonlinear\n, or \n-bridge none\n). The default \ncopy\n bridge is not compatible with this encoder.\n\n\nIt is also recommended to set a small learning rate when using SGD (e.g. \n-learning_rate 0.1\n) or use Adam instead (e.g. \n-optim adam -learning_rate 0.0002\n).\n\n\nDecoders\n\n\nDefault decoder\n\n\nThe default decoder applies attention over the source sequence and implements input feeding by default.\n\n\nInput feeding is an approach to feed attentional vectors \"\nas inputs to the next time steps to inform the model about past alignment decisions\n\" (\nLuong et al. (2015)\n). This can be disabled by setting \n-input_feed 0\n.\n\n\n\n\nResidual connections\n\n\nWith residual connections the input of a layer is element-wise added to the output before feeding to the next layer. This approach proved to be useful for the gradient flow with deep RNN stacks (more than 4 layers).\n\n\n\n\nThe following components support residual connections with the \n-residual\n flag:\n\n\n\n\ndefault encoder\n\n\nbidirectional encoder\n\n\ndefault decoder\n\n\n\n\nBridges\n\n\nA bridge is an additional layer between the encoder and the decoder that defines how to pass the encoder states to the decoder. It can be one of the following:\n\n\n\n\n-bridge copy\n (default): the encoder states are copied\n\n\n-bridge dense\n: the encoder states are forwaded through a dense layer\n\n\n-bridge dense_nonlinear\n: the encoder states are forwaded through a dense layer followed by a non-linearity, here (tanh)\n\n\n-bridge none\n: the encoder states are not passed and the decoder initial states are set to zero\n\n\n\n\nWith the \ncopy\n bridge, encoder and decoder should have the same structure (number of layers, final hidden size, etc.).\n\n\nAttention Model\n\n\nDifferent models are available from \nLuong (2015)\n \"Global Attention Model\".\n\n\n\n\nwhere:\n\n\n$$a_t(s) = \\frac{\\mathrm{exp}(\\mathrm{score}(h_t,\\bar{h}\ns))}{\\sum\n{s'} \\mathrm{exp}(\\mathrm{score}(h_t,\\bar{h}_s)}$$\n\n\nand the score function is one of these:\n\n\n\n\ndot\n: $$\\mathrm{score}(h_t,\\bar{h}_s)=h_t^T\\bar{h}_s$$\n\n\ngeneral\n: $$\\mathrm{score}(h_t,\\bar{h}_s)=h_t^TW_a\\bar{h}_s$$\n\n\nconcat\n: $$\\mathrm{score}(h_t,\\bar{h}_s)=\\nu_a^T.\\mathrm{tanh}(W_a[h_t;\\bar{h}_s])$$\n\n\n\n\nThe model is selected using \n-global_attention\n option or can be disabled with \n-attention none\n option. The default attention model is \ngeneral\n.\n\n\nScheduled Sampling\n\n\nBy default, the decoder at step (t) is using the reference token for step (t-1) to calculate the new output. For some applications, for instance speech recognition, this is leading to propagation of decoding errors and also a mismatch between training and inference workflow. Scheduled sampling (as introduced in \nScheduled sampling for sequence prediction with recurrent neural networks\n. In Advances in Neural Information Processing Systems (pp. 1171-1179).]) is allowing to mix gold reference and generated output in a proportion that will change over time: at the beginning of the training the generated output confidence is very low and it is better to use gold reference, while at the end of the training, we have to rely on generated output to have training in the same conditions than inference.\n\n\nScheduled sampling is activated with \n-scheduled_sampling\n parameter which takes the probability to use the reference token instead of the previously generated one. By default, probability is one meaning that we always use previously generated token. The sampling can be done either at each token level or at the sentence level using \n-scheduled_sampling_scope\n parameter. The decay of the sampling rate is controlled by parameters \n-scheduled_sampling_decay_type\n and \n-scheduled_sampling_decay_rate\n.", 
            "title": "Models"
        }, 
        {
            "location": "/training/models/#encoders", 
            "text": "", 
            "title": "Encoders"
        }, 
        {
            "location": "/training/models/#default-encoder", 
            "text": "The default encoder is a simple recurrent neural network (LSTM or GRU).", 
            "title": "Default encoder"
        }, 
        {
            "location": "/training/models/#bidirectional-encoder", 
            "text": "The bidirectional encoder ( -encoder_type brnn ) consists of two independent encoders: one encoding the normal sequence and the other the reversed sequence. The output and final states are concatenated or summed depending on the  -brnn_merge  option.", 
            "title": "Bidirectional encoder"
        }, 
        {
            "location": "/training/models/#pyramidal-deep-bidirectional-encoder", 
            "text": "The pyramidal deep bidirectional encoder ( -encoder_type pdbrnn ) is an alternative bidirectional encoder that reduces the time dimension after  each  layer based on the  -pdbrnn_reduction  factor and using  -pdbrnn_merge  as the reduction action (sum or concatenation).", 
            "title": "Pyramidal deep bidirectional encoder"
        }, 
        {
            "location": "/training/models/#deep-bidirectional-encoder", 
            "text": "The deep bidirectional encoder ( -encoder_type dbrnn ) is an alternative bidirectional encoder where the outputs of every layers are summed (or concatenated) prior feeding to the next layer. It is a special case of a pyramidal deep bidirectional encoder without time reduction (i.e.  -pdbrnn_reduction = 1 ).", 
            "title": "Deep bidirectional encoder"
        }, 
        {
            "location": "/training/models/#googles-nmt-encoder", 
            "text": "The Google encoder ( -encoder_type gnmt ) is an encoder with a single bidirectional layer as described in  Wu et al. (2016) . The bidirectional states are concatenated and residual connections are enabled by default.", 
            "title": "Google's NMT encoder"
        }, 
        {
            "location": "/training/models/#convolutional-encoder", 
            "text": "The convolutional encoder ( -encoder_type cnn ) is an encoder based on several convolutional layers as described in  Gehring et al. (2017) .  In sequence-to-sequence models, it should be used either without a bridge or with a dense bridge (options  -bridge dense ,  -bridge dense_nonlinear , or  -bridge none ). The default  copy  bridge is not compatible with this encoder.  It is also recommended to set a small learning rate when using SGD (e.g.  -learning_rate 0.1 ) or use Adam instead (e.g.  -optim adam -learning_rate 0.0002 ).", 
            "title": "Convolutional encoder"
        }, 
        {
            "location": "/training/models/#decoders", 
            "text": "", 
            "title": "Decoders"
        }, 
        {
            "location": "/training/models/#default-decoder", 
            "text": "The default decoder applies attention over the source sequence and implements input feeding by default.  Input feeding is an approach to feed attentional vectors \" as inputs to the next time steps to inform the model about past alignment decisions \" ( Luong et al. (2015) ). This can be disabled by setting  -input_feed 0 .", 
            "title": "Default decoder"
        }, 
        {
            "location": "/training/models/#residual-connections", 
            "text": "With residual connections the input of a layer is element-wise added to the output before feeding to the next layer. This approach proved to be useful for the gradient flow with deep RNN stacks (more than 4 layers).   The following components support residual connections with the  -residual  flag:   default encoder  bidirectional encoder  default decoder", 
            "title": "Residual connections"
        }, 
        {
            "location": "/training/models/#bridges", 
            "text": "A bridge is an additional layer between the encoder and the decoder that defines how to pass the encoder states to the decoder. It can be one of the following:   -bridge copy  (default): the encoder states are copied  -bridge dense : the encoder states are forwaded through a dense layer  -bridge dense_nonlinear : the encoder states are forwaded through a dense layer followed by a non-linearity, here (tanh)  -bridge none : the encoder states are not passed and the decoder initial states are set to zero   With the  copy  bridge, encoder and decoder should have the same structure (number of layers, final hidden size, etc.).", 
            "title": "Bridges"
        }, 
        {
            "location": "/training/models/#attention-model", 
            "text": "Different models are available from  Luong (2015)  \"Global Attention Model\".   where:  $$a_t(s) = \\frac{\\mathrm{exp}(\\mathrm{score}(h_t,\\bar{h} s))}{\\sum {s'} \\mathrm{exp}(\\mathrm{score}(h_t,\\bar{h}_s)}$$  and the score function is one of these:   dot : $$\\mathrm{score}(h_t,\\bar{h}_s)=h_t^T\\bar{h}_s$$  general : $$\\mathrm{score}(h_t,\\bar{h}_s)=h_t^TW_a\\bar{h}_s$$  concat : $$\\mathrm{score}(h_t,\\bar{h}_s)=\\nu_a^T.\\mathrm{tanh}(W_a[h_t;\\bar{h}_s])$$   The model is selected using  -global_attention  option or can be disabled with  -attention none  option. The default attention model is  general .", 
            "title": "Attention Model"
        }, 
        {
            "location": "/training/models/#scheduled-sampling", 
            "text": "By default, the decoder at step (t) is using the reference token for step (t-1) to calculate the new output. For some applications, for instance speech recognition, this is leading to propagation of decoding errors and also a mismatch between training and inference workflow. Scheduled sampling (as introduced in  Scheduled sampling for sequence prediction with recurrent neural networks . In Advances in Neural Information Processing Systems (pp. 1171-1179).]) is allowing to mix gold reference and generated output in a proportion that will change over time: at the beginning of the training the generated output confidence is very low and it is better to use gold reference, while at the end of the training, we have to rely on generated output to have training in the same conditions than inference.  Scheduled sampling is activated with  -scheduled_sampling  parameter which takes the probability to use the reference token instead of the previously generated one. By default, probability is one meaning that we always use previously generated token. The sampling can be done either at each token level or at the sentence level using  -scheduled_sampling_scope  parameter. The decay of the sampling rate is controlled by parameters  -scheduled_sampling_decay_type  and  -scheduled_sampling_decay_rate .", 
            "title": "Scheduled Sampling"
        }, 
        {
            "location": "/training/embeddings/", 
            "text": "Word embeddings are learned using a lookup table. Each word is assigned to a random vector within this table that is simply updated with the gradients coming from the network.\n\n\nPretrained\n\n\nWhen training with small amounts of data, performance can be improved by starting with pretrained embeddings. The arguments \n-pre_word_vecs_dec\n and \n-pre_word_vecs_enc\n can be used to specify these files.\n\n\nThe pretrained embeddings must be manually constructed Torch serialized tensors that correspond to the source and target dictionary files. For example:\n\n\nlocal\n \nvocab_size\n \n=\n \n50004\n\n\nlocal\n \nembedding_size\n \n=\n \n500\n\n\n\nlocal\n \nembeddings\n \n=\n \ntorch\n.\nTensor\n(\nvocab_size\n,\n \nembedding_size\n):\nuniform\n()\n\n\n\ntorch\n.\nsave\n(\nenc_embeddings.t7\n,\n \nembeddings\n)\n\n\n\n\n\n\nwhere \nembeddings[i]\n is the embedding of the (i)-th word in the vocabulary.\n\n\nTo automate this process, OpenNMT provides a script \ntools/embeddings.lua\n than can download pretrained embeddings from \nPolyglot\n or convert trained embeddings from \nword2vec\n, \nGloVe\n or \nFastText\n with regard to the word vocabularies generated by \npreprocess.lua\n. Supported format are:\n\n\n\n\nword2vec-bin\n (default): binary format generated by word2vec.\n\n\nword2vec-txt\n: textual word2vec format - starts with header line containing number of words and embedding size, and is then followed by one line per embedding: the first token is the word, and following fields are the embeddings values.\n\n\nglove\n: text format - same format than \nword2vec-txt\n but without header line.\n\n\n\n\n\n\nNote\n\n\nThe script requires the \nlua-zlib\n package.\n\n\n\n\nFor example, to generate pretrained English words embeddings:\n\n\nth tools/embeddings.lua -lang en -dict_file data/demo.src.dict -save_data data/demo-src-emb\n\n\n\n\n\n\n\nNote\n\n\nLanguages codes are Polygot's \nWikipedia Language Codes\n.\n\n\n\n\nOr to map pretrained \nword2vec\n vectors to the built vocabulary:\n\n\nth tools/embeddings.lua -embed_type word2vec -embed_file data/GoogleNews-vectors-negative300.bin -dict_file data/demo.src.dict\n\\\n\n                        -save_data data/demo-src-emb\n\n\n\n\n\n\n\nTip\n\n\nIf vocabs as-is are not found in the embeddings file, you can use \n-approximate\n option to also look for uppercase variants and variants without possible joiner marks. You can dump the non found vocabs by setting \n-save_unknown_dict\n parameter.\n\n\n\n\nFixed\n\n\nBy default these embeddings will be updated during training, but they can be held fixed using \n-fix_word_vecs_enc\n and \n-fix_word_vecs_dec\n options. These options can be enabled or disabled during a retraining.\n\n\n\n\nTip\n\n\nWhen using pretrained word embeddings, if you declare a larger \n-word_vec_size\n then the difference is uniformally initalized and you can use \n-fix_word_vecs_enc pretrained\n (or \n-fix_word_vecs_dec pretrained\n) to fix the pretrained part and optimize the remaining part.\n\n\n\n\nExtraction\n\n\nThe \ntools/extract_embeddings.lua\n script can be used to extract the model word embeddings into text files. They can then be easily transformed into another format for visualization or processing.", 
            "title": "Embeddings"
        }, 
        {
            "location": "/training/embeddings/#pretrained", 
            "text": "When training with small amounts of data, performance can be improved by starting with pretrained embeddings. The arguments  -pre_word_vecs_dec  and  -pre_word_vecs_enc  can be used to specify these files.  The pretrained embeddings must be manually constructed Torch serialized tensors that correspond to the source and target dictionary files. For example:  local   vocab_size   =   50004  local   embedding_size   =   500  local   embeddings   =   torch . Tensor ( vocab_size ,   embedding_size ): uniform ()  torch . save ( enc_embeddings.t7 ,   embeddings )   where  embeddings[i]  is the embedding of the (i)-th word in the vocabulary.  To automate this process, OpenNMT provides a script  tools/embeddings.lua  than can download pretrained embeddings from  Polyglot  or convert trained embeddings from  word2vec ,  GloVe  or  FastText  with regard to the word vocabularies generated by  preprocess.lua . Supported format are:   word2vec-bin  (default): binary format generated by word2vec.  word2vec-txt : textual word2vec format - starts with header line containing number of words and embedding size, and is then followed by one line per embedding: the first token is the word, and following fields are the embeddings values.  glove : text format - same format than  word2vec-txt  but without header line.    Note  The script requires the  lua-zlib  package.   For example, to generate pretrained English words embeddings:  th tools/embeddings.lua -lang en -dict_file data/demo.src.dict -save_data data/demo-src-emb   Note  Languages codes are Polygot's  Wikipedia Language Codes .   Or to map pretrained  word2vec  vectors to the built vocabulary:  th tools/embeddings.lua -embed_type word2vec -embed_file data/GoogleNews-vectors-negative300.bin -dict_file data/demo.src.dict \\ \n                        -save_data data/demo-src-emb   Tip  If vocabs as-is are not found in the embeddings file, you can use  -approximate  option to also look for uppercase variants and variants without possible joiner marks. You can dump the non found vocabs by setting  -save_unknown_dict  parameter.", 
            "title": "Pretrained"
        }, 
        {
            "location": "/training/embeddings/#fixed", 
            "text": "By default these embeddings will be updated during training, but they can be held fixed using  -fix_word_vecs_enc  and  -fix_word_vecs_dec  options. These options can be enabled or disabled during a retraining.   Tip  When using pretrained word embeddings, if you declare a larger  -word_vec_size  then the difference is uniformally initalized and you can use  -fix_word_vecs_enc pretrained  (or  -fix_word_vecs_dec pretrained ) to fix the pretrained part and optimize the remaining part.", 
            "title": "Fixed"
        }, 
        {
            "location": "/training/embeddings/#extraction", 
            "text": "The  tools/extract_embeddings.lua  script can be used to extract the model word embeddings into text files. They can then be easily transformed into another format for visualization or processing.", 
            "title": "Extraction"
        }, 
        {
            "location": "/training/logs/", 
            "text": "During the training, some information are displayed every \n-report_every\n iterations. These logs are usually needed to evaluate the training progress, efficiency and convergence.\n\n\n\n\nNote\n\n\nMeasurements are reported as an average since the previous print.\n\n\n\n\nPerplexity\n\n\nA key information is the \ntraining perplexity\n defined by:\n\n\n$$ppl(X,Y)=\\exp(\\frac{-\\sum_{i=1}^{|Y|}\\log P(y_i|y_{i-1},\\dotsc,y_{1},X)}{|Y|})$$\n\n\nwith (X) being the source sequence, (Y) the true target sequence and (y_i) the (i)-th target word. The numerator is the negative log likelihood and the loss function value.\n\n\nYou want the perplexity to go down and be low in which case it means your model fits well the training data.\n\n\nAt the end of an epoch, the logs report by default the \nvalidation perplexity\n with the same formula but applied on the validation data. It shows how well your model fits unseen data. You can select other validation metrics with the \n-validation_metric\n option.\n\n\n\n\nNote\n\n\nDuring evaluation on the validation dataset, dropout is turned off.\n\n\n\n\nLogs management\n\n\nSome advanced options are available to manage your logs like using a file (\n-log_file\n), or disabling them entirely (\n-disable_logs\n). See the options of the script to learn about them.", 
            "title": "Logs"
        }, 
        {
            "location": "/training/logs/#perplexity", 
            "text": "A key information is the  training perplexity  defined by:  $$ppl(X,Y)=\\exp(\\frac{-\\sum_{i=1}^{|Y|}\\log P(y_i|y_{i-1},\\dotsc,y_{1},X)}{|Y|})$$  with (X) being the source sequence, (Y) the true target sequence and (y_i) the (i)-th target word. The numerator is the negative log likelihood and the loss function value.  You want the perplexity to go down and be low in which case it means your model fits well the training data.  At the end of an epoch, the logs report by default the  validation perplexity  with the same formula but applied on the validation data. It shows how well your model fits unseen data. You can select other validation metrics with the  -validation_metric  option.   Note  During evaluation on the validation dataset, dropout is turned off.", 
            "title": "Perplexity"
        }, 
        {
            "location": "/training/logs/#logs-management", 
            "text": "Some advanced options are available to manage your logs like using a file ( -log_file ), or disabling them entirely ( -disable_logs ). See the options of the script to learn about them.", 
            "title": "Logs management"
        }, 
        {
            "location": "/training/multi_gpu/", 
            "text": "OpenNMT can make use of multiple GPU during the training by implementing \ndata parallelism\n. This technique trains batches in parallel on different network replicas. To use data parallelism, assign a list of GPU identifiers to the \n-gpuid\n option. For example:\n\n\nth train.lua -data data/demo-train.t7 -save_model demo -gpuid \n1\n \n2\n \n4\n\n\n\n\n\n\nwill use the first, the second and the fourth GPU of the machine as returned by the CUDA API.\n\n\n\n\nNote\n\n\nnvidia-smi\n enumerates devices based on the driver API which can be in a different order than the CUDA API.\n\n\n\n\nSynchronous\n\n\nIn this default mode, each replica processes in parallel a different batch at each iteration. The gradients from each replica are accumulated, and parameters updated and synchronized.\n\n\n\n\nWarning\n\n\nWhen using (N) GPU(s), the actual batch size is (N \\times) \n-max_batch_size\n.\n\n\n\n\nAsynchronous\n\n\n(Also known as asynchronous SGD or downpour SGD.)\n\n\nIn this mode enabled with the \n-async_parallel\n flag, the different replicas are independently\ncalculating their own gradients, updating a master copy of the parameters and getting updated values\nof the parameters. To enable convergence at the beginning of the training, only one replica is working for the first \n-async_parallel_minbatch\n iterations to prepare a better initialization for the asynchronous part.\n\n\n\n\nWarning\n\n\nA GPU core is dedicated to store the master copy of the parameters and is not used for training.\n\n\n\n\n\n\nNote\n\n\nAs training logs and saving require synchronization, consider using higher \n-report_every\n and \n-save_every\n values.", 
            "title": "Multi GPU"
        }, 
        {
            "location": "/training/multi_gpu/#synchronous", 
            "text": "In this default mode, each replica processes in parallel a different batch at each iteration. The gradients from each replica are accumulated, and parameters updated and synchronized.   Warning  When using (N) GPU(s), the actual batch size is (N \\times)  -max_batch_size .", 
            "title": "Synchronous"
        }, 
        {
            "location": "/training/multi_gpu/#asynchronous", 
            "text": "(Also known as asynchronous SGD or downpour SGD.)  In this mode enabled with the  -async_parallel  flag, the different replicas are independently\ncalculating their own gradients, updating a master copy of the parameters and getting updated values\nof the parameters. To enable convergence at the beginning of the training, only one replica is working for the first  -async_parallel_minbatch  iterations to prepare a better initialization for the asynchronous part.   Warning  A GPU core is dedicated to store the master copy of the parameters and is not used for training.    Note  As training logs and saving require synchronization, consider using higher  -report_every  and  -save_every  values.", 
            "title": "Asynchronous"
        }, 
        {
            "location": "/training/retraining/", 
            "text": "By default, OpenNMT saves a checkpoint every 5000 iterations and at the end of each epoch. For more frequent or infrequent saves, you can use the \n-save_every\n and \n-save_every_epochs\n options which define the number of iterations and epochs after which the training saves a checkpoint.\n\n\nThere are several reasons one may want to train from a saved model with the \n-train_from\n option:\n\n\n\n\ncontinuing a stopped training\n\n\ncontinuing the training with a smaller batch size\n\n\ntraining a model on new data (incremental adaptation)\n\n\nstarting a training from pre-trained parameters\n\n\netc.\n\n\n\n\nConsiderations\n\n\nWhen training from an existing model, some settings can not be changed:\n\n\n\n\nthe model topology (layers, hidden size, etc.)\n\n\nthe vocabularies\n\n\n\n\n\n\nExceptions\n\n\n-dropout\n, \n-fix_word_vecs_enc\n and \n-fix_word_vecs_dec\n are model options that can be changed for a retraining.\n\n\n\n\nResuming a stopped training\n\n\nIt is common that a training stops: crash, server reboot, user action, etc. In this case, you may want to continue the training for more epochs by using using the \n-continue\n flag. For example:\n\n\n# start the initial training\n\nth train.lua -gpuid \n1\n -data data/demo-train.t7 -save_model demo -save_every \n50\n\n\n\n# train for several epochs...\n\n\n\n# need to reboot the server!\n\n\n\n# continue the training from the last checkpoint\n\nth train.lua -gpuid \n1\n -data data/demo-train.t7 -save_model demo -save_every \n50\n -train_from demo_checkpoint.t7 -continue\n\n\n\n\n\nThe \n-continue\n flag ensures that the training continues with the same configuration and optimization states. In particular, the following options are set to their last known value:\n\n\n\n\n-curriculum\n\n\n-decay\n\n\n-learning_rate_decay\n\n\n-learning_rate\n\n\n-max_grad_norm\n\n\n-min_learning_rate\n\n\n-optim\n\n\n-start_decay_at\n\n\n-start_decay_ppl_delta\n\n\n-start_epoch\n\n\n-start_iteration\n\n\n\n\n\n\nNote\n\n\nThe \n-end_epoch\n value is not automatically set as the user may want to continue its training for more epochs past the end.\n\n\n\n\nAdditionally, the \n-continue\n flag retrieves from the previous training:\n\n\n\n\nthe non-SGD optimizers states\n\n\nthe random generator states\n\n\nthe batch order (when continuing from an intermediate checkpoint)\n\n\n\n\nTraining from pre-trained parameters\n\n\nAnother use case it to use a base model and train it further with new training options (in particular the optimization method and the learning rate). Using \n-train_from\n without \n-continue\n will start a new training with parameters initialized from a pre-trained model.", 
            "title": "Retraining"
        }, 
        {
            "location": "/training/retraining/#considerations", 
            "text": "When training from an existing model, some settings can not be changed:   the model topology (layers, hidden size, etc.)  the vocabularies    Exceptions  -dropout ,  -fix_word_vecs_enc  and  -fix_word_vecs_dec  are model options that can be changed for a retraining.", 
            "title": "Considerations"
        }, 
        {
            "location": "/training/retraining/#resuming-a-stopped-training", 
            "text": "It is common that a training stops: crash, server reboot, user action, etc. In this case, you may want to continue the training for more epochs by using using the  -continue  flag. For example:  # start the initial training \nth train.lua -gpuid  1  -data data/demo-train.t7 -save_model demo -save_every  50  # train for several epochs...  # need to reboot the server!  # continue the training from the last checkpoint \nth train.lua -gpuid  1  -data data/demo-train.t7 -save_model demo -save_every  50  -train_from demo_checkpoint.t7 -continue  The  -continue  flag ensures that the training continues with the same configuration and optimization states. In particular, the following options are set to their last known value:   -curriculum  -decay  -learning_rate_decay  -learning_rate  -max_grad_norm  -min_learning_rate  -optim  -start_decay_at  -start_decay_ppl_delta  -start_epoch  -start_iteration    Note  The  -end_epoch  value is not automatically set as the user may want to continue its training for more epochs past the end.   Additionally, the  -continue  flag retrieves from the previous training:   the non-SGD optimizers states  the random generator states  the batch order (when continuing from an intermediate checkpoint)", 
            "title": "Resuming a stopped training"
        }, 
        {
            "location": "/training/retraining/#training-from-pre-trained-parameters", 
            "text": "Another use case it to use a base model and train it further with new training options (in particular the optimization method and the learning rate). Using  -train_from  without  -continue  will start a new training with parameters initialized from a pre-trained model.", 
            "title": "Training from pre-trained parameters"
        }, 
        {
            "location": "/training/regularization/", 
            "text": "To prevent neural networks from overfitting and increase generalization capacity, several regularization methods are available.\n\n\nDropout\n\n\nDropout is an effective and simple regularization technique introduced by \nSrivastava et al., 2015\n. Dropout is only applied during training. The idea is to disable for a given batch individual neurons with some probability (p). Setting (p) to 0 disables the dropout. Default regularization is dropout with (p=0.2).\n\n\n\n\n\n\nTip\n\n\nDropout value can be changed dynamically when restarting the training. So it can be adjusted all along the training process.\n\n\n\n\nDropout is applied on the output of each layer, the output of the attention layer, and can be enabled also between word embeddings and the first layer with the \n-dropout_input\n option.\n\n\nBecause of recurrence, applying dropout to recurrent neural networks requires some specific care and two implementations are available and can be configured using the \n-dropout_type\n option:\n\n\n\n\nnaive\n (default): implements the approach described in \nZaremba et al., 2015\n. The dropout is only applied on non-recurrent connections.\n\n\nvariational\n: implements the approach described in \nGal et al., 2016\n. In this approach, dropout is also applied to the recurrent connections but each timestep applies the same dropout mask.\n\n\n\n\nThe following picture (from Gal et al. paper) describes both different approaches. On the left side, the naive dropout: no dropout on recurrent connections, and dropout for each timestep is different. On the right side, the variational dropout: there is dropout on recurrent connections, but dropout for each timesteps are the same,\n\n\n\n\nFinally, dropout can also be applied to the words themselves. In that case, a set of individual words randomly selected with probability (p_{words}) are replaced by padding tokens. You can apply this option using \n-dropout_words pwords\n with a non zero value.", 
            "title": "Regularization"
        }, 
        {
            "location": "/training/regularization/#dropout", 
            "text": "Dropout is an effective and simple regularization technique introduced by  Srivastava et al., 2015 . Dropout is only applied during training. The idea is to disable for a given batch individual neurons with some probability (p). Setting (p) to 0 disables the dropout. Default regularization is dropout with (p=0.2).    Tip  Dropout value can be changed dynamically when restarting the training. So it can be adjusted all along the training process.   Dropout is applied on the output of each layer, the output of the attention layer, and can be enabled also between word embeddings and the first layer with the  -dropout_input  option.  Because of recurrence, applying dropout to recurrent neural networks requires some specific care and two implementations are available and can be configured using the  -dropout_type  option:   naive  (default): implements the approach described in  Zaremba et al., 2015 . The dropout is only applied on non-recurrent connections.  variational : implements the approach described in  Gal et al., 2016 . In this approach, dropout is also applied to the recurrent connections but each timestep applies the same dropout mask.   The following picture (from Gal et al. paper) describes both different approaches. On the left side, the naive dropout: no dropout on recurrent connections, and dropout for each timestep is different. On the right side, the variational dropout: there is dropout on recurrent connections, but dropout for each timesteps are the same,   Finally, dropout can also be applied to the words themselves. In that case, a set of individual words randomly selected with probability (p_{words}) are replaced by padding tokens. You can apply this option using  -dropout_words pwords  with a non zero value.", 
            "title": "Dropout"
        }, 
        {
            "location": "/training/decay/", 
            "text": "OpenNMT's training implements empirical learning rate decay strategies. Experiences showed that using a decay strategy systematically yield better performance.\n\n\nWhen a decay condition is met, the following update rule is applied:\n\n\n$$lr^{(t+1)} = lr^{(t)} \\times decay$$\n\n\nwhere (lr^{(0)}=) \n-learning_rate\n and (decay=) \n-learning_rate_decay\n.\n\n\nLearning rate decay is applied when using SGD or Adam optimizers. For the latter, you can also set the \n-decay_method restart\n option to reset the optimizer state when the decay is applied to simulate \"Adam with restarts\" from \nDenkowski et al. 2017\n.\n\n\nLearning rate updates are always computed at the end of an epoch. If this is a too large unit for your particular use case, consider using \ndata sampling\n.\n\n\nAdditionally, it may be useful to set a minimum learning rate with \n-min_learning_rate\n to stop the training earlier when the learning rate is too small to make a difference.\n\n\nDefault\n\n\nBy default, the decay is applied when one of the following conditions is met:\n\n\n\n\nThe validation score is not improving more than \n-start_decay_score_delta\n.\n\n\nThe current epoch is past \n-start_decay_at\n.\n\n\n\n\nOnce one of the conditions is met, the learning rate is decayed after \neach\n remaining epoch.\n\n\nEpoch-based\n\n\nWith the \n-decay epoch_only\n option, the learning rate is only decayed when the condition is met on the epoch:\n\n\n\n\nThe current epoch is past \n-start_decay_at\n.\n\n\n\n\nScore-based\n\n\nWith the \n-decay score_only\n option, the learning rate is only decayed when the condition is met on the validation score:\n\n\n\n\nThe validation score is not improving more than \n-start_decay_score_delta\n.", 
            "title": "Decay strategies"
        }, 
        {
            "location": "/training/decay/#default", 
            "text": "By default, the decay is applied when one of the following conditions is met:   The validation score is not improving more than  -start_decay_score_delta .  The current epoch is past  -start_decay_at .   Once one of the conditions is met, the learning rate is decayed after  each  remaining epoch.", 
            "title": "Default"
        }, 
        {
            "location": "/training/decay/#epoch-based", 
            "text": "With the  -decay epoch_only  option, the learning rate is only decayed when the condition is met on the epoch:   The current epoch is past  -start_decay_at .", 
            "title": "Epoch-based"
        }, 
        {
            "location": "/training/decay/#score-based", 
            "text": "With the  -decay score_only  option, the learning rate is only decayed when the condition is met on the validation score:   The validation score is not improving more than  -start_decay_score_delta .", 
            "title": "Score-based"
        }, 
        {
            "location": "/training/sampling/", 
            "text": "Data sampling is a technique to select a subset of the training set at each epoch. This could be a way to make the epoch unit smaller or select relevant training sequences at each epoch. This is also necessary for working on very large dataset - where the full data does not need to be loaded in memory for each epoch. There are two implementations for sampling: \nmemory sampling\n and \nfile sampling\n.\n\n\nBoth implementations also support vocabulary sampling (also called \nImportance Sampling\n).\n\n\nImportance Sampling\n\n\nWhen sampling, with the option \n-sample_vocab\n it is also possible to restrict the generated vocabulary to the current sample which gives an approximate of the full softmax as defined here \nJean et al, 2015\n via an \"importance sampling\" approach.\n\n\n\n\nTip\n\n\nImportance sampling is particularly useful when training systems with very large output vocabulary for faster computation.\n\n\n\n\nMemory Sampling\n\n\nMemory Sampling\n is enabled using \n-sample N\n option where (N) is the number of sequences to select at each epoch. There are different methods for selecting these (N) sentences corresponding to the \n-sample_type\n option: \nuniform\n (default), \nperplexity\n or \npartition\n.\n\n\nUniform\n\n\nThe simplest data sampling is to uniformly select a subset of the training data. Using the \n-sample N\n option, the training will randomly choose (N) training sequences at each epoch.\n\n\nA typical use case is to reduce the length of the epochs for more frequent learning rate updates and validation perplexity computation.\n\n\nPerplexity-based\n\n\nThis approach is an attempt to feed relevant training data at each epoch. When using the flag \n-sample_type perplexity\n, the perplexity of each sequence is used to generate a multinomial probability distribution over the training sequences. The higher the perplexity, the more likely the sequence is selected.\n\n\nAlternatively, perplexity-based sampling can be enabled when an average training perplexity is met with the \n-sample_perplexity_init\n option.\n\n\n\n\nWarning\n\n\nThis perplexity-based approach is experimental and effects are to be experimented. This also results in a ~10% slowdown as the perplexity of \neach\n sequence has to be independently computed.\n\n\n\n\nPartition\n\n\nWhen using the flag \n-sample_type partition\n, samples are drawn without random, uniformally and incrementally from the corpus training. Use this mode for making sure all training sequences will be sent the same number of time.\n\n\nFile Sampling\n\n\nFile Sampling\n is enabled using \n-gsample V\n option: (V) is either an integer and in that case it represents the number of sentences to sample from the dataset, or a float values and in that case, it represents a relative size based on the full dataset size (e.g: 0.1 being 10%).\n\n\nFile Sampling can only be used with on-the-fly preprocessing and tokenization as an alternative to sequential tokenization, preprocessing, training - and this is refers as \nDynamic Dataset\n below.\n\n\nIn \nFile Sampling\n, the only available sampling method is uniform meaning that the sentences are selected uniformly in each corpus of the dataset. However, it is possible to modify the distribution of the sampling for the different files using sampling rule file as described below.\n\n\nDynamic Dataset\n\n\nIt is possible to provide raw files directly to training script. For that, instead of using the \n-data D\n option, you have to use preprocessing data selection options (such as \n-train_src\n, \n-train_tgt\n, or \n-training_dir\n option). Note these modes are exclusive. Corpus can be pre-tokenized, or you can provide tokenization options for both source and target (or source only for language models) prefixing all tokenization options with \n-tok_src_\n, \n-tok_tgt_\n or \ntok_\n. For instance - the following commandlines use all the files from \nbaseline\n directory with source \n.en\n suffix and target \n.fr\n suffix. The source is tokenized in aggressive mode, and is using case feature, while the target is tokenized in aggressive mode and limited to 30 words sequences.\n\n\nth train.lua\n  -train_dir baseline\n  -src_suffix .en -tgt_suffix .fr\n  -tok_src_mode aggressive -tok_src_case_feature\n  -tok_tgt_mode aggressive\n  -tgt_seq_length 30\n  -save_model baseline\n\n\n\n\n\nThe available options are \npreprocess.lua\n options documented \nhere\n and \ntokenize.lua\n options documented \nhere\n.\n\n\n\n\nTip\n\n\nIt is simpler (and faster) to use dynamic dataset associated with file sampling when working on corpus with more than 10 million sentences.\n\n\n\n\nSampling distribution rules\n\n\nWhen the set of training files is heterogenous, you can specify the proportion of each file using a distribution rule file specified with \n-gsample_dist FILE\n option.\n\n\nThe rule file is list of rule in each line applied. A rule is a \nLuaPattern SPACE WEIGHT\n. The first rule in the file matching (with \nLuaPattern\n a filename) is applied for the file. \nLuaPattern\n can be a lua regex (see \nhttps://www.lua.org/pil/20.2.html\n) or \n*\n matching everything.\n\n\nFor instance, let us say you have the following files in your \ntrain_dir\n directory:\n\n\ngeneric.src, generic.tgt\nIT1.src, IT1.tgt\nIT2.src, IT2.tgt\nMSDN.src, MSDN.tgt\ncolloquial.src, colloquial.tgt\nnews.src, news.tgt\n\n\n\n\n\nand using the following rules:\n\n\nIT,MSDN 20\ncolloquial 10\ngeneric 65\n* 5\n\n\n\n\n\nThe following rules apply:\n\n\n\n\ngeneric 65\n matches \ngeneric.{src,tgt}\n\n\nIT,MSDN 20\n matches \nIT{1,2}.{src,tgt}\n and \nMSDN.{src,tgt}\n\n\ncolloquial 10\n matches \ncolloquial.{src,tgt}\n\n\n* 5\n maches \nnews.{src,tgt}\n\n\n\n\nThe weights are dynamically normalized to 1. Here we will make sure that 65% of the sample will be composed of sentences from \ngeneric.{src,tgt}\n and only 20% from \nIT{1,2}.{src,tgt}\n and \nMSDN.{src,tgt}\n. To build the sample, the sampling preparation algorithm might oversample some of the corpus if is too small.\n\n\n\n\nWarning\n\n\nIf one file could not be match by a rule, it would be completely excluded.\n\n\n\n\nTo test your distribution rules, it is possible to execute a dry run of the preprocessor:\n\n\nth preprocess.lua -gsample_dist rules.txt -gsample \n100000\n -train_dir data/ -dry_run", 
            "title": "Data sampling"
        }, 
        {
            "location": "/training/sampling/#importance-sampling", 
            "text": "When sampling, with the option  -sample_vocab  it is also possible to restrict the generated vocabulary to the current sample which gives an approximate of the full softmax as defined here  Jean et al, 2015  via an \"importance sampling\" approach.   Tip  Importance sampling is particularly useful when training systems with very large output vocabulary for faster computation.", 
            "title": "Importance Sampling"
        }, 
        {
            "location": "/training/sampling/#memory-sampling", 
            "text": "Memory Sampling  is enabled using  -sample N  option where (N) is the number of sequences to select at each epoch. There are different methods for selecting these (N) sentences corresponding to the  -sample_type  option:  uniform  (default),  perplexity  or  partition .", 
            "title": "Memory Sampling"
        }, 
        {
            "location": "/training/sampling/#uniform", 
            "text": "The simplest data sampling is to uniformly select a subset of the training data. Using the  -sample N  option, the training will randomly choose (N) training sequences at each epoch.  A typical use case is to reduce the length of the epochs for more frequent learning rate updates and validation perplexity computation.", 
            "title": "Uniform"
        }, 
        {
            "location": "/training/sampling/#perplexity-based", 
            "text": "This approach is an attempt to feed relevant training data at each epoch. When using the flag  -sample_type perplexity , the perplexity of each sequence is used to generate a multinomial probability distribution over the training sequences. The higher the perplexity, the more likely the sequence is selected.  Alternatively, perplexity-based sampling can be enabled when an average training perplexity is met with the  -sample_perplexity_init  option.   Warning  This perplexity-based approach is experimental and effects are to be experimented. This also results in a ~10% slowdown as the perplexity of  each  sequence has to be independently computed.", 
            "title": "Perplexity-based"
        }, 
        {
            "location": "/training/sampling/#partition", 
            "text": "When using the flag  -sample_type partition , samples are drawn without random, uniformally and incrementally from the corpus training. Use this mode for making sure all training sequences will be sent the same number of time.", 
            "title": "Partition"
        }, 
        {
            "location": "/training/sampling/#file-sampling", 
            "text": "File Sampling  is enabled using  -gsample V  option: (V) is either an integer and in that case it represents the number of sentences to sample from the dataset, or a float values and in that case, it represents a relative size based on the full dataset size (e.g: 0.1 being 10%).  File Sampling can only be used with on-the-fly preprocessing and tokenization as an alternative to sequential tokenization, preprocessing, training - and this is refers as  Dynamic Dataset  below.  In  File Sampling , the only available sampling method is uniform meaning that the sentences are selected uniformly in each corpus of the dataset. However, it is possible to modify the distribution of the sampling for the different files using sampling rule file as described below.", 
            "title": "File Sampling"
        }, 
        {
            "location": "/training/sampling/#dynamic-dataset", 
            "text": "It is possible to provide raw files directly to training script. For that, instead of using the  -data D  option, you have to use preprocessing data selection options (such as  -train_src ,  -train_tgt , or  -training_dir  option). Note these modes are exclusive. Corpus can be pre-tokenized, or you can provide tokenization options for both source and target (or source only for language models) prefixing all tokenization options with  -tok_src_ ,  -tok_tgt_  or  tok_ . For instance - the following commandlines use all the files from  baseline  directory with source  .en  suffix and target  .fr  suffix. The source is tokenized in aggressive mode, and is using case feature, while the target is tokenized in aggressive mode and limited to 30 words sequences.  th train.lua\n  -train_dir baseline\n  -src_suffix .en -tgt_suffix .fr\n  -tok_src_mode aggressive -tok_src_case_feature\n  -tok_tgt_mode aggressive\n  -tgt_seq_length 30\n  -save_model baseline  The available options are  preprocess.lua  options documented  here  and  tokenize.lua  options documented  here .   Tip  It is simpler (and faster) to use dynamic dataset associated with file sampling when working on corpus with more than 10 million sentences.", 
            "title": "Dynamic Dataset"
        }, 
        {
            "location": "/training/sampling/#sampling-distribution-rules", 
            "text": "When the set of training files is heterogenous, you can specify the proportion of each file using a distribution rule file specified with  -gsample_dist FILE  option.  The rule file is list of rule in each line applied. A rule is a  LuaPattern SPACE WEIGHT . The first rule in the file matching (with  LuaPattern  a filename) is applied for the file.  LuaPattern  can be a lua regex (see  https://www.lua.org/pil/20.2.html ) or  *  matching everything.  For instance, let us say you have the following files in your  train_dir  directory:  generic.src, generic.tgt\nIT1.src, IT1.tgt\nIT2.src, IT2.tgt\nMSDN.src, MSDN.tgt\ncolloquial.src, colloquial.tgt\nnews.src, news.tgt  and using the following rules:  IT,MSDN 20\ncolloquial 10\ngeneric 65\n* 5  The following rules apply:   generic 65  matches  generic.{src,tgt}  IT,MSDN 20  matches  IT{1,2}.{src,tgt}  and  MSDN.{src,tgt}  colloquial 10  matches  colloquial.{src,tgt}  * 5  maches  news.{src,tgt}   The weights are dynamically normalized to 1. Here we will make sure that 65% of the sample will be composed of sentences from  generic.{src,tgt}  and only 20% from  IT{1,2}.{src,tgt}  and  MSDN.{src,tgt} . To build the sample, the sampling preparation algorithm might oversample some of the corpus if is too small.   Warning  If one file could not be match by a rule, it would be completely excluded.   To test your distribution rules, it is possible to execute a dry run of the preprocessor:  th preprocess.lua -gsample_dist rules.txt -gsample  100000  -train_dir data/ -dry_run", 
            "title": "Sampling distribution rules"
        }, 
        {
            "location": "/translation/inference/", 
            "text": "Release models\n\n\nAfter training a model, you may want to release it for inference only by using the \ntools/release_model.lua\n script. A released model takes less space on disk and is compatible with both CPU and GPU translation.\n\n\nth tools/release_model.lua -model model.t7 -gpuid \n1\n\n\n\n\n\n\nBy default, it will create a \nmodel_release.t7\n file. See \nth tools/release_model.lua -h\n for advanced options.\n\n\n\n\nWarning\n\n\nA GPU is required to load non released models and released models can no longer be used for training.\n\n\n\n\nInference engine\n\n\nCTranslate is a C++ implementation of \ntranslate.lua\n for integration in existing products. Take a look at the \nGitHub project\n for more information.", 
            "title": "Inference"
        }, 
        {
            "location": "/translation/inference/#release-models", 
            "text": "After training a model, you may want to release it for inference only by using the  tools/release_model.lua  script. A released model takes less space on disk and is compatible with both CPU and GPU translation.  th tools/release_model.lua -model model.t7 -gpuid  1   By default, it will create a  model_release.t7  file. See  th tools/release_model.lua -h  for advanced options.   Warning  A GPU is required to load non released models and released models can no longer be used for training.", 
            "title": "Release models"
        }, 
        {
            "location": "/translation/inference/#inference-engine", 
            "text": "CTranslate is a C++ implementation of  translate.lua  for integration in existing products. Take a look at the  GitHub project  for more information.", 
            "title": "Inference engine"
        }, 
        {
            "location": "/translation/beam_search/", 
            "text": "By default, translation is done using beam search. The \n-beam_size\n option can be used to trade-off translation time and search accuracy, with \n-beam_size 1\n giving greedy search. The small default beam size is often enough in practice.\n\n\nBeam search can also be used to provide an approximate n-best list of translations by setting \n-n_best\n greater than 1. For analysis, the translation command also takes an oracle/gold \n-tgt\n file and will output a comparison of scores.\n\n\nHypotheses filtering\n\n\nThe beam search provides a built-in filter based on unknown words: \n-max_num_unks\n. Hypotheses with more unknown words than this value are dropped.\n\n\n\n\nNote\n\n\nAs dropped hypotheses temporarily reduce the beam size, the \n-pre_filter_factor\n is a way to increase the number of considered hypotheses before applying filters.\n\n\n\n\nNormalization\n\n\nThe beam search also supports various normalization techniques that are disabled by default and can be used to biased the scores generated by the model:\n\n\n$$s(Y,X)=\\frac{\\log P(Y|X)}{lp(Y)}+cp(X,Y)$$\n\n\nwhere (X) is the source, (Y) is the current target, and the functions as defined below. An additional penalty on end of sentence tokens can also be added to prioritize longer sentences.\n\n\nLength normalization\n\n\nScores are normalized by the following formula as defined in \nWu et al. (2016)\n:\n\n\n$$lp(Y) = \\frac{(5+|Y|)^\\alpha}{(5+1)^\\alpha}$$\n\n\nwhere (|Y|) is the current target length and (\\alpha) is the length normalization coefficient \n-length_norm\n.\n\n\nCoverage normalization\n\n\nScores are penalized by the following formula as defined in \nWu et al. (2016)\n:\n\n\n$$cp(X,Y) = \\beta\\sum_{i=1}^{|X|}\\log(\\min(\\sum_{j=1}^{|Y|}p_{i,j},1.0))$$\n\n\nwhere (p_{i,j}) is the attention probability of the (j)-th target word (y_j) on the (i)-th source word (x_i), (|X|) is the source length, (|Y|) is the current target length and (\\beta) is the coverage normalization coefficient \n-coverage_norm\n.\n\n\nEnd of sentence normalization\n\n\nThe score of the end of sentence token is penalized by the following formula:\n\n\n$$ep(X,Y)=\\gamma\\frac{|X|}{|Y|}$$\n\n\nwhere (|X|) is the source length, (|Y|) is the current target length and (\\gamma) is the end of sentence normalization coefficient \n-eos_norm\n.\n\n\nDecoding with auxiliary language model\n\n\nBeam search can use an additional language model to modify score of each option as defined in \nGulcehre et al. (2015)\n as \"Shallow Fusion\":\n\n\n$$s(Y,X) = s_{TM}(Y,X) + \\beta.s_{LM}(Y)$$\n\n\nWhere (s_{LM}(Y)) is the language model log-probability of the sequence  (Y) and (\\beta) is defined by \n-lm_weight\n parameter.\nTo activate the language model, simply use \n-lm_model lm.t7\n.\n\n\n\n\nWarning\n\n\nThe language model cannot use bidirectional RNN and needs to share the same vocabulary (tokens and features) than the translation model.\n\n\n\n\nOutput attention to a file\n\n\nThe option \n-save_attention FILE\n can be used to save attention state to a file during translation. The format of the file is as following (compatible with NEMATUS):\n\n\nsentence id ||| target words ||| score ||| source words ||| number of source words ||| number of target words\nALIGNMENT FOR T_1\nALIGNMENT FOR T_2\n...\nALIGNMENT FOR T_n\n\n\n\n\n\nWhere \nT_1\n ... \nT_n\n are the target words - each alignement line is space separated probability to source word.\n\n\nVisualizing the beam search\n\n\nTo visualize the beam search exploration, you can use the option \n-save_beam_to beam.json\n. It will save a JSON serialization of the beam search history.\n\n\n\n\nNote\n\n\nThis option requires the \ndkjson\n package.\n\n\n\n\nThis representation can then be visualized dynamically using the \ngenerate_beam_viz.py\n script from the \nOpenNMT/VisTools\n repository:\n\n\ngit clone https://github.com/OpenNMT/VisTools.git\n\ncd\n VisTools\nmkdir out/\npython generate_beam_viz.py -d ~/OpenNMT/beam.json -o out/\nfirefox out/000000.html", 
            "title": "Beam search"
        }, 
        {
            "location": "/translation/beam_search/#hypotheses-filtering", 
            "text": "The beam search provides a built-in filter based on unknown words:  -max_num_unks . Hypotheses with more unknown words than this value are dropped.   Note  As dropped hypotheses temporarily reduce the beam size, the  -pre_filter_factor  is a way to increase the number of considered hypotheses before applying filters.", 
            "title": "Hypotheses filtering"
        }, 
        {
            "location": "/translation/beam_search/#normalization", 
            "text": "The beam search also supports various normalization techniques that are disabled by default and can be used to biased the scores generated by the model:  $$s(Y,X)=\\frac{\\log P(Y|X)}{lp(Y)}+cp(X,Y)$$  where (X) is the source, (Y) is the current target, and the functions as defined below. An additional penalty on end of sentence tokens can also be added to prioritize longer sentences.", 
            "title": "Normalization"
        }, 
        {
            "location": "/translation/beam_search/#length-normalization", 
            "text": "Scores are normalized by the following formula as defined in  Wu et al. (2016) :  $$lp(Y) = \\frac{(5+|Y|)^\\alpha}{(5+1)^\\alpha}$$  where (|Y|) is the current target length and (\\alpha) is the length normalization coefficient  -length_norm .", 
            "title": "Length normalization"
        }, 
        {
            "location": "/translation/beam_search/#coverage-normalization", 
            "text": "Scores are penalized by the following formula as defined in  Wu et al. (2016) :  $$cp(X,Y) = \\beta\\sum_{i=1}^{|X|}\\log(\\min(\\sum_{j=1}^{|Y|}p_{i,j},1.0))$$  where (p_{i,j}) is the attention probability of the (j)-th target word (y_j) on the (i)-th source word (x_i), (|X|) is the source length, (|Y|) is the current target length and (\\beta) is the coverage normalization coefficient  -coverage_norm .", 
            "title": "Coverage normalization"
        }, 
        {
            "location": "/translation/beam_search/#end-of-sentence-normalization", 
            "text": "The score of the end of sentence token is penalized by the following formula:  $$ep(X,Y)=\\gamma\\frac{|X|}{|Y|}$$  where (|X|) is the source length, (|Y|) is the current target length and (\\gamma) is the end of sentence normalization coefficient  -eos_norm .", 
            "title": "End of sentence normalization"
        }, 
        {
            "location": "/translation/beam_search/#decoding-with-auxiliary-language-model", 
            "text": "Beam search can use an additional language model to modify score of each option as defined in  Gulcehre et al. (2015)  as \"Shallow Fusion\":  $$s(Y,X) = s_{TM}(Y,X) + \\beta.s_{LM}(Y)$$  Where (s_{LM}(Y)) is the language model log-probability of the sequence  (Y) and (\\beta) is defined by  -lm_weight  parameter.\nTo activate the language model, simply use  -lm_model lm.t7 .   Warning  The language model cannot use bidirectional RNN and needs to share the same vocabulary (tokens and features) than the translation model.", 
            "title": "Decoding with auxiliary language model"
        }, 
        {
            "location": "/translation/beam_search/#output-attention-to-a-file", 
            "text": "The option  -save_attention FILE  can be used to save attention state to a file during translation. The format of the file is as following (compatible with NEMATUS):  sentence id ||| target words ||| score ||| source words ||| number of source words ||| number of target words\nALIGNMENT FOR T_1\nALIGNMENT FOR T_2\n...\nALIGNMENT FOR T_n  Where  T_1  ...  T_n  are the target words - each alignement line is space separated probability to source word.", 
            "title": "Output attention to a file"
        }, 
        {
            "location": "/translation/beam_search/#visualizing-the-beam-search", 
            "text": "To visualize the beam search exploration, you can use the option  -save_beam_to beam.json . It will save a JSON serialization of the beam search history.   Note  This option requires the  dkjson  package.   This representation can then be visualized dynamically using the  generate_beam_viz.py  script from the  OpenNMT/VisTools  repository:  git clone https://github.com/OpenNMT/VisTools.git cd  VisTools\nmkdir out/\npython generate_beam_viz.py -d ~/OpenNMT/beam.json -o out/\nfirefox out/000000.html", 
            "title": "Visualizing the beam search"
        }, 
        {
            "location": "/translation/unknowns/", 
            "text": "The default translation mode allows the model to produce the \nunk\n symbol when it is not sure of the specific target word.\n\n\nOften times \nunk\n symbols will correspond to proper names that can be directly transposed between languages. The \n-replace_unk\n option will substitute \nunk\n with source words that have the highest attention weight. The \n-replace_unk_tagged\n option will do the same, but wrap the token in a \uff5funk:xxxxx\uff60 tag.\n\n\nPhrase table\n\n\nAlternatively, advanced users may prefer to provide a pre-constructed phrase table from an external aligner (such as \nfast_align\n) using the \n-phrase_table\n option to allow for non-identity replacement.\n\n\nInstead of copying the source token with the highest attention, it will lookup in the phrase table for a possible translation. If a valid replacement is not found only then the source token will be copied.\n\n\nThe phrase table is a file with one translation per line in the format:\n\n\nsource|||target\n\n\n\n\n\nWhere \nsource\n and \ntarget\n are \ncase sensitive\n and \nsingle\n tokens.\n\n\nWorkarounds\n\n\nSeveral techniques exist to minimize the out-of-vocabulary issue:\n\n\n\n\nsub-tokenization like BPE or \"wordpiece\" to simulate \nopen\n vocabularies\n\n\nmixed word/characters model as described in \nWu et al. (2016)", 
            "title": "Unknown words"
        }, 
        {
            "location": "/translation/unknowns/#phrase-table", 
            "text": "Alternatively, advanced users may prefer to provide a pre-constructed phrase table from an external aligner (such as  fast_align ) using the  -phrase_table  option to allow for non-identity replacement.  Instead of copying the source token with the highest attention, it will lookup in the phrase table for a possible translation. If a valid replacement is not found only then the source token will be copied.  The phrase table is a file with one translation per line in the format:  source|||target  Where  source  and  target  are  case sensitive  and  single  tokens.", 
            "title": "Phrase table"
        }, 
        {
            "location": "/translation/unknowns/#workarounds", 
            "text": "Several techniques exist to minimize the out-of-vocabulary issue:   sub-tokenization like BPE or \"wordpiece\" to simulate  open  vocabularies  mixed word/characters model as described in  Wu et al. (2016)", 
            "title": "Workarounds"
        }, 
        {
            "location": "/tools/tokenization/", 
            "text": "OpenNMT provides generic tokenization utilities to quickly process new training data. The goal of the tokenization is to convert raw sentences into sequences of tokens. In that process two main operations are performed in sequence:\n\n\n\n\nnormalization - which applies some uniform transformation on the source sequences to identify and protect some specific sequences (for instance url), normalize characters (for instance all types of quotes, unicode variants) or even to normalize some variants (like dates) into unique representation simpler for the translation process\n\n\nthe tokenization itself - which transform the actual normalized sentence into a sequence of space-separated tokens together with possible features (case).\n\n\n\n\nNormalization\n\n\nNormalization is performed by user commandline tool which has to work in \"pipeline\" mode: sentences from standard input are normalized and produced on the standard output. For instance, the following python script is normalizing unicode representation (using NFC representation), turns French quotes \n\u00ab\u00bb\n into English quotes \n\u201c\u201d\n, and protect \"hashtags\" sequences:\n\n\n#!/usr/bin/python\n\n\n# -*- coding: utf-8 -*-\n\n\n\nimport\n \nunicodedata\n\n\nimport\n \nre\n\n\nimport\n \nsys\n\n\n\nfor\n \nline\n \nin\n \nsys\n.\nstdin\n:\n\n  \nline\n \n=\n \nline\n.\nstrip\n()\n\n  \nline\n \n=\n \nunicodedata\n.\nnormalize\n(\nNFC\n,\n \nline\n.\nencode\n(\nutf8\n))\n\n  \nline\n \n=\n \nline\n.\nreplace\n(\nu\n\u00ab\n,\n \nu\n\u201c\n)\n.\nreplace\n(\nu\n\u00bb\n,\n \nu\n\u201d\n)\n\n  \nline\n \n=\n \nline\n.\nencode\n(\nutf8\n)\n.\nsub\n(\nr\n(^|[^S\\w])#([A-Za-z0-9_]+)\n,\n \n\\\\\n1\uff5f#\n\\\\\n2\uff60\n)\n\n  \nprint\n(\nline\n)\n\n\n\n\n\n\nNormalization script is called as part of tokenization adding the option \n-normalize_cmd \"normalize.py\"\n.\n\n\nTokenization\n\n\nTo tokenize a corpus:\n\n\nth tools/tokenize.lua OPTIONS \n file \n file.tok\n\n\n\n\n\nAvailable tokenization modes are defined \nhere\n.\n\n\nIn order to perform \ndetokenization\n, tokenization can introduce a joiner annotation mark \n\uffed\n.\n\n\nDetokenization\n\n\nIf you activate \n-joiner_annotate\n marker, the tokenization is reversible. Just use:\n\n\nth tools/detokenize.lua OPTIONS \n file.tok \n file.detok\n\n\n\n\n\nSpecial characters\n\n\n\n\n\uffe8\n (U+FFE8) is the feature separator symbol. If such character is used in source text, it is replaced by its non presentation form \n\u2502\n (U+2502).\n\n\n\uffed\n (U+FFED) is the default joiner marker (generated in \n-joiner_annotate marker\n mode). If such character is used in source text, it is replaced by its non presentation form \n\u25a0\n (U+25A0)\n\n\n\uff5f...\uff60\n (U+FF5F, U+FF60) are marking a sequence as protected - it won't be tokenized and its case feature is \nN\n.\n\n\n\n\nMixed casing words\n\n\n-segment_case\n feature enables tokenizer to segment words into subwords with one of 3 casing types (truecase ('House'), uppercase ('HOUSE') or lowercase ('house')), which helps  restore right casing during  detokenization. This feature is especially useful for texts with a signficant number of words with mixed casing ('WiFi' -\n 'Wi' and 'Fi').\n\n\nWiFi --\n wi\uffe8C fi\uffe8C\nTVs --\n tv\uffe8U s\uffe8L\n\n\n\n\n\nAlphabet Segmentation\n\n\nTwo options provide specific tokenization depending on alphabet:\n\n\n\n\n-segment_alphabet_change\n: tokenize a sequence between two letters when their alphabets differ - for instance between a Latin alphabet character and a Han character.\n\n\n-segment_alphabet Alphabet\n: tokenize all words of the indicated alphabet into characters - for instance to split a chinese sentence into characters, use \n-segment_alphabet Han\n:\n\n\n\n\n\u541b\u5b50\u4e4b\u5fc3\u4e0d\u80dc\u5176\u5c0f\uff0c\u800c\u6c14\u91cf\u6db5\u76d6\u4e00\u4e16\u3002 --\n \u541b \u5b50 \u4e4b \u5fc3 \u4e0d \u80dc \u5176 \u5c0f \uff0c \u800c \u6c14 \u91cf \u6db5 \u76d6 \u4e00 \u4e16 \u3002\n\n\n\n\n\nNumber Segmentation\n\n\nThe option \n-segment_number\n tokenizes numbers by digits. This option is interesting for full handling of numeric entities conversion/translation by neural networks.\n\n\n1984 --\n 1 9 8 4\n\n\n\n\n\nBPE\n\n\nOpenNMT's BPE module fully supports the \noriginal BPE\n as default mode:\n\n\ntools/learn_bpe.lua -size \n30000\n -save_bpe codes \n input_tokenized\ntools/tokenize.lua -bpe_model codes \n input_tokenized\n\n\n\n\n\nwith three additional features:\n\n\n1. Accept raw text as input and use OpenNMT's tokenizer for pre-tokenization before BPE training\n\n\ntools/learn_bpe.lua -size \n30000\n -save_bpe codes -tok_mode aggressive -tok_segment_alphabet_change \n[\n OTHER_TOK_OPTIONS \n]\n \n[\n OTHER_BPE_TRAINING_OPTIONS \n]\n \n input_raw\ntools/tokenize.lua -bpe_model codes -mode aggressive -segment_alphabet_change \n[\n SAME_TOK_OPTIONS \n]\n \n[\n OTHER_BPE_INFERENCE_OPTIONS \n]\n \n input_raw\n\n\n\n\n\n\n\nNote\n\n\nAll TOK_OPTIONS for learn_bpe.lua have their equivalent for tokenize.lua without the prefix \ntok_\n\nBPE_INFERENCE_OPTIONS for tokenize.lua are those of Tokenizer options with the prefix \nbpe_\n\n\n\n\n\n\nWarning\n\n\nWhen applying BPE for any data set, the same TOK_OPTIONS should be used for learn_bpe.lua and tokenize.lua\n\n\n\n\n2. Add BPE_TRAINING_OPTION for different modes of handling prefixes and/or suffixes: \n-bpe_mode\n\n\n\n\nsuffix\n: BPE merge operations are learnt to distinguish sub-tokens like \"ent\" in the middle of a word and \"ent\n\\w\n\" at the end of a word. \"\n\\w\n\" is an artificial marker appended to the end of each token input and treated as a single unit before doing statistics on bigrams. This is the default mode which is useful for most of the languages.\n\n\nprefix\n: BPE merge operations are learnt to distinguish sub-tokens like \"ent\" in the middle of a word and \"\nw>ent\" at the beginning of a word. \"\nw>\" is an artificial marker appended to the beginning of each token input and treated as a single unit before doing statistics on bigrams.\n\n\nboth\n: \nsuffix\n + \nprefix\n\n\nnone\n: No artificial marker is appended to input tokens, a sub-token is treated equally whether it is in the middle or at the beginning or at the end of a token.\n\n\n\n\n3. Add BPE_INFERENCE_OPTION for BPE in addition to the case feature: \n-bpe_case_insensitive\n\n\nOpenNMT's tokenization flow first applies BPE then add the case feature for each input token. With the standard BPE, \"Constitution\" and \"constitution\" may result in the different sequences of sub-tokens:\n\n\nConstitution --\n con\uffe8C sti\uffe8l tu\uffe8l tion\uffe8l\nconstitution --\n consti\uffe8l tu\uffe8l tion\uffe8l\n\n\n\n\n\nIf you want a \ncaseless\n split so that you can take the best from using case feature, and you can achieve that with the following command lines:\n\n\n# We don\nt need BPE to care about case\n\ntools/learn_bpe.lua -size \n30000\n -save_bpe codes_lc -tok_case_feature \n[\n OTHER_TOK_OPTIONS \n]\n \n[\n OTHER_BPE_TRAINING_OPTIONS \n]\n \n input_raw\n\n\n# The case information is preserved in the true case input\n\ntools/tokenize.lua -bpe_model codes_lc -bpe_case_insensitive -case_feature \n[\n SAME_TOK_OPTIONS \n]\n \n[\n OTHER_BPE_INFERENCE_OPTIONS \n]\n \n input_raw\n\n\n\n\n\nThe output of the previous example would be:\n\n\nConstitution --\n con\uffe8C sti\uffe8l tu\uffe8l tion\uffe8l\nconstitution --\n con\uffe8l sti\uffe8l tu\uffe8l tion\uffe8l", 
            "title": "Tokenization"
        }, 
        {
            "location": "/tools/tokenization/#normalization", 
            "text": "Normalization is performed by user commandline tool which has to work in \"pipeline\" mode: sentences from standard input are normalized and produced on the standard output. For instance, the following python script is normalizing unicode representation (using NFC representation), turns French quotes  \u00ab\u00bb  into English quotes  \u201c\u201d , and protect \"hashtags\" sequences:  #!/usr/bin/python  # -*- coding: utf-8 -*-  import   unicodedata  import   re  import   sys  for   line   in   sys . stdin : \n   line   =   line . strip () \n   line   =   unicodedata . normalize ( NFC ,   line . encode ( utf8 )) \n   line   =   line . replace ( u \u00ab ,   u \u201c ) . replace ( u \u00bb ,   u \u201d ) \n   line   =   line . encode ( utf8 ) . sub ( r (^|[^S\\w])#([A-Za-z0-9_]+) ,   \\\\ 1\uff5f# \\\\ 2\uff60 ) \n   print ( line )   Normalization script is called as part of tokenization adding the option  -normalize_cmd \"normalize.py\" .", 
            "title": "Normalization"
        }, 
        {
            "location": "/tools/tokenization/#tokenization", 
            "text": "To tokenize a corpus:  th tools/tokenize.lua OPTIONS   file   file.tok  Available tokenization modes are defined  here .  In order to perform  detokenization , tokenization can introduce a joiner annotation mark  \uffed .", 
            "title": "Tokenization"
        }, 
        {
            "location": "/tools/tokenization/#detokenization", 
            "text": "If you activate  -joiner_annotate  marker, the tokenization is reversible. Just use:  th tools/detokenize.lua OPTIONS   file.tok   file.detok", 
            "title": "Detokenization"
        }, 
        {
            "location": "/tools/tokenization/#special-characters", 
            "text": "\uffe8  (U+FFE8) is the feature separator symbol. If such character is used in source text, it is replaced by its non presentation form  \u2502  (U+2502).  \uffed  (U+FFED) is the default joiner marker (generated in  -joiner_annotate marker  mode). If such character is used in source text, it is replaced by its non presentation form  \u25a0  (U+25A0)  \uff5f...\uff60  (U+FF5F, U+FF60) are marking a sequence as protected - it won't be tokenized and its case feature is  N .", 
            "title": "Special characters"
        }, 
        {
            "location": "/tools/tokenization/#mixed-casing-words", 
            "text": "-segment_case  feature enables tokenizer to segment words into subwords with one of 3 casing types (truecase ('House'), uppercase ('HOUSE') or lowercase ('house')), which helps  restore right casing during  detokenization. This feature is especially useful for texts with a signficant number of words with mixed casing ('WiFi' -  'Wi' and 'Fi').  WiFi --  wi\uffe8C fi\uffe8C\nTVs --  tv\uffe8U s\uffe8L", 
            "title": "Mixed casing words"
        }, 
        {
            "location": "/tools/tokenization/#alphabet-segmentation", 
            "text": "Two options provide specific tokenization depending on alphabet:   -segment_alphabet_change : tokenize a sequence between two letters when their alphabets differ - for instance between a Latin alphabet character and a Han character.  -segment_alphabet Alphabet : tokenize all words of the indicated alphabet into characters - for instance to split a chinese sentence into characters, use  -segment_alphabet Han :   \u541b\u5b50\u4e4b\u5fc3\u4e0d\u80dc\u5176\u5c0f\uff0c\u800c\u6c14\u91cf\u6db5\u76d6\u4e00\u4e16\u3002 --  \u541b \u5b50 \u4e4b \u5fc3 \u4e0d \u80dc \u5176 \u5c0f \uff0c \u800c \u6c14 \u91cf \u6db5 \u76d6 \u4e00 \u4e16 \u3002", 
            "title": "Alphabet Segmentation"
        }, 
        {
            "location": "/tools/tokenization/#number-segmentation", 
            "text": "The option  -segment_number  tokenizes numbers by digits. This option is interesting for full handling of numeric entities conversion/translation by neural networks.  1984 --  1 9 8 4", 
            "title": "Number Segmentation"
        }, 
        {
            "location": "/tools/tokenization/#bpe", 
            "text": "OpenNMT's BPE module fully supports the  original BPE  as default mode:  tools/learn_bpe.lua -size  30000  -save_bpe codes   input_tokenized\ntools/tokenize.lua -bpe_model codes   input_tokenized  with three additional features:  1. Accept raw text as input and use OpenNMT's tokenizer for pre-tokenization before BPE training  tools/learn_bpe.lua -size  30000  -save_bpe codes -tok_mode aggressive -tok_segment_alphabet_change  [  OTHER_TOK_OPTIONS  ]   [  OTHER_BPE_TRAINING_OPTIONS  ]    input_raw\ntools/tokenize.lua -bpe_model codes -mode aggressive -segment_alphabet_change  [  SAME_TOK_OPTIONS  ]   [  OTHER_BPE_INFERENCE_OPTIONS  ]    input_raw   Note  All TOK_OPTIONS for learn_bpe.lua have their equivalent for tokenize.lua without the prefix  tok_ \nBPE_INFERENCE_OPTIONS for tokenize.lua are those of Tokenizer options with the prefix  bpe_    Warning  When applying BPE for any data set, the same TOK_OPTIONS should be used for learn_bpe.lua and tokenize.lua   2. Add BPE_TRAINING_OPTION for different modes of handling prefixes and/or suffixes:  -bpe_mode   suffix : BPE merge operations are learnt to distinguish sub-tokens like \"ent\" in the middle of a word and \"ent \\w \" at the end of a word. \" \\w \" is an artificial marker appended to the end of each token input and treated as a single unit before doing statistics on bigrams. This is the default mode which is useful for most of the languages.  prefix : BPE merge operations are learnt to distinguish sub-tokens like \"ent\" in the middle of a word and \" w>ent\" at the beginning of a word. \" w>\" is an artificial marker appended to the beginning of each token input and treated as a single unit before doing statistics on bigrams.  both :  suffix  +  prefix  none : No artificial marker is appended to input tokens, a sub-token is treated equally whether it is in the middle or at the beginning or at the end of a token.   3. Add BPE_INFERENCE_OPTION for BPE in addition to the case feature:  -bpe_case_insensitive  OpenNMT's tokenization flow first applies BPE then add the case feature for each input token. With the standard BPE, \"Constitution\" and \"constitution\" may result in the different sequences of sub-tokens:  Constitution --  con\uffe8C sti\uffe8l tu\uffe8l tion\uffe8l\nconstitution --  consti\uffe8l tu\uffe8l tion\uffe8l  If you want a  caseless  split so that you can take the best from using case feature, and you can achieve that with the following command lines:  # We don t need BPE to care about case \ntools/learn_bpe.lua -size  30000  -save_bpe codes_lc -tok_case_feature  [  OTHER_TOK_OPTIONS  ]   [  OTHER_BPE_TRAINING_OPTIONS  ]    input_raw # The case information is preserved in the true case input \ntools/tokenize.lua -bpe_model codes_lc -bpe_case_insensitive -case_feature  [  SAME_TOK_OPTIONS  ]   [  OTHER_BPE_INFERENCE_OPTIONS  ]    input_raw  The output of the previous example would be:  Constitution --  con\uffe8C sti\uffe8l tu\uffe8l tion\uffe8l\nconstitution --  con\uffe8l sti\uffe8l tu\uffe8l tion\uffe8l", 
            "title": "BPE"
        }, 
        {
            "location": "/tools/scorer/", 
            "text": "OpenNMT provides native implementation of scoring metrics - BLEU, TER, DLRATIO\n\n\nAll metrics can be used as a validation metric (see \noption \n-validation_metric\n) during training or standalone using \ntools/score.lua\n:\n\n\n$ th tools/score.lua REFERENCE \n[\n-sample SN\n]\n \n[\n-scorer bleu\n|\nter\n|\ndlratio\n]\n PARAMS \n OUT\n\n\n\n\n\nThe actual metric is selected with \nscorer\n option and the output is a line with 3 field, tab separated like:\n\n\n34.73        +/-0.83        BLEU = 34.77, 79.8/49.1/29.6/17.6 (BP=0.919, ratio=0.922, hyp_len=26742, ref_len=28995)\n54.77                       TER = 54.77 (Ins 1.8, Del 4.4, Sub 9.6, Shft 1.9, WdSh 2.6)\n\n\n\n\n\nThe fields are:\n\n\n\n\nnumeric value of the score\n\n\n95% confidence error margin (1.96*standard deviation) for k samples of half-size\n\n\nformated scorer output\n\n\n\n\n\n\nTip\n\n\nError margin\n is a simple way to know if score variation is part of metric calculation variation or is significant.\n\n\n\n\nBLEU\n\n\nBLEU\n is a metric widely used for evaluation of machine translation output.\n\n\nSyntax follows \nmulti-bleu.perl\n syntax:\n\n\n$ th tools/score.lua REFERENCE \n[\n-sample SN\n]\n \n[\n-scorer bleu\n]\n \n[\n-order N\n]\n \n OUT\n\n\n\n\n\ngenerating:\n\n\n[06/17/17 09:39:04 INFO] 4 references, 1002 sentences\nBLEU = 34.77 +/- 0.43, 79.8/49.1/29.6/17.6 (BP=0.919, ratio=0.922, hyp_len=26742, ref_len=28995)\n\n\n\n\n\nwhere:\n\n\n\n\nREFERENCE\n is either a single file, or a prefix for multiple-reference \nREFERENCE0\n, \nREFERENCE1\n, ...\n\n\n-order\n is bleu n-gram order (default 4)\n\n\n\n\nTER\n\n\nTER\n is an error metric for machine translation that messures the number of edits required to change a system output into one of the references. It is generally prefered to BLEU for estimation of sentence post-editing effort.\n\n\nDLRATIO\n\n\nDamerau-Levenshtein edit distance\n is edit distance between 2 sentences. It is a simplified version of \nTER\n (in particular, \nTER\n that also integrates numbers of sequence shift).", 
            "title": "Scorer"
        }, 
        {
            "location": "/tools/scorer/#bleu", 
            "text": "BLEU  is a metric widely used for evaluation of machine translation output.  Syntax follows  multi-bleu.perl  syntax:  $ th tools/score.lua REFERENCE  [ -sample SN ]   [ -scorer bleu ]   [ -order N ]    OUT  generating:  [06/17/17 09:39:04 INFO] 4 references, 1002 sentences\nBLEU = 34.77 +/- 0.43, 79.8/49.1/29.6/17.6 (BP=0.919, ratio=0.922, hyp_len=26742, ref_len=28995)  where:   REFERENCE  is either a single file, or a prefix for multiple-reference  REFERENCE0 ,  REFERENCE1 , ...  -order  is bleu n-gram order (default 4)", 
            "title": "BLEU"
        }, 
        {
            "location": "/tools/scorer/#ter", 
            "text": "TER  is an error metric for machine translation that messures the number of edits required to change a system output into one of the references. It is generally prefered to BLEU for estimation of sentence post-editing effort.", 
            "title": "TER"
        }, 
        {
            "location": "/tools/scorer/#dlratio", 
            "text": "Damerau-Levenshtein edit distance  is edit distance between 2 sentences. It is a simplified version of  TER  (in particular,  TER  that also integrates numbers of sequence shift).", 
            "title": "DLRATIO"
        }, 
        {
            "location": "/tools/servers/", 
            "text": "OpenNMT provides simple translation servers to easily showcase your results remotely.\n\n\nREST\n\n\nYou can use an easy REST syntax to simply send plain text. Sentences will be tokenized, translated and then detokenized using OpenNMT tools.\n\n\nThe server uses the \nrestserver-xavante\n dependency, you need to install it by running:\n\n\nluarocks install restserver-xavante\n\n\n\n\n\nThe translation server can be run using any of the arguments from \ntokenize.lua\n or \ntranslate.lua\n.\n\n\nth tools/rest_translation_server.lua -model ../Recipes/baseline-1M-enfr/exp/model-baseline-1M-enfr_epoch13_3.44.t7 -gpuid \n1\n -host ... -port -case_feature -bpe_model ...\n\n\n\n\n\n\n\nNote\n\n\nThe default host is set to \n127.0.0.1\n and default port to \n7784\n.\n\n\n\n\nYou can test it with a \ncurl\n command locally or from any other client:\n\n\ncurl -v -H \nContent-Type: application/json\n -X POST -d \n[{ \nsrc\n : \nHello World\n }]\n http://IP_address:7784/translator/translate\n\n\n\n\n\nAnswer will be embedded in a JSON format, translated sentence in the \ntgt\n section. Additionally you can get the attention matrix with the \n-withAttn\n option in the server command line.\n\n\nZeroMQ\n\n\nThe server uses the 0MQ for RPC. You can install 0MQ and the Lua bindings on Ubuntu by running:\n\n\nsudo apt-get install libzmq-dev\nluarocks install dkjson\nluarocks install lua-zmq \nZEROMQ_LIBDIR\n=\n/usr/lib/x86_64-linux-gnu/ \nZEROMQ_INCDIR\n=\n/usr/include\n\n\n\n\n\nThe translation server can be run using any of the arguments from \ntranslate.lua\n.\n\n\nth tools/translation_server.lua -host ... -port ... -model ...\n\n\n\n\n\n\n\nNote\n\n\nThe default host is set to \n127.0.0.1\n which only allows local access. If you want to support remote access, use \n0.0.0.0\n instead.\n\n\n\n\nIt runs as a message queue that takes in a JSON batch of src sentences. For example the following 5 lines of Python\ncode can be used to send a single sentence for translation.\n\n\nimport\n \nzmq\n,\n \nsys\n,\n \njson\n\n\nsock\n \n=\n \nzmq\n.\nContext\n()\n.\nsocket\n(\nzmq\n.\nREQ\n)\n\n\nsock\n.\nconnect\n(\ntcp://127.0.0.1:5556\n)\n\n\nsock\n.\nsend\n(\njson\n.\ndumps\n([{\nsrc\n:\n \n \n.\njoin\n(\nsys\n.\nargv\n[\n1\n:])}]))\n\n\nprint\n \nsock\n.\nrecv\n()\n\n\n\n\n\n\nFor a longer example, see our \nPython/Flask server\n in development.", 
            "title": "Servers"
        }, 
        {
            "location": "/tools/servers/#rest", 
            "text": "You can use an easy REST syntax to simply send plain text. Sentences will be tokenized, translated and then detokenized using OpenNMT tools.  The server uses the  restserver-xavante  dependency, you need to install it by running:  luarocks install restserver-xavante  The translation server can be run using any of the arguments from  tokenize.lua  or  translate.lua .  th tools/rest_translation_server.lua -model ../Recipes/baseline-1M-enfr/exp/model-baseline-1M-enfr_epoch13_3.44.t7 -gpuid  1  -host ... -port -case_feature -bpe_model ...   Note  The default host is set to  127.0.0.1  and default port to  7784 .   You can test it with a  curl  command locally or from any other client:  curl -v -H  Content-Type: application/json  -X POST -d  [{  src  :  Hello World  }]  http://IP_address:7784/translator/translate  Answer will be embedded in a JSON format, translated sentence in the  tgt  section. Additionally you can get the attention matrix with the  -withAttn  option in the server command line.", 
            "title": "REST"
        }, 
        {
            "location": "/tools/servers/#zeromq", 
            "text": "The server uses the 0MQ for RPC. You can install 0MQ and the Lua bindings on Ubuntu by running:  sudo apt-get install libzmq-dev\nluarocks install dkjson\nluarocks install lua-zmq  ZEROMQ_LIBDIR = /usr/lib/x86_64-linux-gnu/  ZEROMQ_INCDIR = /usr/include  The translation server can be run using any of the arguments from  translate.lua .  th tools/translation_server.lua -host ... -port ... -model ...   Note  The default host is set to  127.0.0.1  which only allows local access. If you want to support remote access, use  0.0.0.0  instead.   It runs as a message queue that takes in a JSON batch of src sentences. For example the following 5 lines of Python\ncode can be used to send a single sentence for translation.  import   zmq ,   sys ,   json  sock   =   zmq . Context () . socket ( zmq . REQ )  sock . connect ( tcp://127.0.0.1:5556 )  sock . send ( json . dumps ([{ src :     . join ( sys . argv [ 1 :])}]))  print   sock . recv ()   For a longer example, see our  Python/Flask server  in development.", 
            "title": "ZeroMQ"
        }, 
        {
            "location": "/options/usage/", 
            "text": "By default, OpenNMT's scripts can only be called from the root of OpenNMT's directory. If calling the scripts from any directory is more convenient to you, you need to extend the \nLUA_PATH\n:\n\n\nexport\n \nLUA_PATH\n=\n$LUA_PATH\n;/path/to/OpenNMT/?.lua\n\n\n\n\n\n\nConfiguration files\n\n\nYou can pass options using a configuration file. The file has a simple key-value syntax with one \noption = value\n per line. Here is an example:\n\n\n$ cat generic.txt\nrnn_size = 600\nlayers = 4\nbrnn = true\nsave_model = generic\n\n\n\n\n\nIt handles empty lines and ignores lines prefixed with \n#\n.\n\n\nYou can then pass this file along other options on the command line:\n\n\nth train.lua -config generic.txt -data data/demo-train.t7 -gpuid \n1\n\n\n\n\n\n\nIf an option appears both in the file and on the command line, the file takes priority.\n\n\nBoolean options\n\n\nBoolean options can be used without argument. In this case, their presence on the command line negates their default value. For example the option \n-brnn\n enables bidirectional encoder when added to the command line.\n\n\nThey optionally accept an argument to make it more practical in scripts:\n\n\n\n\n0\n or \nfalse\n\n\n1\n or \ntrue\n\n\n\n\nMultiple arguments\n\n\nSome options can take multiple arguments (\ntable\n argument type in the option listings). You can either space-separate (\nvalue1 value2 value3\n) or comma-separate (\nvalue1,value2,value3\n) the values.", 
            "title": "Scripts usage"
        }, 
        {
            "location": "/options/usage/#configuration-files", 
            "text": "You can pass options using a configuration file. The file has a simple key-value syntax with one  option = value  per line. Here is an example:  $ cat generic.txt\nrnn_size = 600\nlayers = 4\nbrnn = true\nsave_model = generic  It handles empty lines and ignores lines prefixed with  # .  You can then pass this file along other options on the command line:  th train.lua -config generic.txt -data data/demo-train.t7 -gpuid  1   If an option appears both in the file and on the command line, the file takes priority.", 
            "title": "Configuration files"
        }, 
        {
            "location": "/options/usage/#boolean-options", 
            "text": "Boolean options can be used without argument. In this case, their presence on the command line negates their default value. For example the option  -brnn  enables bidirectional encoder when added to the command line.  They optionally accept an argument to make it more practical in scripts:   0  or  false  1  or  true", 
            "title": "Boolean options"
        }, 
        {
            "location": "/options/usage/#multiple-arguments", 
            "text": "Some options can take multiple arguments ( table  argument type in the option listings). You can either space-separate ( value1 value2 value3 ) or comma-separate ( value1,value2,value3 ) the values.", 
            "title": "Multiple arguments"
        }, 
        {
            "location": "/options/preprocess/", 
            "text": "preprocess.lua\n options:\n\n\n\n\n-h [\nboolean\n]\n (default: \nfalse\n)\nThis help.\n\n\n-md [\nboolean\n]\n (default: \nfalse\n)\nDump help in Markdown format.\n\n\n-config \nstring\n (default: \n''\n)\nLoad options from this file.\n\n\n-save_config \nstring\n (default: \n''\n)\nSave options to this file.\n\n\n\n\nPreprocess options\n\n\n\n\n-data_type \nstring\n (accepted: \nbitext\n, \nmonotext\n, \nfeattext\n; default: \nbitext\n)\nType of data to preprocess. Use 'monotext' for monolingual data. This option impacts all options choices.\n\n\n-dry_run [\nboolean\n]\n (default: \nfalse\n)\nIf set, this will only prepare the preprocessor. Useful when using file sampling to test distribution rules.\n\n\n-save_data \nstring\n (default: \n''\n)\nOutput file for the prepared data.\n\n\n\n\nData options\n\n\n\n\n-train_dir \nstring\n (default: \n''\n)\nPath to training files directory.\n\n\n-train_src \nstring\n (default: \n''\n)\nPath to the training source data.\n\n\n-train_tgt \nstring\n (default: \n''\n)\nPath to the training target data.\n\n\n-valid_src \nstring\n (default: \n''\n)\nPath to the validation source data.\n\n\n-valid_tgt \nstring\n (default: \n''\n)\nPath to the validation target data.\n\n\n-src_vocab \nstring\n (default: \n''\n)\nPath to an existing source vocabulary.\n\n\n-src_suffix \nstring\n (default: \n.src\n)\nSuffix for source files in train/valid directories.\n\n\n-src_vocab_size \ntable\n (default: \n50000\n)\nList of source vocabularies size: \nword[ feat1[ feat2[ ...] ] ]\n. If = 0, vocabularies are not pruned.\n\n\n-src_words_min_frequency \ntable\n (default: \n0\n)\nList of source words min frequency: \nword[ feat1[ feat2[ ...] ] ]\n. If = 0, vocabularies are pruned by size.\n\n\n-tgt_vocab \nstring\n (default: \n''\n)\nPath to an existing target vocabulary.\n\n\n-tgt_suffix \nstring\n (default: \n.tgt\n)\nSuffix for target files in train/valid directories.\n\n\n-tgt_vocab_size \ntable\n (default: \n50000\n)\nList of target vocabularies size: \nword[ feat1[ feat2[ ...] ] ]\n. If = 0, vocabularies are not pruned.\n\n\n-tgt_words_min_frequency \ntable\n (default: \n0\n)\nList of target words min frequency: \nword[ feat1[ feat2[ ...] ] ]\n. If = 0, vocabularies are pruned by size.\n\n\n-src_seq_length \nnumber\n (default: \n50\n)\nMaximum source sequence length.\n\n\n-tgt_seq_length \nnumber\n (default: \n50\n)\nMaximum target sequence length.\n\n\n-check_plength [\nboolean\n]\n (default: \nfalse\n)\nCheck source and target have same length (for seq tagging).\n\n\n-features_vocabs_prefix \nstring\n (default: \n''\n)\nPath prefix to existing features vocabularies.\n\n\n-time_shift_feature [\nboolean\n]\n (default: \ntrue\n)\nTime shift features on the decoder side.\n\n\n-keep_frequency [\nboolean\n]\n (default: \nfalse\n)\nKeep frequency of words in dictionary.\n\n\n-gsample \nnumber\n (default: \n0\n)\nIf not zero, extract a new sample from the corpus. In training mode, file sampling is done at each epoch. Values between 0 and 1 indicate ratio, values higher than 1 indicate data size\n\n\n-gsample_dist \nstring\n (default: \n''\n)\nConfiguration file with data class distribution to use for sampling training corpus. If not set, sampling is uniform.\n\n\n-sort [\nboolean\n]\n (default: \ntrue\n)\nIf set, sort the sequences by size to build batches without source padding.\n\n\n-shuffle [\nboolean\n]\n (default: \ntrue\n)\nIf set, shuffle the data (prior sorting).\n\n\n-idx_files [\nboolean\n]\n (default: \nfalse\n)\nIf set, source and target files are 'key value' with key match between source and target.\n\n\n-report_progress_every \nnumber\n (default: \n100000\n)\nReport status every this many sentences.\n\n\n-preprocess_pthreads \nnumber\n (default: \n4\n)\nNumber of parallel threads for preprocessing.\n\n\n\n\nTokenizer options\n\n\n\n\n-tok_src_mode \nstring\n (accepted: \nconservative\n, \naggressive\n, \nspace\n; default: \nspace\n)\nDefine how aggressive should the tokenization be. \nspace\n is space-tokenization.\n\n\n-tok_tgt_mode \nstring\n (accepted: \nconservative\n, \naggressive\n, \nspace\n; default: \nspace\n)\nDefine how aggressive should the tokenization be. \nspace\n is space-tokenization.\n\n\n-tok_src_joiner_annotate [\nboolean\n]\n (default: \nfalse\n)\nInclude joiner annotation using \n-joiner\n character.\n\n\n-tok_tgt_joiner_annotate [\nboolean\n]\n (default: \nfalse\n)\nInclude joiner annotation using \n-joiner\n character.\n\n\n-tok_src_joiner \nstring\n (default: \n\uffed\n)\nCharacter used to annotate joiners.\n\n\n-tok_tgt_joiner \nstring\n (default: \n\uffed\n)\nCharacter used to annotate joiners.\n\n\n-tok_src_joiner_new [\nboolean\n]\n (default: \nfalse\n)\nIn \n-joiner_annotate\n mode, \n-joiner\n is an independent token.\n\n\n-tok_tgt_joiner_new [\nboolean\n]\n (default: \nfalse\n)\nIn \n-joiner_annotate\n mode, \n-joiner\n is an independent token.\n\n\n-tok_src_case_feature [\nboolean\n]\n (default: \nfalse\n)\nGenerate case feature.\n\n\n-tok_tgt_case_feature [\nboolean\n]\n (default: \nfalse\n)\nGenerate case feature.\n\n\n-tok_src_segment_case [\nboolean\n]\n (default: \nfalse\n)\nSegment case feature, splits AbC to Ab C to be able to restore case\n\n\n-tok_tgt_segment_case [\nboolean\n]\n (default: \nfalse\n)\nSegment case feature, splits AbC to Ab C to be able to restore case\n\n\n-tok_src_segment_alphabet \ntable\n (accepted: \nKanbun\n, \nCherokee\n, \nGujarati\n, \nCyrillic\n, \nGurmukhi\n, \nGeorgian\n, \nHanunoo\n, \nHangul\n, \nGreek\n, \nOgham\n, \nBengali\n, \nArmenian\n, \nArabic\n, \nTagbanwa\n, \nKhmer\n, \nMalayalam\n, \nSinhala\n, \nHiragana\n, \nDevanagari\n, \nThaana\n, \nBopomofo\n, \nSyriac\n, \nHebrew\n, \nKangxi\n, \nHan\n, \nLimbu\n, \nTamil\n, \nMongolian\n, \nKannada\n, \nTagalog\n, \nEthiopic\n, \nTelugu\n, \nMyanmar\n, \nLao\n, \nOriya\n, \nBraille\n, \nKatakana\n, \nLatin\n, \nBuhid\n, \nThai\n, \nYi\n, \nTibetan\n)\nSegment all letters from indicated alphabet.\n\n\n-tok_tgt_segment_alphabet \ntable\n (accepted: \nKanbun\n, \nCherokee\n, \nGujarati\n, \nCyrillic\n, \nGurmukhi\n, \nGeorgian\n, \nHanunoo\n, \nHangul\n, \nGreek\n, \nOgham\n, \nBengali\n, \nArmenian\n, \nArabic\n, \nTagbanwa\n, \nKhmer\n, \nMalayalam\n, \nSinhala\n, \nHiragana\n, \nDevanagari\n, \nThaana\n, \nBopomofo\n, \nSyriac\n, \nHebrew\n, \nKangxi\n, \nHan\n, \nLimbu\n, \nTamil\n, \nMongolian\n, \nKannada\n, \nTagalog\n, \nEthiopic\n, \nTelugu\n, \nMyanmar\n, \nLao\n, \nOriya\n, \nBraille\n, \nKatakana\n, \nLatin\n, \nBuhid\n, \nThai\n, \nYi\n, \nTibetan\n)\nSegment all letters from indicated alphabet.\n\n\n-tok_src_segment_numbers [\nboolean\n]\n (default: \nfalse\n)\nSegment numbers into single digits.\n\n\n-tok_tgt_segment_numbers [\nboolean\n]\n (default: \nfalse\n)\nSegment numbers into single digits.\n\n\n-tok_src_segment_alphabet_change [\nboolean\n]\n (default: \nfalse\n)\nSegment if alphabet change between 2 letters.\n\n\n-tok_tgt_segment_alphabet_change [\nboolean\n]\n (default: \nfalse\n)\nSegment if alphabet change between 2 letters.\n\n\n-tok_src_bpe_model \nstring\n (default: \n''\n)\nApply Byte Pair Encoding if the BPE model path is given. If the option is used, BPE related options will be overridden/set automatically if the BPE model specified by \n-bpe_model\n is learnt using \nlearn_bpe.lua\n.\n\n\n-tok_tgt_bpe_model \nstring\n (default: \n''\n)\nApply Byte Pair Encoding if the BPE model path is given. If the option is used, BPE related options will be overridden/set automatically if the BPE model specified by \n-bpe_model\n is learnt using \nlearn_bpe.lua\n.\n\n\n-tok_src_bpe_EOT_marker \nstring\n (default: \n/w\n)\nMarker used to mark the End of Token while applying BPE in mode 'prefix' or 'both'.\n\n\n-tok_tgt_bpe_EOT_marker \nstring\n (default: \n/w\n)\nMarker used to mark the End of Token while applying BPE in mode 'prefix' or 'both'.\n\n\n-tok_src_bpe_BOT_marker \nstring\n (default: \nw\n)\nMarker used to mark the Beginning of Token while applying BPE in mode 'suffix' or 'both'.\n\n\n-tok_tgt_bpe_BOT_marker \nstring\n (default: \nw\n)\nMarker used to mark the Beginning of Token while applying BPE in mode 'suffix' or 'both'.\n\n\n-tok_src_bpe_case_insensitive [\nboolean\n]\n (default: \nfalse\n)\nApply BPE internally in lowercase, but still output the truecase units. This option will be overridden/set automatically if the BPE model specified by \n-bpe_model\n is learnt using \nlearn_bpe.lua\n.\n\n\n-tok_tgt_bpe_case_insensitive [\nboolean\n]\n (default: \nfalse\n)\nApply BPE internally in lowercase, but still output the truecase units. This option will be overridden/set automatically if the BPE model specified by \n-bpe_model\n is learnt using \nlearn_bpe.lua\n.\n\n\n-tok_src_bpe_mode \nstring\n (accepted: \nsuffix\n, \nprefix\n, \nboth\n, \nnone\n; default: \nsuffix\n)\nDefine the BPE mode. This option will be overridden/set automatically if the BPE model specified by \n-bpe_model\n is learnt using \nlearn_bpe.lua\n. \nprefix\n: append \n-bpe_BOT_marker\n to the begining of each word to learn prefix-oriented pair statistics; \nsuffix\n: append \n-bpe_EOT_marker\n to the end of each word to learn suffix-oriented pair statistics, as in the original Python script; \nboth\n: \nsuffix\n and \nprefix\n; \nnone\n: no \nsuffix\n nor \nprefix\n.\n\n\n-tok_tgt_bpe_mode \nstring\n (accepted: \nsuffix\n, \nprefix\n, \nboth\n, \nnone\n; default: \nsuffix\n)\nDefine the BPE mode. This option will be overridden/set automatically if the BPE model specified by \n-bpe_model\n is learnt using \nlearn_bpe.lua\n. \nprefix\n: append \n-bpe_BOT_marker\n to the begining of each word to learn prefix-oriented pair statistics; \nsuffix\n: append \n-bpe_EOT_marker\n to the end of each word to learn suffix-oriented pair statistics, as in the original Python script; \nboth\n: \nsuffix\n and \nprefix\n; \nnone\n: no \nsuffix\n nor \nprefix\n.\n\n\n-tok_src_normalize_cmd \nstring\n (default: \n''\n)\nCommand for on-the-fly corpus normalization. It should work in 'pipeline' mode.\n\n\n-tok_tgt_normalize_cmd \nstring\n (default: \n''\n)\nCommand for on-the-fly corpus normalization. It should work in 'pipeline' mode.\n\n\n\n\nLogger options\n\n\n\n\n-log_file \nstring\n (default: \n''\n)\nOutput logs to a file under this path instead of stdout.\n\n\n-disable_logs [\nboolean\n]\n (default: \nfalse\n)\nIf set, output nothing.\n\n\n-log_level \nstring\n (accepted: \nDEBUG\n, \nINFO\n, \nWARNING\n, \nERROR\n, \nNOERROR\n; default: \nINFO\n)\nOutput logs at this level and above.\n\n\n\n\nOther options\n\n\n\n\n-seed \nnumber\n (default: \n3425\n)\nRandom seed.", 
            "title": "preprocess.lua"
        }, 
        {
            "location": "/options/preprocess/#preprocess-options", 
            "text": "-data_type  string  (accepted:  bitext ,  monotext ,  feattext ; default:  bitext ) Type of data to preprocess. Use 'monotext' for monolingual data. This option impacts all options choices.  -dry_run [ boolean ]  (default:  false ) If set, this will only prepare the preprocessor. Useful when using file sampling to test distribution rules.  -save_data  string  (default:  '' ) Output file for the prepared data.", 
            "title": "Preprocess options"
        }, 
        {
            "location": "/options/preprocess/#data-options", 
            "text": "-train_dir  string  (default:  '' ) Path to training files directory.  -train_src  string  (default:  '' ) Path to the training source data.  -train_tgt  string  (default:  '' ) Path to the training target data.  -valid_src  string  (default:  '' ) Path to the validation source data.  -valid_tgt  string  (default:  '' ) Path to the validation target data.  -src_vocab  string  (default:  '' ) Path to an existing source vocabulary.  -src_suffix  string  (default:  .src ) Suffix for source files in train/valid directories.  -src_vocab_size  table  (default:  50000 ) List of source vocabularies size:  word[ feat1[ feat2[ ...] ] ] . If = 0, vocabularies are not pruned.  -src_words_min_frequency  table  (default:  0 ) List of source words min frequency:  word[ feat1[ feat2[ ...] ] ] . If = 0, vocabularies are pruned by size.  -tgt_vocab  string  (default:  '' ) Path to an existing target vocabulary.  -tgt_suffix  string  (default:  .tgt ) Suffix for target files in train/valid directories.  -tgt_vocab_size  table  (default:  50000 ) List of target vocabularies size:  word[ feat1[ feat2[ ...] ] ] . If = 0, vocabularies are not pruned.  -tgt_words_min_frequency  table  (default:  0 ) List of target words min frequency:  word[ feat1[ feat2[ ...] ] ] . If = 0, vocabularies are pruned by size.  -src_seq_length  number  (default:  50 ) Maximum source sequence length.  -tgt_seq_length  number  (default:  50 ) Maximum target sequence length.  -check_plength [ boolean ]  (default:  false ) Check source and target have same length (for seq tagging).  -features_vocabs_prefix  string  (default:  '' ) Path prefix to existing features vocabularies.  -time_shift_feature [ boolean ]  (default:  true ) Time shift features on the decoder side.  -keep_frequency [ boolean ]  (default:  false ) Keep frequency of words in dictionary.  -gsample  number  (default:  0 ) If not zero, extract a new sample from the corpus. In training mode, file sampling is done at each epoch. Values between 0 and 1 indicate ratio, values higher than 1 indicate data size  -gsample_dist  string  (default:  '' ) Configuration file with data class distribution to use for sampling training corpus. If not set, sampling is uniform.  -sort [ boolean ]  (default:  true ) If set, sort the sequences by size to build batches without source padding.  -shuffle [ boolean ]  (default:  true ) If set, shuffle the data (prior sorting).  -idx_files [ boolean ]  (default:  false ) If set, source and target files are 'key value' with key match between source and target.  -report_progress_every  number  (default:  100000 ) Report status every this many sentences.  -preprocess_pthreads  number  (default:  4 ) Number of parallel threads for preprocessing.", 
            "title": "Data options"
        }, 
        {
            "location": "/options/preprocess/#tokenizer-options", 
            "text": "-tok_src_mode  string  (accepted:  conservative ,  aggressive ,  space ; default:  space ) Define how aggressive should the tokenization be.  space  is space-tokenization.  -tok_tgt_mode  string  (accepted:  conservative ,  aggressive ,  space ; default:  space ) Define how aggressive should the tokenization be.  space  is space-tokenization.  -tok_src_joiner_annotate [ boolean ]  (default:  false ) Include joiner annotation using  -joiner  character.  -tok_tgt_joiner_annotate [ boolean ]  (default:  false ) Include joiner annotation using  -joiner  character.  -tok_src_joiner  string  (default:  \uffed ) Character used to annotate joiners.  -tok_tgt_joiner  string  (default:  \uffed ) Character used to annotate joiners.  -tok_src_joiner_new [ boolean ]  (default:  false ) In  -joiner_annotate  mode,  -joiner  is an independent token.  -tok_tgt_joiner_new [ boolean ]  (default:  false ) In  -joiner_annotate  mode,  -joiner  is an independent token.  -tok_src_case_feature [ boolean ]  (default:  false ) Generate case feature.  -tok_tgt_case_feature [ boolean ]  (default:  false ) Generate case feature.  -tok_src_segment_case [ boolean ]  (default:  false ) Segment case feature, splits AbC to Ab C to be able to restore case  -tok_tgt_segment_case [ boolean ]  (default:  false ) Segment case feature, splits AbC to Ab C to be able to restore case  -tok_src_segment_alphabet  table  (accepted:  Kanbun ,  Cherokee ,  Gujarati ,  Cyrillic ,  Gurmukhi ,  Georgian ,  Hanunoo ,  Hangul ,  Greek ,  Ogham ,  Bengali ,  Armenian ,  Arabic ,  Tagbanwa ,  Khmer ,  Malayalam ,  Sinhala ,  Hiragana ,  Devanagari ,  Thaana ,  Bopomofo ,  Syriac ,  Hebrew ,  Kangxi ,  Han ,  Limbu ,  Tamil ,  Mongolian ,  Kannada ,  Tagalog ,  Ethiopic ,  Telugu ,  Myanmar ,  Lao ,  Oriya ,  Braille ,  Katakana ,  Latin ,  Buhid ,  Thai ,  Yi ,  Tibetan ) Segment all letters from indicated alphabet.  -tok_tgt_segment_alphabet  table  (accepted:  Kanbun ,  Cherokee ,  Gujarati ,  Cyrillic ,  Gurmukhi ,  Georgian ,  Hanunoo ,  Hangul ,  Greek ,  Ogham ,  Bengali ,  Armenian ,  Arabic ,  Tagbanwa ,  Khmer ,  Malayalam ,  Sinhala ,  Hiragana ,  Devanagari ,  Thaana ,  Bopomofo ,  Syriac ,  Hebrew ,  Kangxi ,  Han ,  Limbu ,  Tamil ,  Mongolian ,  Kannada ,  Tagalog ,  Ethiopic ,  Telugu ,  Myanmar ,  Lao ,  Oriya ,  Braille ,  Katakana ,  Latin ,  Buhid ,  Thai ,  Yi ,  Tibetan ) Segment all letters from indicated alphabet.  -tok_src_segment_numbers [ boolean ]  (default:  false ) Segment numbers into single digits.  -tok_tgt_segment_numbers [ boolean ]  (default:  false ) Segment numbers into single digits.  -tok_src_segment_alphabet_change [ boolean ]  (default:  false ) Segment if alphabet change between 2 letters.  -tok_tgt_segment_alphabet_change [ boolean ]  (default:  false ) Segment if alphabet change between 2 letters.  -tok_src_bpe_model  string  (default:  '' ) Apply Byte Pair Encoding if the BPE model path is given. If the option is used, BPE related options will be overridden/set automatically if the BPE model specified by  -bpe_model  is learnt using  learn_bpe.lua .  -tok_tgt_bpe_model  string  (default:  '' ) Apply Byte Pair Encoding if the BPE model path is given. If the option is used, BPE related options will be overridden/set automatically if the BPE model specified by  -bpe_model  is learnt using  learn_bpe.lua .  -tok_src_bpe_EOT_marker  string  (default:  /w ) Marker used to mark the End of Token while applying BPE in mode 'prefix' or 'both'.  -tok_tgt_bpe_EOT_marker  string  (default:  /w ) Marker used to mark the End of Token while applying BPE in mode 'prefix' or 'both'.  -tok_src_bpe_BOT_marker  string  (default:  w ) Marker used to mark the Beginning of Token while applying BPE in mode 'suffix' or 'both'.  -tok_tgt_bpe_BOT_marker  string  (default:  w ) Marker used to mark the Beginning of Token while applying BPE in mode 'suffix' or 'both'.  -tok_src_bpe_case_insensitive [ boolean ]  (default:  false ) Apply BPE internally in lowercase, but still output the truecase units. This option will be overridden/set automatically if the BPE model specified by  -bpe_model  is learnt using  learn_bpe.lua .  -tok_tgt_bpe_case_insensitive [ boolean ]  (default:  false ) Apply BPE internally in lowercase, but still output the truecase units. This option will be overridden/set automatically if the BPE model specified by  -bpe_model  is learnt using  learn_bpe.lua .  -tok_src_bpe_mode  string  (accepted:  suffix ,  prefix ,  both ,  none ; default:  suffix ) Define the BPE mode. This option will be overridden/set automatically if the BPE model specified by  -bpe_model  is learnt using  learn_bpe.lua .  prefix : append  -bpe_BOT_marker  to the begining of each word to learn prefix-oriented pair statistics;  suffix : append  -bpe_EOT_marker  to the end of each word to learn suffix-oriented pair statistics, as in the original Python script;  both :  suffix  and  prefix ;  none : no  suffix  nor  prefix .  -tok_tgt_bpe_mode  string  (accepted:  suffix ,  prefix ,  both ,  none ; default:  suffix ) Define the BPE mode. This option will be overridden/set automatically if the BPE model specified by  -bpe_model  is learnt using  learn_bpe.lua .  prefix : append  -bpe_BOT_marker  to the begining of each word to learn prefix-oriented pair statistics;  suffix : append  -bpe_EOT_marker  to the end of each word to learn suffix-oriented pair statistics, as in the original Python script;  both :  suffix  and  prefix ;  none : no  suffix  nor  prefix .  -tok_src_normalize_cmd  string  (default:  '' ) Command for on-the-fly corpus normalization. It should work in 'pipeline' mode.  -tok_tgt_normalize_cmd  string  (default:  '' ) Command for on-the-fly corpus normalization. It should work in 'pipeline' mode.", 
            "title": "Tokenizer options"
        }, 
        {
            "location": "/options/preprocess/#logger-options", 
            "text": "-log_file  string  (default:  '' ) Output logs to a file under this path instead of stdout.  -disable_logs [ boolean ]  (default:  false ) If set, output nothing.  -log_level  string  (accepted:  DEBUG ,  INFO ,  WARNING ,  ERROR ,  NOERROR ; default:  INFO ) Output logs at this level and above.", 
            "title": "Logger options"
        }, 
        {
            "location": "/options/preprocess/#other-options", 
            "text": "-seed  number  (default:  3425 ) Random seed.", 
            "title": "Other options"
        }, 
        {
            "location": "/options/train/", 
            "text": "train.lua\n options:\n\n\n\n\n-h [\nboolean\n]\n (default: \nfalse\n)\nThis help.\n\n\n-md [\nboolean\n]\n (default: \nfalse\n)\nDump help in Markdown format.\n\n\n-config \nstring\n (default: \n''\n)\nLoad options from this file.\n\n\n-save_config \nstring\n (default: \n''\n)\nSave options to this file.\n\n\n\n\nData options\n\n\n\n\n-data \nstring\n (default: \n''\n)\nPath to the data package \n*-train.t7\n generated by the preprocessing step.\n\n\n\n\nSampled dataset options\n\n\n\n\n-sample \nnumber\n (default: \n0\n)\nNumber of instances to sample from train data in each epoch.\n\n\n-sample_type \nstring\n (accepted: \nuniform\n, \nperplexity\n, \npartition\n; default: \nuniform\n)\nDefine the partition type. \nuniform\n draws randomly the sample, \nperplexity\n uses perplexity as a probability distribution when sampling (with \n-sample_perplexity_init\n and \n-sample_perplexity_max\n options), \npartition\n draws different subsets at each epoch.\n\n\n-sample_perplexity_init \nnumber\n (default: \n15\n)\nStart perplexity-based sampling when average train perplexity per batch falls below this value.\n\n\n-sample_perplexity_max \nnumber\n (default: \n-1.5\n)\nWhen greater than 0, instances with perplexity above this value will be considered as noise and ignored; when less than 0, mode + \n-sample_perplexity_max\n * stdev will be used as threshold.\n\n\n\n\nData options\n\n\n\n\n-train_dir \nstring\n (default: \n''\n)\nPath to training files directory.\n\n\n-train_src \nstring\n (default: \n''\n)\nPath to the training source data.\n\n\n-train_tgt \nstring\n (default: \n''\n)\nPath to the training target data.\n\n\n-valid_src \nstring\n (default: \n''\n)\nPath to the validation source data.\n\n\n-valid_tgt \nstring\n (default: \n''\n)\nPath to the validation target data.\n\n\n-src_vocab \nstring\n (default: \n''\n)\nPath to an existing source vocabulary.\n\n\n-src_suffix \nstring\n (default: \n.src\n)\nSuffix for source files in train/valid directories.\n\n\n-src_vocab_size \ntable\n (default: \n50000\n)\nList of source vocabularies size: \nword[ feat1[ feat2[ ...] ] ]\n. If = 0, vocabularies are not pruned.\n\n\n-src_words_min_frequency \ntable\n (default: \n0\n)\nList of source words min frequency: \nword[ feat1[ feat2[ ...] ] ]\n. If = 0, vocabularies are pruned by size.\n\n\n-tgt_vocab \nstring\n (default: \n''\n)\nPath to an existing target vocabulary.\n\n\n-tgt_suffix \nstring\n (default: \n.tgt\n)\nSuffix for target files in train/valid directories.\n\n\n-tgt_vocab_size \ntable\n (default: \n50000\n)\nList of target vocabularies size: \nword[ feat1[ feat2[ ...] ] ]\n. If = 0, vocabularies are not pruned.\n\n\n-tgt_words_min_frequency \ntable\n (default: \n0\n)\nList of target words min frequency: \nword[ feat1[ feat2[ ...] ] ]\n. If = 0, vocabularies are pruned by size.\n\n\n-src_seq_length \nnumber\n (default: \n50\n)\nMaximum source sequence length.\n\n\n-tgt_seq_length \nnumber\n (default: \n50\n)\nMaximum target sequence length.\n\n\n-check_plength [\nboolean\n]\n (default: \nfalse\n)\nCheck source and target have same length (for seq tagging).\n\n\n-features_vocabs_prefix \nstring\n (default: \n''\n)\nPath prefix to existing features vocabularies.\n\n\n-time_shift_feature [\nboolean\n]\n (default: \ntrue\n)\nTime shift features on the decoder side.\n\n\n-keep_frequency [\nboolean\n]\n (default: \nfalse\n)\nKeep frequency of words in dictionary.\n\n\n-gsample \nnumber\n (default: \n0\n)\nIf not zero, extract a new sample from the corpus. In training mode, file sampling is done at each epoch. Values between 0 and 1 indicate ratio, values higher than 1 indicate data size\n\n\n-gsample_dist \nstring\n (default: \n''\n)\nConfiguration file with data class distribution to use for sampling training corpus. If not set, sampling is uniform.\n\n\n-sort [\nboolean\n]\n (default: \ntrue\n)\nIf set, sort the sequences by size to build batches without source padding.\n\n\n-shuffle [\nboolean\n]\n (default: \ntrue\n)\nIf set, shuffle the data (prior sorting).\n\n\n-idx_files [\nboolean\n]\n (default: \nfalse\n)\nIf set, source and target files are 'key value' with key match between source and target.\n\n\n-report_progress_every \nnumber\n (default: \n100000\n)\nReport status every this many sentences.\n\n\n-preprocess_pthreads \nnumber\n (default: \n4\n)\nNumber of parallel threads for preprocessing.\n\n\n\n\nTokenizer options\n\n\n\n\n-tok_src_mode \nstring\n (accepted: \nconservative\n, \naggressive\n, \nspace\n; default: \nspace\n)\nDefine how aggressive should the tokenization be. \nspace\n is space-tokenization.\n\n\n-tok_tgt_mode \nstring\n (accepted: \nconservative\n, \naggressive\n, \nspace\n; default: \nspace\n)\nDefine how aggressive should the tokenization be. \nspace\n is space-tokenization.\n\n\n-tok_src_joiner_annotate [\nboolean\n]\n (default: \nfalse\n)\nInclude joiner annotation using \n-joiner\n character.\n\n\n-tok_tgt_joiner_annotate [\nboolean\n]\n (default: \nfalse\n)\nInclude joiner annotation using \n-joiner\n character.\n\n\n-tok_src_joiner \nstring\n (default: \n\uffed\n)\nCharacter used to annotate joiners.\n\n\n-tok_tgt_joiner \nstring\n (default: \n\uffed\n)\nCharacter used to annotate joiners.\n\n\n-tok_src_joiner_new [\nboolean\n]\n (default: \nfalse\n)\nIn \n-joiner_annotate\n mode, \n-joiner\n is an independent token.\n\n\n-tok_tgt_joiner_new [\nboolean\n]\n (default: \nfalse\n)\nIn \n-joiner_annotate\n mode, \n-joiner\n is an independent token.\n\n\n-tok_src_case_feature [\nboolean\n]\n (default: \nfalse\n)\nGenerate case feature.\n\n\n-tok_tgt_case_feature [\nboolean\n]\n (default: \nfalse\n)\nGenerate case feature.\n\n\n-tok_src_segment_case [\nboolean\n]\n (default: \nfalse\n)\nSegment case feature, splits AbC to Ab C to be able to restore case\n\n\n-tok_tgt_segment_case [\nboolean\n]\n (default: \nfalse\n)\nSegment case feature, splits AbC to Ab C to be able to restore case\n\n\n-tok_src_segment_alphabet \ntable\n (accepted: \nKhmer\n, \nTagalog\n, \nThaana\n, \nGurmukhi\n, \nDevanagari\n, \nArabic\n, \nTelugu\n, \nCyrillic\n, \nKangxi\n, \nBuhid\n, \nGreek\n, \nKannada\n, \nBraille\n, \nMongolian\n, \nCherokee\n, \nLatin\n, \nYi\n, \nHiragana\n, \nHanunoo\n, \nLao\n, \nBengali\n, \nThai\n, \nKanbun\n, \nHebrew\n, \nLimbu\n, \nOriya\n, \nKatakana\n, \nTibetan\n, \nSinhala\n, \nMalayalam\n, \nEthiopic\n, \nGeorgian\n, \nMyanmar\n, \nHan\n, \nSyriac\n, \nGujarati\n, \nArmenian\n, \nOgham\n, \nBopomofo\n, \nTamil\n, \nHangul\n, \nTagbanwa\n)\nSegment all letters from indicated alphabet.\n\n\n-tok_tgt_segment_alphabet \ntable\n (accepted: \nKhmer\n, \nTagalog\n, \nThaana\n, \nGurmukhi\n, \nDevanagari\n, \nArabic\n, \nTelugu\n, \nCyrillic\n, \nKangxi\n, \nBuhid\n, \nGreek\n, \nKannada\n, \nBraille\n, \nMongolian\n, \nCherokee\n, \nLatin\n, \nYi\n, \nHiragana\n, \nHanunoo\n, \nLao\n, \nBengali\n, \nThai\n, \nKanbun\n, \nHebrew\n, \nLimbu\n, \nOriya\n, \nKatakana\n, \nTibetan\n, \nSinhala\n, \nMalayalam\n, \nEthiopic\n, \nGeorgian\n, \nMyanmar\n, \nHan\n, \nSyriac\n, \nGujarati\n, \nArmenian\n, \nOgham\n, \nBopomofo\n, \nTamil\n, \nHangul\n, \nTagbanwa\n)\nSegment all letters from indicated alphabet.\n\n\n-tok_src_segment_numbers [\nboolean\n]\n (default: \nfalse\n)\nSegment numbers into single digits.\n\n\n-tok_tgt_segment_numbers [\nboolean\n]\n (default: \nfalse\n)\nSegment numbers into single digits.\n\n\n-tok_src_segment_alphabet_change [\nboolean\n]\n (default: \nfalse\n)\nSegment if alphabet change between 2 letters.\n\n\n-tok_tgt_segment_alphabet_change [\nboolean\n]\n (default: \nfalse\n)\nSegment if alphabet change between 2 letters.\n\n\n-tok_src_bpe_model \nstring\n (default: \n''\n)\nApply Byte Pair Encoding if the BPE model path is given. If the option is used, BPE related options will be overridden/set automatically if the BPE model specified by \n-bpe_model\n is learnt using \nlearn_bpe.lua\n.\n\n\n-tok_tgt_bpe_model \nstring\n (default: \n''\n)\nApply Byte Pair Encoding if the BPE model path is given. If the option is used, BPE related options will be overridden/set automatically if the BPE model specified by \n-bpe_model\n is learnt using \nlearn_bpe.lua\n.\n\n\n-tok_src_bpe_EOT_marker \nstring\n (default: \n/w\n)\nMarker used to mark the End of Token while applying BPE in mode 'prefix' or 'both'.\n\n\n-tok_tgt_bpe_EOT_marker \nstring\n (default: \n/w\n)\nMarker used to mark the End of Token while applying BPE in mode 'prefix' or 'both'.\n\n\n-tok_src_bpe_BOT_marker \nstring\n (default: \nw\n)\nMarker used to mark the Beginning of Token while applying BPE in mode 'suffix' or 'both'.\n\n\n-tok_tgt_bpe_BOT_marker \nstring\n (default: \nw\n)\nMarker used to mark the Beginning of Token while applying BPE in mode 'suffix' or 'both'.\n\n\n-tok_src_bpe_case_insensitive [\nboolean\n]\n (default: \nfalse\n)\nApply BPE internally in lowercase, but still output the truecase units. This option will be overridden/set automatically if the BPE model specified by \n-bpe_model\n is learnt using \nlearn_bpe.lua\n.\n\n\n-tok_tgt_bpe_case_insensitive [\nboolean\n]\n (default: \nfalse\n)\nApply BPE internally in lowercase, but still output the truecase units. This option will be overridden/set automatically if the BPE model specified by \n-bpe_model\n is learnt using \nlearn_bpe.lua\n.\n\n\n-tok_src_bpe_mode \nstring\n (accepted: \nsuffix\n, \nprefix\n, \nboth\n, \nnone\n; default: \nsuffix\n)\nDefine the BPE mode. This option will be overridden/set automatically if the BPE model specified by \n-bpe_model\n is learnt using \nlearn_bpe.lua\n. \nprefix\n: append \n-bpe_BOT_marker\n to the begining of each word to learn prefix-oriented pair statistics; \nsuffix\n: append \n-bpe_EOT_marker\n to the end of each word to learn suffix-oriented pair statistics, as in the original Python script; \nboth\n: \nsuffix\n and \nprefix\n; \nnone\n: no \nsuffix\n nor \nprefix\n.\n\n\n-tok_tgt_bpe_mode \nstring\n (accepted: \nsuffix\n, \nprefix\n, \nboth\n, \nnone\n; default: \nsuffix\n)\nDefine the BPE mode. This option will be overridden/set automatically if the BPE model specified by \n-bpe_model\n is learnt using \nlearn_bpe.lua\n. \nprefix\n: append \n-bpe_BOT_marker\n to the begining of each word to learn prefix-oriented pair statistics; \nsuffix\n: append \n-bpe_EOT_marker\n to the end of each word to learn suffix-oriented pair statistics, as in the original Python script; \nboth\n: \nsuffix\n and \nprefix\n; \nnone\n: no \nsuffix\n nor \nprefix\n.\n\n\n-tok_src_normalize_cmd \nstring\n (default: \n''\n)\nCommand for on-the-fly corpus normalization. It should work in 'pipeline' mode.\n\n\n-tok_tgt_normalize_cmd \nstring\n (default: \n''\n)\nCommand for on-the-fly corpus normalization. It should work in 'pipeline' mode.\n\n\n\n\nSampled Vocabulary options\n\n\n\n\n-sample_vocab [\nboolean\n]\n (default: \nfalse\n)\nUse importance sampling as an approximation of the full output vocabulary softmax.\n\n\n\n\nModel options\n\n\n\n\n-model_type \nstring\n (accepted: \nlm\n, \nseq2seq\n, \nseqtagger\n; default: \nseq2seq\n)\nType of model to train. This option impacts all options choices.\n\n\n-param_init \nnumber\n (default: \n0.1\n)\nParameters are initialized over uniform distribution with support (-\nparam_init\n, \nparam_init\n). Set to 0 to rely on each module default initialization.\n\n\n\n\nSequence to Sequence with Attention options\n\n\n\n\n-enc_layers \nnumber\n (default: \n0\n)\nIf \n 0, number of layers of the encoder. This overrides the global \n-layers\n option.\n\n\n-dec_layers \nnumber\n (default: \n0\n)\nIf \n 0, number of layers of the decoder. This overrides the global \n-layers\n option.\n\n\n-word_vec_size \nnumber\n (default: \n0\n)\nShared word embedding size. If set, this overrides \n-src_word_vec_size\n and \n-tgt_word_vec_size\n.\n\n\n-src_word_vec_size \ntable\n (default: \n500\n)\nList of source embedding sizes: \nword[ feat1[ feat2[ ...] ] ]\n.\n\n\n-tgt_word_vec_size \ntable\n (default: \n500\n)\nList of target embedding sizes: \nword[ feat1[ feat2[ ...] ] ]\n.\n\n\n-pre_word_vecs_enc \nstring\n (default: \n''\n)\nPath to pretrained word embeddings on the encoder side serialized as a Torch tensor.\n\n\n-pre_word_vecs_dec \nstring\n (default: \n''\n)\nPath to pretrained word embeddings on the decoder side serialized as a Torch tensor.\n\n\n-fix_word_vecs_enc [\nboolean\n/\nstring\n]\n (accepted: \nfalse\n, \ntrue\n, \npretrained\n; default: \nfalse\n)\nFix word embeddings on the encoder side.\n\n\n-fix_word_vecs_dec [\nboolean\n/\nstring\n]\n (accepted: \nfalse\n, \ntrue\n, \npretrained\n; default: \nfalse\n)\nFix word embeddings on the decoder side.\n\n\n-feat_merge \nstring\n (accepted: \nconcat\n, \nsum\n; default: \nconcat\n)\nMerge action for the features embeddings.\n\n\n-feat_vec_exponent \nnumber\n (default: \n0.7\n)\nWhen features embedding sizes are not set and using \n-feat_merge concat\n, their dimension will be set to \nN^feat_vec_exponent\n where \nN\n is the number of values the feature takes.\n\n\n-feat_vec_size \nnumber\n (default: \n20\n)\nWhen features embedding sizes are not set and using \n-feat_merge sum\n, this is the common embedding size of the features\n\n\n-layers \nnumber\n (default: \n2\n)\nNumber of recurrent layers of the encoder and decoder. See also \n-enc_layers\n, \n-dec_layers\n and \n-bridge\n to assign different layers to the encoder and decoder.\n\n\n-rnn_size \nnumber\n (default: \n500\n)\nHidden size of the recurrent unit.\n\n\n-rnn_type \nstring\n (accepted: \nLSTM\n, \nGRU\n; default: \nLSTM\n)\nType of recurrent cell.\n\n\n-dropout \nnumber\n (default: \n0.3\n)\nDropout probability applied between recurrent layers.\n\n\n-dropout_input [\nboolean\n]\n (default: \nfalse\n)\nDropout probability applied to the input of the recurrent module.\n\n\n-dropout_words \nnumber\n (default: \n0\n)\nDropout probability applied to the source sequence.\n\n\n-dropout_type \nstring\n (accepted: \nnaive\n, \nvariational\n; default: \nnaive\n)\nDropout type.\n\n\n-residual [\nboolean\n]\n (default: \nfalse\n)\nAdd residual connections between recurrent layers.\n\n\n-bridge \nstring\n (accepted: \ncopy\n, \ndense\n, \ndense_nonlinear\n, \nnone\n; default: \ncopy\n)\nDefine how to pass encoder states to the decoder. With \ncopy\n, the encoder and decoder must have the same number of layers.\n\n\n-input_feed [\nboolean\n]\n (default: \ntrue\n)\nFeed the context vector at each time step as additional input (via concatenation with the word embeddings) to the decoder.\n\n\n-scheduled_sampling \nnumber\n (default: \n1\n)\nProbability of feeding true (vs. generated) previous token to decoder.\n\n\n-scheduled_sampling_scope \nstring\n (accepted: \ntoken\n, \nsentence\n; default: \ntoken\n)\nApply scheduled sampling at token or sentence level.\n\n\n-scheduled_sampling_decay_type \nstring\n (accepted: \nlinear\n, \ninvsigmoid\n; default: \nlinear\n)\nScheduled Sampling decay type.\n\n\n-scheduled_sampling_decay_rate \nnumber\n (default: \n0\n)\nScheduled Sampling decay rate.\n\n\n-encoder_type \nstring\n (accepted: \nrnn\n, \nbrnn\n, \ndbrnn\n, \npdbrnn\n, \ngnmt\n, \ncnn\n; default: \nrnn\n)\nEncoder type.\n\n\n-attention \nstring\n (accepted: \nnone\n, \nglobal\n; default: \nglobal\n)\nAttention model.\n\n\n-brnn_merge \nstring\n (accepted: \nconcat\n, \nsum\n; default: \nsum\n)\nMerge action for the bidirectional states.\n\n\n-pdbrnn_reduction \nnumber\n (default: \n2\n)\nTime-reduction factor at each layer.\n\n\n-pdbrnn_merge \nstring\n (accepted: \nconcat\n, \nsum\n; default: \nconcat\n)\nMerge action when reducing time.\n\n\n-cnn_layers \nnumber\n (default: \n2\n)\nNumber of convolutional layers in the encoder.\n\n\n-cnn_kernel \nnumber\n (default: \n3\n)\nKernel size for convolutions. Same in each layer.\n\n\n-cnn_size \nnumber\n (default: \n500\n)\nNumber of output units per convolutional layer. Same in each layer.\n\n\n-use_pos_emb [\nboolean\n]\n (default: \ntrue\n)\nAdd positional embeddings to word embeddings.\n\n\n-max_pos \nnumber\n (default: \n50\n)\nMaximum value for positional indexes.\n\n\n\n\nGlobal Attention Model options\n\n\n\n\n-global_attention \nstring\n (accepted: \ngeneral\n, \ndot\n, \nconcat\n; default: \ngeneral\n)\nGlobal attention model type.\n\n\n\n\nTrainer options\n\n\n\n\n-save_every \nnumber\n (default: \n5000\n)\nSave intermediate models every this many iterations within an epoch. If = 0, will not save intermediate models.\n\n\n-save_every_epochs \nnumber\n (default: \n1\n)\nSave a model every this many epochs. If = 0, will not save a model at each epoch.\n\n\n-report_every \nnumber\n (default: \n50\n)\nReport progress every this many iterations within an epoch.\n\n\n-async_parallel [\nboolean\n]\n (default: \nfalse\n)\nWhen training on multiple GPUs, update parameters asynchronously.\n\n\n-async_parallel_minbatch \nnumber\n (default: \n1000\n)\nIn asynchronous training, minimal number of sequential batches before being parallel.\n\n\n-start_iteration \nnumber\n (default: \n1\n)\nIf loading from a checkpoint, the iteration from which to start.\n\n\n-start_epoch \nnumber\n (default: \n1\n)\nIf loading from a checkpoint, the epoch from which to start.\n\n\n-end_epoch \nnumber\n (default: \n13\n)\nThe final epoch of the training. If = 0, train forever unless another stopping condition is met (e.g. \n-min_learning_rate\n is reached).\n\n\n-curriculum \nnumber\n (default: \n0\n)\nFor this many epochs, order the minibatches based on source length (from smaller to longer). Sometimes setting this to 1 will increase convergence speed.\n\n\n-validation_metric \nstring\n (accepted: \nperplexity\n, \nloss\n, \nbleu\n, \nter\n, \ndlratio\n; default: \nperplexity\n)\nMetric to use for validation.\n\n\n-save_validation_translation_every \nnumber\n (default: \n0\n)\nWhen using translation-based validation metrics (e.g. BLEU, TER, etc.), also save the translation every this many epochs to the file \nsave_model\n_epochN_validation_translation.txt\n. If = 0, will not save validation translation.\n\n\n\n\nOptimization options\n\n\n\n\n-max_batch_size \nnumber\n (default: \n64\n)\nMaximum batch size.\n\n\n-uneven_batches [\nboolean\n]\n (default: \nfalse\n)\nIf set, batches are filled up to \n-max_batch_size\n even if the source lengths are different. Slower but needed for some tasks.\n\n\n-optim \nstring\n (accepted: \nsgd\n, \nadagrad\n, \nadadelta\n, \nadam\n; default: \nsgd\n)\nOptimization method.\n\n\n-learning_rate \nnumber\n (default: \n1\n)\nInitial learning rate. If \nadagrad\n or \nadam\n is used, then this is the global learning rate. Recommended settings are: \nsgd\n = 1, \nadagrad\n = 0.1, \nadam\n = 0.0002.\n\n\n-min_learning_rate \nnumber\n (default: \n0\n)\nDo not continue the training past this learning rate value.\n\n\n-max_grad_norm \nnumber\n (default: \n5\n)\nClip the gradients L2-norm to this value. Set to 0 to disable.\n\n\n-learning_rate_decay \nnumber\n (default: \n0.7\n)\nLearning rate decay factor: \nlearning_rate = learning_rate * learning_rate_decay\n.\n\n\n-start_decay_at \nnumber\n (default: \n9\n)\nIn \"default\" decay mode, start decay after this epoch.\n\n\n-start_decay_score_delta \nnumber\n (default: \n0\n)\nStart decay when validation score improvement is lower than this value.\n\n\n-decay \nstring\n (accepted: \ndefault\n, \nepoch_only\n, \nscore_only\n; default: \ndefault\n)\nWhen to apply learning rate decay. \ndefault\n: decay after each epoch past \n-start_decay_at\n or as soon as the validation score is not improving more than \n-start_decay_score_delta\n, \nepoch_only\n: only decay after each epoch past \n-start_decay_at\n, \nscore_only\n: only decay when validation score is not improving more than \n-start_decay_score_delta\n.\n\n\n-decay_method \nstring\n (accepted: \ndefault\n, \nrestart\n; default: \ndefault\n)\nIf \nrestart\n is set, the optimizer states (if any) will be reset when the decay condition is met.\n\n\n\n\nSaver options\n\n\n\n\n-save_model \nstring\n (required)\nModel filename (the model will be saved as \nsave_model\n_epochN_PPL.t7\n where \nPPL\n is the validation perplexity.\n\n\n-train_from \nstring\n (default: \n''\n)\nPath to a checkpoint.\n\n\n-continue [\nboolean\n]\n (default: \nfalse\n)\nIf set, continue the training where it left off.\n\n\n\n\nTranslator options\n\n\n\n\n-model \nstring\n (default: \n''\n)\nPath to the serialized model file.\n\n\n-lm_model \nstring\n (default: \n''\n)\nPath to serialized language model file.\n\n\n-lm_weight \nnumber\n (default: \n0.1\n)\nRelative weight of language model.\n\n\n-beam_size \nnumber\n (default: \n5\n)\nBeam size.\n\n\n-max_sent_length \nnumber\n (default: \n250\n)\nMaximum output sentence length.\n\n\n-replace_unk [\nboolean\n]\n (default: \nfalse\n)\nReplace the generated \n tokens with the source token that has the highest attention weight. If \n-phrase_table\n is provided, it will lookup the identified source token and give the corresponding target token. If it is not provided (or the identified source token does not exist in the table) then it will copy the source token\n\n\n-replace_unk_tagged [\nboolean\n]\n (default: \nfalse\n)\nThe same as -replace_unk, but wrap the replaced token in \uff5funk:xxxxx\uff60 if it is not found in the phrase table.\n\n\n-phrase_table \nstring\n (default: \n''\n)\nPath to source-target dictionary to replace \nunk\n tokens.\n\n\n-n_best \nnumber\n (default: \n1\n)\nIf \n 1, it will also output an n-best list of decoded sentences.\n\n\n-max_num_unks \nnumber\n (default: \ninf\n)\nAll sequences with more \nunk\ns than this will be ignored during beam search.\n\n\n-target_subdict \nstring\n (default: \n''\n)\nPath to target words dictionary corresponding to the source.\n\n\n-pre_filter_factor \nnumber\n (default: \n1\n)\nOptional, set this only if filter is being used. Before applying filters, hypotheses with top \nbeam_size * pre_filter_factor\n scores will be considered. If the returned hypotheses voilate filters, then set this to a larger value to consider more.\n\n\n-length_norm \nnumber\n (default: \n0\n)\nLength normalization coefficient (alpha). If set to 0, no length normalization.\n\n\n-coverage_norm \nnumber\n (default: \n0\n)\nCoverage normalization coefficient (beta). An extra coverage term multiplied by beta is added to hypotheses scores. If is set to 0, no coverage normalization.\n\n\n-eos_norm \nnumber\n (default: \n0\n)\nEnd of sentence normalization coefficient (gamma). If set to 0, no EOS normalization.\n\n\n-dump_input_encoding [\nboolean\n]\n (default: \nfalse\n)\nInstead of generating target tokens conditional on the source tokens, we print the representation (encoding/embedding) of the input.\n\n\n-save_beam_to \nstring\n (default: \n''\n)\nPath to a file where the beam search exploration will be saved in a JSON format. Requires the \ndkjson\n package.\n\n\n\n\nCrayon options\n\n\n\n\n-exp_host \nstring\n (default: \n127.0.0.1\n)\nCrayon server IP.\n\n\n-exp_port \nstring\n (default: \n8889\n)\nCrayon server port.\n\n\n-exp \nstring\n (default: \n''\n)\nCrayon experiment name.\n\n\n\n\nCuda options\n\n\n\n\n-gpuid \ntable\n (default: \n0\n)\nList of GPU identifiers (1-indexed). CPU is used when set to 0.\n\n\n-fallback_to_cpu [\nboolean\n]\n (default: \nfalse\n)\nIf GPU can't be used, rollback on the CPU.\n\n\n-fp16 [\nboolean\n]\n (default: \nfalse\n)\nUse half-precision float on GPU.\n\n\n-no_nccl [\nboolean\n]\n (default: \nfalse\n)\nDisable usage of nccl in parallel mode.\n\n\n\n\nLogger options\n\n\n\n\n-log_file \nstring\n (default: \n''\n)\nOutput logs to a file under this path instead of stdout.\n\n\n-disable_logs [\nboolean\n]\n (default: \nfalse\n)\nIf set, output nothing.\n\n\n-log_level \nstring\n (accepted: \nDEBUG\n, \nINFO\n, \nWARNING\n, \nERROR\n, \nNOERROR\n; default: \nINFO\n)\nOutput logs at this level and above.\n\n\n\n\nOther options\n\n\n\n\n-disable_mem_optimization [\nboolean\n]\n (default: \nfalse\n)\nDisable sharing of internal buffers between clones for visualization or development.\n\n\n-profiler [\nboolean\n]\n (default: \nfalse\n)\nGenerate profiling logs.\n\n\n-seed \nnumber\n (default: \n3435\n)\nRandom seed.", 
            "title": "train.lua"
        }, 
        {
            "location": "/options/train/#data-options", 
            "text": "-data  string  (default:  '' ) Path to the data package  *-train.t7  generated by the preprocessing step.", 
            "title": "Data options"
        }, 
        {
            "location": "/options/train/#sampled-dataset-options", 
            "text": "-sample  number  (default:  0 ) Number of instances to sample from train data in each epoch.  -sample_type  string  (accepted:  uniform ,  perplexity ,  partition ; default:  uniform ) Define the partition type.  uniform  draws randomly the sample,  perplexity  uses perplexity as a probability distribution when sampling (with  -sample_perplexity_init  and  -sample_perplexity_max  options),  partition  draws different subsets at each epoch.  -sample_perplexity_init  number  (default:  15 ) Start perplexity-based sampling when average train perplexity per batch falls below this value.  -sample_perplexity_max  number  (default:  -1.5 ) When greater than 0, instances with perplexity above this value will be considered as noise and ignored; when less than 0, mode +  -sample_perplexity_max  * stdev will be used as threshold.", 
            "title": "Sampled dataset options"
        }, 
        {
            "location": "/options/train/#data-options_1", 
            "text": "-train_dir  string  (default:  '' ) Path to training files directory.  -train_src  string  (default:  '' ) Path to the training source data.  -train_tgt  string  (default:  '' ) Path to the training target data.  -valid_src  string  (default:  '' ) Path to the validation source data.  -valid_tgt  string  (default:  '' ) Path to the validation target data.  -src_vocab  string  (default:  '' ) Path to an existing source vocabulary.  -src_suffix  string  (default:  .src ) Suffix for source files in train/valid directories.  -src_vocab_size  table  (default:  50000 ) List of source vocabularies size:  word[ feat1[ feat2[ ...] ] ] . If = 0, vocabularies are not pruned.  -src_words_min_frequency  table  (default:  0 ) List of source words min frequency:  word[ feat1[ feat2[ ...] ] ] . If = 0, vocabularies are pruned by size.  -tgt_vocab  string  (default:  '' ) Path to an existing target vocabulary.  -tgt_suffix  string  (default:  .tgt ) Suffix for target files in train/valid directories.  -tgt_vocab_size  table  (default:  50000 ) List of target vocabularies size:  word[ feat1[ feat2[ ...] ] ] . If = 0, vocabularies are not pruned.  -tgt_words_min_frequency  table  (default:  0 ) List of target words min frequency:  word[ feat1[ feat2[ ...] ] ] . If = 0, vocabularies are pruned by size.  -src_seq_length  number  (default:  50 ) Maximum source sequence length.  -tgt_seq_length  number  (default:  50 ) Maximum target sequence length.  -check_plength [ boolean ]  (default:  false ) Check source and target have same length (for seq tagging).  -features_vocabs_prefix  string  (default:  '' ) Path prefix to existing features vocabularies.  -time_shift_feature [ boolean ]  (default:  true ) Time shift features on the decoder side.  -keep_frequency [ boolean ]  (default:  false ) Keep frequency of words in dictionary.  -gsample  number  (default:  0 ) If not zero, extract a new sample from the corpus. In training mode, file sampling is done at each epoch. Values between 0 and 1 indicate ratio, values higher than 1 indicate data size  -gsample_dist  string  (default:  '' ) Configuration file with data class distribution to use for sampling training corpus. If not set, sampling is uniform.  -sort [ boolean ]  (default:  true ) If set, sort the sequences by size to build batches without source padding.  -shuffle [ boolean ]  (default:  true ) If set, shuffle the data (prior sorting).  -idx_files [ boolean ]  (default:  false ) If set, source and target files are 'key value' with key match between source and target.  -report_progress_every  number  (default:  100000 ) Report status every this many sentences.  -preprocess_pthreads  number  (default:  4 ) Number of parallel threads for preprocessing.", 
            "title": "Data options"
        }, 
        {
            "location": "/options/train/#tokenizer-options", 
            "text": "-tok_src_mode  string  (accepted:  conservative ,  aggressive ,  space ; default:  space ) Define how aggressive should the tokenization be.  space  is space-tokenization.  -tok_tgt_mode  string  (accepted:  conservative ,  aggressive ,  space ; default:  space ) Define how aggressive should the tokenization be.  space  is space-tokenization.  -tok_src_joiner_annotate [ boolean ]  (default:  false ) Include joiner annotation using  -joiner  character.  -tok_tgt_joiner_annotate [ boolean ]  (default:  false ) Include joiner annotation using  -joiner  character.  -tok_src_joiner  string  (default:  \uffed ) Character used to annotate joiners.  -tok_tgt_joiner  string  (default:  \uffed ) Character used to annotate joiners.  -tok_src_joiner_new [ boolean ]  (default:  false ) In  -joiner_annotate  mode,  -joiner  is an independent token.  -tok_tgt_joiner_new [ boolean ]  (default:  false ) In  -joiner_annotate  mode,  -joiner  is an independent token.  -tok_src_case_feature [ boolean ]  (default:  false ) Generate case feature.  -tok_tgt_case_feature [ boolean ]  (default:  false ) Generate case feature.  -tok_src_segment_case [ boolean ]  (default:  false ) Segment case feature, splits AbC to Ab C to be able to restore case  -tok_tgt_segment_case [ boolean ]  (default:  false ) Segment case feature, splits AbC to Ab C to be able to restore case  -tok_src_segment_alphabet  table  (accepted:  Khmer ,  Tagalog ,  Thaana ,  Gurmukhi ,  Devanagari ,  Arabic ,  Telugu ,  Cyrillic ,  Kangxi ,  Buhid ,  Greek ,  Kannada ,  Braille ,  Mongolian ,  Cherokee ,  Latin ,  Yi ,  Hiragana ,  Hanunoo ,  Lao ,  Bengali ,  Thai ,  Kanbun ,  Hebrew ,  Limbu ,  Oriya ,  Katakana ,  Tibetan ,  Sinhala ,  Malayalam ,  Ethiopic ,  Georgian ,  Myanmar ,  Han ,  Syriac ,  Gujarati ,  Armenian ,  Ogham ,  Bopomofo ,  Tamil ,  Hangul ,  Tagbanwa ) Segment all letters from indicated alphabet.  -tok_tgt_segment_alphabet  table  (accepted:  Khmer ,  Tagalog ,  Thaana ,  Gurmukhi ,  Devanagari ,  Arabic ,  Telugu ,  Cyrillic ,  Kangxi ,  Buhid ,  Greek ,  Kannada ,  Braille ,  Mongolian ,  Cherokee ,  Latin ,  Yi ,  Hiragana ,  Hanunoo ,  Lao ,  Bengali ,  Thai ,  Kanbun ,  Hebrew ,  Limbu ,  Oriya ,  Katakana ,  Tibetan ,  Sinhala ,  Malayalam ,  Ethiopic ,  Georgian ,  Myanmar ,  Han ,  Syriac ,  Gujarati ,  Armenian ,  Ogham ,  Bopomofo ,  Tamil ,  Hangul ,  Tagbanwa ) Segment all letters from indicated alphabet.  -tok_src_segment_numbers [ boolean ]  (default:  false ) Segment numbers into single digits.  -tok_tgt_segment_numbers [ boolean ]  (default:  false ) Segment numbers into single digits.  -tok_src_segment_alphabet_change [ boolean ]  (default:  false ) Segment if alphabet change between 2 letters.  -tok_tgt_segment_alphabet_change [ boolean ]  (default:  false ) Segment if alphabet change between 2 letters.  -tok_src_bpe_model  string  (default:  '' ) Apply Byte Pair Encoding if the BPE model path is given. If the option is used, BPE related options will be overridden/set automatically if the BPE model specified by  -bpe_model  is learnt using  learn_bpe.lua .  -tok_tgt_bpe_model  string  (default:  '' ) Apply Byte Pair Encoding if the BPE model path is given. If the option is used, BPE related options will be overridden/set automatically if the BPE model specified by  -bpe_model  is learnt using  learn_bpe.lua .  -tok_src_bpe_EOT_marker  string  (default:  /w ) Marker used to mark the End of Token while applying BPE in mode 'prefix' or 'both'.  -tok_tgt_bpe_EOT_marker  string  (default:  /w ) Marker used to mark the End of Token while applying BPE in mode 'prefix' or 'both'.  -tok_src_bpe_BOT_marker  string  (default:  w ) Marker used to mark the Beginning of Token while applying BPE in mode 'suffix' or 'both'.  -tok_tgt_bpe_BOT_marker  string  (default:  w ) Marker used to mark the Beginning of Token while applying BPE in mode 'suffix' or 'both'.  -tok_src_bpe_case_insensitive [ boolean ]  (default:  false ) Apply BPE internally in lowercase, but still output the truecase units. This option will be overridden/set automatically if the BPE model specified by  -bpe_model  is learnt using  learn_bpe.lua .  -tok_tgt_bpe_case_insensitive [ boolean ]  (default:  false ) Apply BPE internally in lowercase, but still output the truecase units. This option will be overridden/set automatically if the BPE model specified by  -bpe_model  is learnt using  learn_bpe.lua .  -tok_src_bpe_mode  string  (accepted:  suffix ,  prefix ,  both ,  none ; default:  suffix ) Define the BPE mode. This option will be overridden/set automatically if the BPE model specified by  -bpe_model  is learnt using  learn_bpe.lua .  prefix : append  -bpe_BOT_marker  to the begining of each word to learn prefix-oriented pair statistics;  suffix : append  -bpe_EOT_marker  to the end of each word to learn suffix-oriented pair statistics, as in the original Python script;  both :  suffix  and  prefix ;  none : no  suffix  nor  prefix .  -tok_tgt_bpe_mode  string  (accepted:  suffix ,  prefix ,  both ,  none ; default:  suffix ) Define the BPE mode. This option will be overridden/set automatically if the BPE model specified by  -bpe_model  is learnt using  learn_bpe.lua .  prefix : append  -bpe_BOT_marker  to the begining of each word to learn prefix-oriented pair statistics;  suffix : append  -bpe_EOT_marker  to the end of each word to learn suffix-oriented pair statistics, as in the original Python script;  both :  suffix  and  prefix ;  none : no  suffix  nor  prefix .  -tok_src_normalize_cmd  string  (default:  '' ) Command for on-the-fly corpus normalization. It should work in 'pipeline' mode.  -tok_tgt_normalize_cmd  string  (default:  '' ) Command for on-the-fly corpus normalization. It should work in 'pipeline' mode.", 
            "title": "Tokenizer options"
        }, 
        {
            "location": "/options/train/#sampled-vocabulary-options", 
            "text": "-sample_vocab [ boolean ]  (default:  false ) Use importance sampling as an approximation of the full output vocabulary softmax.", 
            "title": "Sampled Vocabulary options"
        }, 
        {
            "location": "/options/train/#model-options", 
            "text": "-model_type  string  (accepted:  lm ,  seq2seq ,  seqtagger ; default:  seq2seq ) Type of model to train. This option impacts all options choices.  -param_init  number  (default:  0.1 ) Parameters are initialized over uniform distribution with support (- param_init ,  param_init ). Set to 0 to rely on each module default initialization.", 
            "title": "Model options"
        }, 
        {
            "location": "/options/train/#sequence-to-sequence-with-attention-options", 
            "text": "-enc_layers  number  (default:  0 ) If   0, number of layers of the encoder. This overrides the global  -layers  option.  -dec_layers  number  (default:  0 ) If   0, number of layers of the decoder. This overrides the global  -layers  option.  -word_vec_size  number  (default:  0 ) Shared word embedding size. If set, this overrides  -src_word_vec_size  and  -tgt_word_vec_size .  -src_word_vec_size  table  (default:  500 ) List of source embedding sizes:  word[ feat1[ feat2[ ...] ] ] .  -tgt_word_vec_size  table  (default:  500 ) List of target embedding sizes:  word[ feat1[ feat2[ ...] ] ] .  -pre_word_vecs_enc  string  (default:  '' ) Path to pretrained word embeddings on the encoder side serialized as a Torch tensor.  -pre_word_vecs_dec  string  (default:  '' ) Path to pretrained word embeddings on the decoder side serialized as a Torch tensor.  -fix_word_vecs_enc [ boolean / string ]  (accepted:  false ,  true ,  pretrained ; default:  false ) Fix word embeddings on the encoder side.  -fix_word_vecs_dec [ boolean / string ]  (accepted:  false ,  true ,  pretrained ; default:  false ) Fix word embeddings on the decoder side.  -feat_merge  string  (accepted:  concat ,  sum ; default:  concat ) Merge action for the features embeddings.  -feat_vec_exponent  number  (default:  0.7 ) When features embedding sizes are not set and using  -feat_merge concat , their dimension will be set to  N^feat_vec_exponent  where  N  is the number of values the feature takes.  -feat_vec_size  number  (default:  20 ) When features embedding sizes are not set and using  -feat_merge sum , this is the common embedding size of the features  -layers  number  (default:  2 ) Number of recurrent layers of the encoder and decoder. See also  -enc_layers ,  -dec_layers  and  -bridge  to assign different layers to the encoder and decoder.  -rnn_size  number  (default:  500 ) Hidden size of the recurrent unit.  -rnn_type  string  (accepted:  LSTM ,  GRU ; default:  LSTM ) Type of recurrent cell.  -dropout  number  (default:  0.3 ) Dropout probability applied between recurrent layers.  -dropout_input [ boolean ]  (default:  false ) Dropout probability applied to the input of the recurrent module.  -dropout_words  number  (default:  0 ) Dropout probability applied to the source sequence.  -dropout_type  string  (accepted:  naive ,  variational ; default:  naive ) Dropout type.  -residual [ boolean ]  (default:  false ) Add residual connections between recurrent layers.  -bridge  string  (accepted:  copy ,  dense ,  dense_nonlinear ,  none ; default:  copy ) Define how to pass encoder states to the decoder. With  copy , the encoder and decoder must have the same number of layers.  -input_feed [ boolean ]  (default:  true ) Feed the context vector at each time step as additional input (via concatenation with the word embeddings) to the decoder.  -scheduled_sampling  number  (default:  1 ) Probability of feeding true (vs. generated) previous token to decoder.  -scheduled_sampling_scope  string  (accepted:  token ,  sentence ; default:  token ) Apply scheduled sampling at token or sentence level.  -scheduled_sampling_decay_type  string  (accepted:  linear ,  invsigmoid ; default:  linear ) Scheduled Sampling decay type.  -scheduled_sampling_decay_rate  number  (default:  0 ) Scheduled Sampling decay rate.  -encoder_type  string  (accepted:  rnn ,  brnn ,  dbrnn ,  pdbrnn ,  gnmt ,  cnn ; default:  rnn ) Encoder type.  -attention  string  (accepted:  none ,  global ; default:  global ) Attention model.  -brnn_merge  string  (accepted:  concat ,  sum ; default:  sum ) Merge action for the bidirectional states.  -pdbrnn_reduction  number  (default:  2 ) Time-reduction factor at each layer.  -pdbrnn_merge  string  (accepted:  concat ,  sum ; default:  concat ) Merge action when reducing time.  -cnn_layers  number  (default:  2 ) Number of convolutional layers in the encoder.  -cnn_kernel  number  (default:  3 ) Kernel size for convolutions. Same in each layer.  -cnn_size  number  (default:  500 ) Number of output units per convolutional layer. Same in each layer.  -use_pos_emb [ boolean ]  (default:  true ) Add positional embeddings to word embeddings.  -max_pos  number  (default:  50 ) Maximum value for positional indexes.", 
            "title": "Sequence to Sequence with Attention options"
        }, 
        {
            "location": "/options/train/#global-attention-model-options", 
            "text": "-global_attention  string  (accepted:  general ,  dot ,  concat ; default:  general ) Global attention model type.", 
            "title": "Global Attention Model options"
        }, 
        {
            "location": "/options/train/#trainer-options", 
            "text": "-save_every  number  (default:  5000 ) Save intermediate models every this many iterations within an epoch. If = 0, will not save intermediate models.  -save_every_epochs  number  (default:  1 ) Save a model every this many epochs. If = 0, will not save a model at each epoch.  -report_every  number  (default:  50 ) Report progress every this many iterations within an epoch.  -async_parallel [ boolean ]  (default:  false ) When training on multiple GPUs, update parameters asynchronously.  -async_parallel_minbatch  number  (default:  1000 ) In asynchronous training, minimal number of sequential batches before being parallel.  -start_iteration  number  (default:  1 ) If loading from a checkpoint, the iteration from which to start.  -start_epoch  number  (default:  1 ) If loading from a checkpoint, the epoch from which to start.  -end_epoch  number  (default:  13 ) The final epoch of the training. If = 0, train forever unless another stopping condition is met (e.g.  -min_learning_rate  is reached).  -curriculum  number  (default:  0 ) For this many epochs, order the minibatches based on source length (from smaller to longer). Sometimes setting this to 1 will increase convergence speed.  -validation_metric  string  (accepted:  perplexity ,  loss ,  bleu ,  ter ,  dlratio ; default:  perplexity ) Metric to use for validation.  -save_validation_translation_every  number  (default:  0 ) When using translation-based validation metrics (e.g. BLEU, TER, etc.), also save the translation every this many epochs to the file  save_model _epochN_validation_translation.txt . If = 0, will not save validation translation.", 
            "title": "Trainer options"
        }, 
        {
            "location": "/options/train/#optimization-options", 
            "text": "-max_batch_size  number  (default:  64 ) Maximum batch size.  -uneven_batches [ boolean ]  (default:  false ) If set, batches are filled up to  -max_batch_size  even if the source lengths are different. Slower but needed for some tasks.  -optim  string  (accepted:  sgd ,  adagrad ,  adadelta ,  adam ; default:  sgd ) Optimization method.  -learning_rate  number  (default:  1 ) Initial learning rate. If  adagrad  or  adam  is used, then this is the global learning rate. Recommended settings are:  sgd  = 1,  adagrad  = 0.1,  adam  = 0.0002.  -min_learning_rate  number  (default:  0 ) Do not continue the training past this learning rate value.  -max_grad_norm  number  (default:  5 ) Clip the gradients L2-norm to this value. Set to 0 to disable.  -learning_rate_decay  number  (default:  0.7 ) Learning rate decay factor:  learning_rate = learning_rate * learning_rate_decay .  -start_decay_at  number  (default:  9 ) In \"default\" decay mode, start decay after this epoch.  -start_decay_score_delta  number  (default:  0 ) Start decay when validation score improvement is lower than this value.  -decay  string  (accepted:  default ,  epoch_only ,  score_only ; default:  default ) When to apply learning rate decay.  default : decay after each epoch past  -start_decay_at  or as soon as the validation score is not improving more than  -start_decay_score_delta ,  epoch_only : only decay after each epoch past  -start_decay_at ,  score_only : only decay when validation score is not improving more than  -start_decay_score_delta .  -decay_method  string  (accepted:  default ,  restart ; default:  default ) If  restart  is set, the optimizer states (if any) will be reset when the decay condition is met.", 
            "title": "Optimization options"
        }, 
        {
            "location": "/options/train/#saver-options", 
            "text": "-save_model  string  (required) Model filename (the model will be saved as  save_model _epochN_PPL.t7  where  PPL  is the validation perplexity.  -train_from  string  (default:  '' ) Path to a checkpoint.  -continue [ boolean ]  (default:  false ) If set, continue the training where it left off.", 
            "title": "Saver options"
        }, 
        {
            "location": "/options/train/#translator-options", 
            "text": "-model  string  (default:  '' ) Path to the serialized model file.  -lm_model  string  (default:  '' ) Path to serialized language model file.  -lm_weight  number  (default:  0.1 ) Relative weight of language model.  -beam_size  number  (default:  5 ) Beam size.  -max_sent_length  number  (default:  250 ) Maximum output sentence length.  -replace_unk [ boolean ]  (default:  false ) Replace the generated   tokens with the source token that has the highest attention weight. If  -phrase_table  is provided, it will lookup the identified source token and give the corresponding target token. If it is not provided (or the identified source token does not exist in the table) then it will copy the source token  -replace_unk_tagged [ boolean ]  (default:  false ) The same as -replace_unk, but wrap the replaced token in \uff5funk:xxxxx\uff60 if it is not found in the phrase table.  -phrase_table  string  (default:  '' ) Path to source-target dictionary to replace  unk  tokens.  -n_best  number  (default:  1 ) If   1, it will also output an n-best list of decoded sentences.  -max_num_unks  number  (default:  inf ) All sequences with more  unk s than this will be ignored during beam search.  -target_subdict  string  (default:  '' ) Path to target words dictionary corresponding to the source.  -pre_filter_factor  number  (default:  1 ) Optional, set this only if filter is being used. Before applying filters, hypotheses with top  beam_size * pre_filter_factor  scores will be considered. If the returned hypotheses voilate filters, then set this to a larger value to consider more.  -length_norm  number  (default:  0 ) Length normalization coefficient (alpha). If set to 0, no length normalization.  -coverage_norm  number  (default:  0 ) Coverage normalization coefficient (beta). An extra coverage term multiplied by beta is added to hypotheses scores. If is set to 0, no coverage normalization.  -eos_norm  number  (default:  0 ) End of sentence normalization coefficient (gamma). If set to 0, no EOS normalization.  -dump_input_encoding [ boolean ]  (default:  false ) Instead of generating target tokens conditional on the source tokens, we print the representation (encoding/embedding) of the input.  -save_beam_to  string  (default:  '' ) Path to a file where the beam search exploration will be saved in a JSON format. Requires the  dkjson  package.", 
            "title": "Translator options"
        }, 
        {
            "location": "/options/train/#crayon-options", 
            "text": "-exp_host  string  (default:  127.0.0.1 ) Crayon server IP.  -exp_port  string  (default:  8889 ) Crayon server port.  -exp  string  (default:  '' ) Crayon experiment name.", 
            "title": "Crayon options"
        }, 
        {
            "location": "/options/train/#cuda-options", 
            "text": "-gpuid  table  (default:  0 ) List of GPU identifiers (1-indexed). CPU is used when set to 0.  -fallback_to_cpu [ boolean ]  (default:  false ) If GPU can't be used, rollback on the CPU.  -fp16 [ boolean ]  (default:  false ) Use half-precision float on GPU.  -no_nccl [ boolean ]  (default:  false ) Disable usage of nccl in parallel mode.", 
            "title": "Cuda options"
        }, 
        {
            "location": "/options/train/#logger-options", 
            "text": "-log_file  string  (default:  '' ) Output logs to a file under this path instead of stdout.  -disable_logs [ boolean ]  (default:  false ) If set, output nothing.  -log_level  string  (accepted:  DEBUG ,  INFO ,  WARNING ,  ERROR ,  NOERROR ; default:  INFO ) Output logs at this level and above.", 
            "title": "Logger options"
        }, 
        {
            "location": "/options/train/#other-options", 
            "text": "-disable_mem_optimization [ boolean ]  (default:  false ) Disable sharing of internal buffers between clones for visualization or development.  -profiler [ boolean ]  (default:  false ) Generate profiling logs.  -seed  number  (default:  3435 ) Random seed.", 
            "title": "Other options"
        }, 
        {
            "location": "/options/translate/", 
            "text": "translate.lua\n options:\n\n\n\n\n-h [\nboolean\n]\n (default: \nfalse\n)\nThis help.\n\n\n-md [\nboolean\n]\n (default: \nfalse\n)\nDump help in Markdown format.\n\n\n-config \nstring\n (default: \n''\n)\nLoad options from this file.\n\n\n-save_config \nstring\n (default: \n''\n)\nSave options to this file.\n\n\n\n\nData options\n\n\n\n\n-src \nstring\n (required)\nSource sequences to translate.\n\n\n-tgt \nstring\n (default: \n''\n)\nOptional true target sequences.\n\n\n-output \nstring\n (default: \npred.txt\n)\nOutput file.\n\n\n-save_attention \nstring\n (default: \n''\n)\nOptional attention output file.\n\n\n-batch_size \nnumber\n (default: \n30\n)\nBatch size.\n\n\n-idx_files [\nboolean\n]\n (default: \nfalse\n)\nIf set, source and target files are 'key value' with key match between source and target.\n\n\n\n\nTranslator options\n\n\n\n\n-model \nstring\n (default: \n''\n)\nPath to the serialized model file.\n\n\n-lm_model \nstring\n (default: \n''\n)\nPath to serialized language model file.\n\n\n-lm_weight \nnumber\n (default: \n0.1\n)\nRelative weight of language model.\n\n\n-beam_size \nnumber\n (default: \n5\n)\nBeam size.\n\n\n-max_sent_length \nnumber\n (default: \n250\n)\nMaximum output sentence length.\n\n\n-replace_unk [\nboolean\n]\n (default: \nfalse\n)\nReplace the generated \n tokens with the source token that has the highest attention weight. If \n-phrase_table\n is provided, it will lookup the identified source token and give the corresponding target token. If it is not provided (or the identified source token does not exist in the table) then it will copy the source token\n\n\n-replace_unk_tagged [\nboolean\n]\n (default: \nfalse\n)\nThe same as -replace_unk, but wrap the replaced token in \uff5funk:xxxxx\uff60 if it is not found in the phrase table.\n\n\n-phrase_table \nstring\n (default: \n''\n)\nPath to source-target dictionary to replace \nunk\n tokens.\n\n\n-n_best \nnumber\n (default: \n1\n)\nIf \n 1, it will also output an n-best list of decoded sentences.\n\n\n-max_num_unks \nnumber\n (default: \ninf\n)\nAll sequences with more \nunk\ns than this will be ignored during beam search.\n\n\n-target_subdict \nstring\n (default: \n''\n)\nPath to target words dictionary corresponding to the source.\n\n\n-pre_filter_factor \nnumber\n (default: \n1\n)\nOptional, set this only if filter is being used. Before applying filters, hypotheses with top \nbeam_size * pre_filter_factor\n scores will be considered. If the returned hypotheses voilate filters, then set this to a larger value to consider more.\n\n\n-length_norm \nnumber\n (default: \n0\n)\nLength normalization coefficient (alpha). If set to 0, no length normalization.\n\n\n-coverage_norm \nnumber\n (default: \n0\n)\nCoverage normalization coefficient (beta). An extra coverage term multiplied by beta is added to hypotheses scores. If is set to 0, no coverage normalization.\n\n\n-eos_norm \nnumber\n (default: \n0\n)\nEnd of sentence normalization coefficient (gamma). If set to 0, no EOS normalization.\n\n\n-dump_input_encoding [\nboolean\n]\n (default: \nfalse\n)\nInstead of generating target tokens conditional on the source tokens, we print the representation (encoding/embedding) of the input.\n\n\n-save_beam_to \nstring\n (default: \n''\n)\nPath to a file where the beam search exploration will be saved in a JSON format. Requires the \ndkjson\n package.\n\n\n\n\nTokenizer options\n\n\n\n\n-tok_src_mode \nstring\n (accepted: \nconservative\n, \naggressive\n, \nspace\n; default: \nspace\n)\nDefine how aggressive should the tokenization be. \nspace\n is space-tokenization.\n\n\n-tok_tgt_mode \nstring\n (accepted: \nconservative\n, \naggressive\n, \nspace\n; default: \nspace\n)\nDefine how aggressive should the tokenization be. \nspace\n is space-tokenization.\n\n\n-tok_src_joiner_annotate [\nboolean\n]\n (default: \nfalse\n)\nInclude joiner annotation using \n-joiner\n character.\n\n\n-tok_tgt_joiner_annotate [\nboolean\n]\n (default: \nfalse\n)\nInclude joiner annotation using \n-joiner\n character.\n\n\n-tok_src_joiner \nstring\n (default: \n\uffed\n)\nCharacter used to annotate joiners.\n\n\n-tok_tgt_joiner \nstring\n (default: \n\uffed\n)\nCharacter used to annotate joiners.\n\n\n-tok_src_joiner_new [\nboolean\n]\n (default: \nfalse\n)\nIn \n-joiner_annotate\n mode, \n-joiner\n is an independent token.\n\n\n-tok_tgt_joiner_new [\nboolean\n]\n (default: \nfalse\n)\nIn \n-joiner_annotate\n mode, \n-joiner\n is an independent token.\n\n\n-tok_src_case_feature [\nboolean\n]\n (default: \nfalse\n)\nGenerate case feature.\n\n\n-tok_tgt_case_feature [\nboolean\n]\n (default: \nfalse\n)\nGenerate case feature.\n\n\n-tok_src_segment_case [\nboolean\n]\n (default: \nfalse\n)\nSegment case feature, splits AbC to Ab C to be able to restore case\n\n\n-tok_tgt_segment_case [\nboolean\n]\n (default: \nfalse\n)\nSegment case feature, splits AbC to Ab C to be able to restore case\n\n\n-tok_src_segment_alphabet \ntable\n (accepted: \nArabic\n, \nKangxi\n, \nCyrillic\n, \nDevanagari\n, \nThaana\n, \nHangul\n, \nHanunoo\n, \nMongolian\n, \nHebrew\n, \nTagbanwa\n, \nCherokee\n, \nBopomofo\n, \nKanbun\n, \nTelugu\n, \nBraille\n, \nTagalog\n, \nKatakana\n, \nHiragana\n, \nKannada\n, \nYi\n, \nGreek\n, \nGujarati\n, \nGurmukhi\n, \nArmenian\n, \nSyriac\n, \nHan\n, \nMalayalam\n, \nKhmer\n, \nBuhid\n, \nOgham\n, \nEthiopic\n, \nGeorgian\n, \nMyanmar\n, \nSinhala\n, \nOriya\n, \nLatin\n, \nLao\n, \nLimbu\n, \nTibetan\n, \nThai\n, \nTamil\n, \nBengali\n)\nSegment all letters from indicated alphabet.\n\n\n-tok_tgt_segment_alphabet \ntable\n (accepted: \nArabic\n, \nKangxi\n, \nCyrillic\n, \nDevanagari\n, \nThaana\n, \nHangul\n, \nHanunoo\n, \nMongolian\n, \nHebrew\n, \nTagbanwa\n, \nCherokee\n, \nBopomofo\n, \nKanbun\n, \nTelugu\n, \nBraille\n, \nTagalog\n, \nKatakana\n, \nHiragana\n, \nKannada\n, \nYi\n, \nGreek\n, \nGujarati\n, \nGurmukhi\n, \nArmenian\n, \nSyriac\n, \nHan\n, \nMalayalam\n, \nKhmer\n, \nBuhid\n, \nOgham\n, \nEthiopic\n, \nGeorgian\n, \nMyanmar\n, \nSinhala\n, \nOriya\n, \nLatin\n, \nLao\n, \nLimbu\n, \nTibetan\n, \nThai\n, \nTamil\n, \nBengali\n)\nSegment all letters from indicated alphabet.\n\n\n-tok_src_segment_numbers [\nboolean\n]\n (default: \nfalse\n)\nSegment numbers into single digits.\n\n\n-tok_tgt_segment_numbers [\nboolean\n]\n (default: \nfalse\n)\nSegment numbers into single digits.\n\n\n-tok_src_segment_alphabet_change [\nboolean\n]\n (default: \nfalse\n)\nSegment if alphabet change between 2 letters.\n\n\n-tok_tgt_segment_alphabet_change [\nboolean\n]\n (default: \nfalse\n)\nSegment if alphabet change between 2 letters.\n\n\n-tok_src_bpe_model \nstring\n (default: \n''\n)\nApply Byte Pair Encoding if the BPE model path is given. If the option is used, BPE related options will be overridden/set automatically if the BPE model specified by \n-bpe_model\n is learnt using \nlearn_bpe.lua\n.\n\n\n-tok_tgt_bpe_model \nstring\n (default: \n''\n)\nApply Byte Pair Encoding if the BPE model path is given. If the option is used, BPE related options will be overridden/set automatically if the BPE model specified by \n-bpe_model\n is learnt using \nlearn_bpe.lua\n.\n\n\n-tok_src_bpe_EOT_marker \nstring\n (default: \n/w\n)\nMarker used to mark the End of Token while applying BPE in mode 'prefix' or 'both'.\n\n\n-tok_tgt_bpe_EOT_marker \nstring\n (default: \n/w\n)\nMarker used to mark the End of Token while applying BPE in mode 'prefix' or 'both'.\n\n\n-tok_src_bpe_BOT_marker \nstring\n (default: \nw\n)\nMarker used to mark the Beginning of Token while applying BPE in mode 'suffix' or 'both'.\n\n\n-tok_tgt_bpe_BOT_marker \nstring\n (default: \nw\n)\nMarker used to mark the Beginning of Token while applying BPE in mode 'suffix' or 'both'.\n\n\n-tok_src_bpe_case_insensitive [\nboolean\n]\n (default: \nfalse\n)\nApply BPE internally in lowercase, but still output the truecase units. This option will be overridden/set automatically if the BPE model specified by \n-bpe_model\n is learnt using \nlearn_bpe.lua\n.\n\n\n-tok_tgt_bpe_case_insensitive [\nboolean\n]\n (default: \nfalse\n)\nApply BPE internally in lowercase, but still output the truecase units. This option will be overridden/set automatically if the BPE model specified by \n-bpe_model\n is learnt using \nlearn_bpe.lua\n.\n\n\n-tok_src_bpe_mode \nstring\n (accepted: \nsuffix\n, \nprefix\n, \nboth\n, \nnone\n; default: \nsuffix\n)\nDefine the BPE mode. This option will be overridden/set automatically if the BPE model specified by \n-bpe_model\n is learnt using \nlearn_bpe.lua\n. \nprefix\n: append \n-bpe_BOT_marker\n to the begining of each word to learn prefix-oriented pair statistics; \nsuffix\n: append \n-bpe_EOT_marker\n to the end of each word to learn suffix-oriented pair statistics, as in the original Python script; \nboth\n: \nsuffix\n and \nprefix\n; \nnone\n: no \nsuffix\n nor \nprefix\n.\n\n\n-tok_tgt_bpe_mode \nstring\n (accepted: \nsuffix\n, \nprefix\n, \nboth\n, \nnone\n; default: \nsuffix\n)\nDefine the BPE mode. This option will be overridden/set automatically if the BPE model specified by \n-bpe_model\n is learnt using \nlearn_bpe.lua\n. \nprefix\n: append \n-bpe_BOT_marker\n to the begining of each word to learn prefix-oriented pair statistics; \nsuffix\n: append \n-bpe_EOT_marker\n to the end of each word to learn suffix-oriented pair statistics, as in the original Python script; \nboth\n: \nsuffix\n and \nprefix\n; \nnone\n: no \nsuffix\n nor \nprefix\n.\n\n\n-tok_src_normalize_cmd \nstring\n (default: \n''\n)\nCommand for on-the-fly corpus normalization. It should work in 'pipeline' mode.\n\n\n-tok_tgt_normalize_cmd \nstring\n (default: \n''\n)\nCommand for on-the-fly corpus normalization. It should work in 'pipeline' mode.\n\n\n-detokenize_output [\nboolean\n]\n (default: \nfalse\n)\nDetokenize output.\n\n\n\n\nCuda options\n\n\n\n\n-gpuid \ntable\n (default: \n0\n)\nList of GPU identifiers (1-indexed). CPU is used when set to 0.\n\n\n-fallback_to_cpu [\nboolean\n]\n (default: \nfalse\n)\nIf GPU can't be used, rollback on the CPU.\n\n\n-fp16 [\nboolean\n]\n (default: \nfalse\n)\nUse half-precision float on GPU.\n\n\n-no_nccl [\nboolean\n]\n (default: \nfalse\n)\nDisable usage of nccl in parallel mode.\n\n\n\n\nLogger options\n\n\n\n\n-log_file \nstring\n (default: \n''\n)\nOutput logs to a file under this path instead of stdout.\n\n\n-disable_logs [\nboolean\n]\n (default: \nfalse\n)\nIf set, output nothing.\n\n\n-log_level \nstring\n (accepted: \nDEBUG\n, \nINFO\n, \nWARNING\n, \nERROR\n, \nNOERROR\n; default: \nINFO\n)\nOutput logs at this level and above.\n\n\n\n\nOther options\n\n\n\n\n-time [\nboolean\n]\n (default: \nfalse\n)\nMeasure average translation time.", 
            "title": "translate.lua"
        }, 
        {
            "location": "/options/translate/#data-options", 
            "text": "-src  string  (required) Source sequences to translate.  -tgt  string  (default:  '' ) Optional true target sequences.  -output  string  (default:  pred.txt ) Output file.  -save_attention  string  (default:  '' ) Optional attention output file.  -batch_size  number  (default:  30 ) Batch size.  -idx_files [ boolean ]  (default:  false ) If set, source and target files are 'key value' with key match between source and target.", 
            "title": "Data options"
        }, 
        {
            "location": "/options/translate/#translator-options", 
            "text": "-model  string  (default:  '' ) Path to the serialized model file.  -lm_model  string  (default:  '' ) Path to serialized language model file.  -lm_weight  number  (default:  0.1 ) Relative weight of language model.  -beam_size  number  (default:  5 ) Beam size.  -max_sent_length  number  (default:  250 ) Maximum output sentence length.  -replace_unk [ boolean ]  (default:  false ) Replace the generated   tokens with the source token that has the highest attention weight. If  -phrase_table  is provided, it will lookup the identified source token and give the corresponding target token. If it is not provided (or the identified source token does not exist in the table) then it will copy the source token  -replace_unk_tagged [ boolean ]  (default:  false ) The same as -replace_unk, but wrap the replaced token in \uff5funk:xxxxx\uff60 if it is not found in the phrase table.  -phrase_table  string  (default:  '' ) Path to source-target dictionary to replace  unk  tokens.  -n_best  number  (default:  1 ) If   1, it will also output an n-best list of decoded sentences.  -max_num_unks  number  (default:  inf ) All sequences with more  unk s than this will be ignored during beam search.  -target_subdict  string  (default:  '' ) Path to target words dictionary corresponding to the source.  -pre_filter_factor  number  (default:  1 ) Optional, set this only if filter is being used. Before applying filters, hypotheses with top  beam_size * pre_filter_factor  scores will be considered. If the returned hypotheses voilate filters, then set this to a larger value to consider more.  -length_norm  number  (default:  0 ) Length normalization coefficient (alpha). If set to 0, no length normalization.  -coverage_norm  number  (default:  0 ) Coverage normalization coefficient (beta). An extra coverage term multiplied by beta is added to hypotheses scores. If is set to 0, no coverage normalization.  -eos_norm  number  (default:  0 ) End of sentence normalization coefficient (gamma). If set to 0, no EOS normalization.  -dump_input_encoding [ boolean ]  (default:  false ) Instead of generating target tokens conditional on the source tokens, we print the representation (encoding/embedding) of the input.  -save_beam_to  string  (default:  '' ) Path to a file where the beam search exploration will be saved in a JSON format. Requires the  dkjson  package.", 
            "title": "Translator options"
        }, 
        {
            "location": "/options/translate/#tokenizer-options", 
            "text": "-tok_src_mode  string  (accepted:  conservative ,  aggressive ,  space ; default:  space ) Define how aggressive should the tokenization be.  space  is space-tokenization.  -tok_tgt_mode  string  (accepted:  conservative ,  aggressive ,  space ; default:  space ) Define how aggressive should the tokenization be.  space  is space-tokenization.  -tok_src_joiner_annotate [ boolean ]  (default:  false ) Include joiner annotation using  -joiner  character.  -tok_tgt_joiner_annotate [ boolean ]  (default:  false ) Include joiner annotation using  -joiner  character.  -tok_src_joiner  string  (default:  \uffed ) Character used to annotate joiners.  -tok_tgt_joiner  string  (default:  \uffed ) Character used to annotate joiners.  -tok_src_joiner_new [ boolean ]  (default:  false ) In  -joiner_annotate  mode,  -joiner  is an independent token.  -tok_tgt_joiner_new [ boolean ]  (default:  false ) In  -joiner_annotate  mode,  -joiner  is an independent token.  -tok_src_case_feature [ boolean ]  (default:  false ) Generate case feature.  -tok_tgt_case_feature [ boolean ]  (default:  false ) Generate case feature.  -tok_src_segment_case [ boolean ]  (default:  false ) Segment case feature, splits AbC to Ab C to be able to restore case  -tok_tgt_segment_case [ boolean ]  (default:  false ) Segment case feature, splits AbC to Ab C to be able to restore case  -tok_src_segment_alphabet  table  (accepted:  Arabic ,  Kangxi ,  Cyrillic ,  Devanagari ,  Thaana ,  Hangul ,  Hanunoo ,  Mongolian ,  Hebrew ,  Tagbanwa ,  Cherokee ,  Bopomofo ,  Kanbun ,  Telugu ,  Braille ,  Tagalog ,  Katakana ,  Hiragana ,  Kannada ,  Yi ,  Greek ,  Gujarati ,  Gurmukhi ,  Armenian ,  Syriac ,  Han ,  Malayalam ,  Khmer ,  Buhid ,  Ogham ,  Ethiopic ,  Georgian ,  Myanmar ,  Sinhala ,  Oriya ,  Latin ,  Lao ,  Limbu ,  Tibetan ,  Thai ,  Tamil ,  Bengali ) Segment all letters from indicated alphabet.  -tok_tgt_segment_alphabet  table  (accepted:  Arabic ,  Kangxi ,  Cyrillic ,  Devanagari ,  Thaana ,  Hangul ,  Hanunoo ,  Mongolian ,  Hebrew ,  Tagbanwa ,  Cherokee ,  Bopomofo ,  Kanbun ,  Telugu ,  Braille ,  Tagalog ,  Katakana ,  Hiragana ,  Kannada ,  Yi ,  Greek ,  Gujarati ,  Gurmukhi ,  Armenian ,  Syriac ,  Han ,  Malayalam ,  Khmer ,  Buhid ,  Ogham ,  Ethiopic ,  Georgian ,  Myanmar ,  Sinhala ,  Oriya ,  Latin ,  Lao ,  Limbu ,  Tibetan ,  Thai ,  Tamil ,  Bengali ) Segment all letters from indicated alphabet.  -tok_src_segment_numbers [ boolean ]  (default:  false ) Segment numbers into single digits.  -tok_tgt_segment_numbers [ boolean ]  (default:  false ) Segment numbers into single digits.  -tok_src_segment_alphabet_change [ boolean ]  (default:  false ) Segment if alphabet change between 2 letters.  -tok_tgt_segment_alphabet_change [ boolean ]  (default:  false ) Segment if alphabet change between 2 letters.  -tok_src_bpe_model  string  (default:  '' ) Apply Byte Pair Encoding if the BPE model path is given. If the option is used, BPE related options will be overridden/set automatically if the BPE model specified by  -bpe_model  is learnt using  learn_bpe.lua .  -tok_tgt_bpe_model  string  (default:  '' ) Apply Byte Pair Encoding if the BPE model path is given. If the option is used, BPE related options will be overridden/set automatically if the BPE model specified by  -bpe_model  is learnt using  learn_bpe.lua .  -tok_src_bpe_EOT_marker  string  (default:  /w ) Marker used to mark the End of Token while applying BPE in mode 'prefix' or 'both'.  -tok_tgt_bpe_EOT_marker  string  (default:  /w ) Marker used to mark the End of Token while applying BPE in mode 'prefix' or 'both'.  -tok_src_bpe_BOT_marker  string  (default:  w ) Marker used to mark the Beginning of Token while applying BPE in mode 'suffix' or 'both'.  -tok_tgt_bpe_BOT_marker  string  (default:  w ) Marker used to mark the Beginning of Token while applying BPE in mode 'suffix' or 'both'.  -tok_src_bpe_case_insensitive [ boolean ]  (default:  false ) Apply BPE internally in lowercase, but still output the truecase units. This option will be overridden/set automatically if the BPE model specified by  -bpe_model  is learnt using  learn_bpe.lua .  -tok_tgt_bpe_case_insensitive [ boolean ]  (default:  false ) Apply BPE internally in lowercase, but still output the truecase units. This option will be overridden/set automatically if the BPE model specified by  -bpe_model  is learnt using  learn_bpe.lua .  -tok_src_bpe_mode  string  (accepted:  suffix ,  prefix ,  both ,  none ; default:  suffix ) Define the BPE mode. This option will be overridden/set automatically if the BPE model specified by  -bpe_model  is learnt using  learn_bpe.lua .  prefix : append  -bpe_BOT_marker  to the begining of each word to learn prefix-oriented pair statistics;  suffix : append  -bpe_EOT_marker  to the end of each word to learn suffix-oriented pair statistics, as in the original Python script;  both :  suffix  and  prefix ;  none : no  suffix  nor  prefix .  -tok_tgt_bpe_mode  string  (accepted:  suffix ,  prefix ,  both ,  none ; default:  suffix ) Define the BPE mode. This option will be overridden/set automatically if the BPE model specified by  -bpe_model  is learnt using  learn_bpe.lua .  prefix : append  -bpe_BOT_marker  to the begining of each word to learn prefix-oriented pair statistics;  suffix : append  -bpe_EOT_marker  to the end of each word to learn suffix-oriented pair statistics, as in the original Python script;  both :  suffix  and  prefix ;  none : no  suffix  nor  prefix .  -tok_src_normalize_cmd  string  (default:  '' ) Command for on-the-fly corpus normalization. It should work in 'pipeline' mode.  -tok_tgt_normalize_cmd  string  (default:  '' ) Command for on-the-fly corpus normalization. It should work in 'pipeline' mode.  -detokenize_output [ boolean ]  (default:  false ) Detokenize output.", 
            "title": "Tokenizer options"
        }, 
        {
            "location": "/options/translate/#cuda-options", 
            "text": "-gpuid  table  (default:  0 ) List of GPU identifiers (1-indexed). CPU is used when set to 0.  -fallback_to_cpu [ boolean ]  (default:  false ) If GPU can't be used, rollback on the CPU.  -fp16 [ boolean ]  (default:  false ) Use half-precision float on GPU.  -no_nccl [ boolean ]  (default:  false ) Disable usage of nccl in parallel mode.", 
            "title": "Cuda options"
        }, 
        {
            "location": "/options/translate/#logger-options", 
            "text": "-log_file  string  (default:  '' ) Output logs to a file under this path instead of stdout.  -disable_logs [ boolean ]  (default:  false ) If set, output nothing.  -log_level  string  (accepted:  DEBUG ,  INFO ,  WARNING ,  ERROR ,  NOERROR ; default:  INFO ) Output logs at this level and above.", 
            "title": "Logger options"
        }, 
        {
            "location": "/options/translate/#other-options", 
            "text": "-time [ boolean ]  (default:  false ) Measure average translation time.", 
            "title": "Other options"
        }, 
        {
            "location": "/options/tag/", 
            "text": "tag.lua\n options:\n\n\n\n\n-h [\nboolean\n]\n (default: \nfalse\n)\nThis help.\n\n\n-md [\nboolean\n]\n (default: \nfalse\n)\nDump help in Markdown format.\n\n\n-config \nstring\n (default: \n''\n)\nLoad options from this file.\n\n\n-save_config \nstring\n (default: \n''\n)\nSave options to this file.\n\n\n\n\nData options\n\n\n\n\n-src \nstring\n (required)\nSource sequences to tag.\n\n\n-output \nstring\n (default: \npred.txt\n)\nOutput file.\n\n\n-idx_files [\nboolean\n]\n (default: \nfalse\n)\nIf set, source and target files are 'key value' with key match between source and target.\n\n\n\n\nTagger options\n\n\n\n\n-model \nstring\n (required)\nPath to the serialized model file.\n\n\n-batch_size \nnumber\n (default: \n30\n)\nBatch size.\n\n\n\n\nCuda options\n\n\n\n\n-gpuid \ntable\n (default: \n0\n)\nList of GPU identifiers (1-indexed). CPU is used when set to 0.\n\n\n-fallback_to_cpu [\nboolean\n]\n (default: \nfalse\n)\nIf GPU can't be used, rollback on the CPU.\n\n\n-fp16 [\nboolean\n]\n (default: \nfalse\n)\nUse half-precision float on GPU.\n\n\n-no_nccl [\nboolean\n]\n (default: \nfalse\n)\nDisable usage of nccl in parallel mode.\n\n\n\n\nLogger options\n\n\n\n\n-log_file \nstring\n (default: \n''\n)\nOutput logs to a file under this path instead of stdout.\n\n\n-disable_logs [\nboolean\n]\n (default: \nfalse\n)\nIf set, output nothing.\n\n\n-log_level \nstring\n (accepted: \nDEBUG\n, \nINFO\n, \nWARNING\n, \nERROR\n, \nNOERROR\n; default: \nINFO\n)\nOutput logs at this level and above.\n\n\n\n\nOther options\n\n\n\n\n-time [\nboolean\n]\n (default: \nfalse\n)\nMeasure average translation time.", 
            "title": "tag.lua"
        }, 
        {
            "location": "/options/tag/#data-options", 
            "text": "-src  string  (required) Source sequences to tag.  -output  string  (default:  pred.txt ) Output file.  -idx_files [ boolean ]  (default:  false ) If set, source and target files are 'key value' with key match between source and target.", 
            "title": "Data options"
        }, 
        {
            "location": "/options/tag/#tagger-options", 
            "text": "-model  string  (required) Path to the serialized model file.  -batch_size  number  (default:  30 ) Batch size.", 
            "title": "Tagger options"
        }, 
        {
            "location": "/options/tag/#cuda-options", 
            "text": "-gpuid  table  (default:  0 ) List of GPU identifiers (1-indexed). CPU is used when set to 0.  -fallback_to_cpu [ boolean ]  (default:  false ) If GPU can't be used, rollback on the CPU.  -fp16 [ boolean ]  (default:  false ) Use half-precision float on GPU.  -no_nccl [ boolean ]  (default:  false ) Disable usage of nccl in parallel mode.", 
            "title": "Cuda options"
        }, 
        {
            "location": "/options/tag/#logger-options", 
            "text": "-log_file  string  (default:  '' ) Output logs to a file under this path instead of stdout.  -disable_logs [ boolean ]  (default:  false ) If set, output nothing.  -log_level  string  (accepted:  DEBUG ,  INFO ,  WARNING ,  ERROR ,  NOERROR ; default:  INFO ) Output logs at this level and above.", 
            "title": "Logger options"
        }, 
        {
            "location": "/options/tag/#other-options", 
            "text": "-time [ boolean ]  (default:  false ) Measure average translation time.", 
            "title": "Other options"
        }, 
        {
            "location": "/options/lm/", 
            "text": "lm.lua\n options:\n\n\n\n\n-h [\nboolean\n]\n (default: \nfalse\n)\nThis help.\n\n\n-md [\nboolean\n]\n (default: \nfalse\n)\nDump help in Markdown format.\n\n\n-config \nstring\n (default: \n''\n)\nLoad options from this file.\n\n\n-save_config \nstring\n (default: \n''\n)\nSave options to this file.\n\n\n\n\nData options\n\n\n\n\nmode\n (accepted: \nscore\n, \nsample\n)\n'score' apply lm to input text, 'sample' samples output based on input text.\n\n\n-src \nstring\n (required)\nSource sequences to sample/score.\n\n\n-output \nstring\n (default: \noutput.txt\n)\nOutput file depend on \nmode\n.\n\n\n\n\nLM options\n\n\n\n\n-model \nstring\n (required)\nPath to the serialized model file.\n\n\n-batch_size \nnumber\n (default: \n30\n)\nBatch size.\n\n\n-max_length \nnumber\n (default: \n100\n)\nMaximal length of sentences in sample mode.\n\n\n-temperature \nnumber\n (default: \n1\n)\nFor \nsample\n mode, higher temperatures cause the model to take more chances and increase diversity of results, but at a cost of more mistakes.\n\n\n\n\nCuda options\n\n\n\n\n-gpuid \ntable\n (default: \n0\n)\nList of GPU identifiers (1-indexed). CPU is used when set to 0.\n\n\n-fallback_to_cpu [\nboolean\n]\n (default: \nfalse\n)\nIf GPU can't be used, rollback on the CPU.\n\n\n-fp16 [\nboolean\n]\n (default: \nfalse\n)\nUse half-precision float on GPU.\n\n\n-no_nccl [\nboolean\n]\n (default: \nfalse\n)\nDisable usage of nccl in parallel mode.\n\n\n\n\nLogger options\n\n\n\n\n-log_file \nstring\n (default: \n''\n)\nOutput logs to a file under this path instead of stdout.\n\n\n-disable_logs [\nboolean\n]\n (default: \nfalse\n)\nIf set, output nothing.\n\n\n-log_level \nstring\n (accepted: \nDEBUG\n, \nINFO\n, \nWARNING\n, \nERROR\n, \nNOERROR\n; default: \nINFO\n)\nOutput logs at this level and above.\n\n\n\n\nOther options\n\n\n\n\n-time [\nboolean\n]\n (default: \nfalse\n)\nMeasure average translation time.", 
            "title": "lm.lua"
        }, 
        {
            "location": "/options/lm/#data-options", 
            "text": "mode  (accepted:  score ,  sample ) 'score' apply lm to input text, 'sample' samples output based on input text.  -src  string  (required) Source sequences to sample/score.  -output  string  (default:  output.txt ) Output file depend on  mode .", 
            "title": "Data options"
        }, 
        {
            "location": "/options/lm/#lm-options", 
            "text": "-model  string  (required) Path to the serialized model file.  -batch_size  number  (default:  30 ) Batch size.  -max_length  number  (default:  100 ) Maximal length of sentences in sample mode.  -temperature  number  (default:  1 ) For  sample  mode, higher temperatures cause the model to take more chances and increase diversity of results, but at a cost of more mistakes.", 
            "title": "LM options"
        }, 
        {
            "location": "/options/lm/#cuda-options", 
            "text": "-gpuid  table  (default:  0 ) List of GPU identifiers (1-indexed). CPU is used when set to 0.  -fallback_to_cpu [ boolean ]  (default:  false ) If GPU can't be used, rollback on the CPU.  -fp16 [ boolean ]  (default:  false ) Use half-precision float on GPU.  -no_nccl [ boolean ]  (default:  false ) Disable usage of nccl in parallel mode.", 
            "title": "Cuda options"
        }, 
        {
            "location": "/options/lm/#logger-options", 
            "text": "-log_file  string  (default:  '' ) Output logs to a file under this path instead of stdout.  -disable_logs [ boolean ]  (default:  false ) If set, output nothing.  -log_level  string  (accepted:  DEBUG ,  INFO ,  WARNING ,  ERROR ,  NOERROR ; default:  INFO ) Output logs at this level and above.", 
            "title": "Logger options"
        }, 
        {
            "location": "/options/lm/#other-options", 
            "text": "-time [ boolean ]  (default:  false ) Measure average translation time.", 
            "title": "Other options"
        }, 
        {
            "location": "/options/build_vocab/", 
            "text": "build_vocab.lua\n options:\n\n\n\n\n-h [\nboolean\n]\n (default: \nfalse\n)\nThis help.\n\n\n-md [\nboolean\n]\n (default: \nfalse\n)\nDump help in Markdown format.\n\n\n-config \nstring\n (default: \n''\n)\nLoad options from this file.\n\n\n-save_config \nstring\n (default: \n''\n)\nSave options to this file.\n\n\n\n\nVocabulary options\n\n\n\n\n-data \nstring\n (required)\nData file.\n\n\n-save_vocab \nstring\n (required)\nVocabulary dictionary prefix.\n\n\n-vocab_size \ntable\n (default: \n50000\n)\nList of source vocabularies size: \nword[ feat1[ feat2[ ...] ] ]\n. If = 0, vocabularies are not pruned.\n\n\n-words_min_frequency \ntable\n (default: \n0\n)\nList of source words min frequency: \nword[ feat1[ feat2[ ...] ] ]\n. If = 0, vocabularies are pruned by size.\n\n\n-keep_frequency [\nboolean\n]\n (default: \nfalse\n)\nKeep frequency of words in dictionary.\n\n\n-idx_files [\nboolean\n]\n (default: \nfalse\n)\nIf set, each line of the data file starts with a first field which is the index of the sentence.\n\n\n\n\nLogger options\n\n\n\n\n-log_file \nstring\n (default: \n''\n)\nOutput logs to a file under this path instead of stdout.\n\n\n-disable_logs [\nboolean\n]\n (default: \nfalse\n)\nIf set, output nothing.\n\n\n-log_level \nstring\n (accepted: \nDEBUG\n, \nINFO\n, \nWARNING\n, \nERROR\n, \nNOERROR\n; default: \nINFO\n)\nOutput logs at this level and above.", 
            "title": "tools/build_vocab.lua"
        }, 
        {
            "location": "/options/build_vocab/#vocabulary-options", 
            "text": "-data  string  (required) Data file.  -save_vocab  string  (required) Vocabulary dictionary prefix.  -vocab_size  table  (default:  50000 ) List of source vocabularies size:  word[ feat1[ feat2[ ...] ] ] . If = 0, vocabularies are not pruned.  -words_min_frequency  table  (default:  0 ) List of source words min frequency:  word[ feat1[ feat2[ ...] ] ] . If = 0, vocabularies are pruned by size.  -keep_frequency [ boolean ]  (default:  false ) Keep frequency of words in dictionary.  -idx_files [ boolean ]  (default:  false ) If set, each line of the data file starts with a first field which is the index of the sentence.", 
            "title": "Vocabulary options"
        }, 
        {
            "location": "/options/build_vocab/#logger-options", 
            "text": "-log_file  string  (default:  '' ) Output logs to a file under this path instead of stdout.  -disable_logs [ boolean ]  (default:  false ) If set, output nothing.  -log_level  string  (accepted:  DEBUG ,  INFO ,  WARNING ,  ERROR ,  NOERROR ; default:  INFO ) Output logs at this level and above.", 
            "title": "Logger options"
        }, 
        {
            "location": "/options/release_model/", 
            "text": "release_model.lua\n options:\n\n\n\n\n-h [\nboolean\n]\n (default: \nfalse\n)\nThis help.\n\n\n-md [\nboolean\n]\n (default: \nfalse\n)\nDump help in Markdown format.\n\n\n-config \nstring\n (default: \n''\n)\nLoad options from this file.\n\n\n-save_config \nstring\n (default: \n''\n)\nSave options to this file.\n\n\n\n\nModel options\n\n\n\n\n-model \nstring\n (required)\nPath to the trained model to release.\n\n\n-output_model \nstring\n (default: \n''\n)\nPath the released model. If not set, the \nrelease\n suffix will be automatically added to the model filename.\n\n\n-force [\nboolean\n]\n (default: \nfalse\n)\nForce output model creation even if the target file exists.\n\n\n\n\nCuda options\n\n\n\n\n-gpuid \ntable\n (default: \n0\n)\nList of GPU identifiers (1-indexed). CPU is used when set to 0.\n\n\n-fallback_to_cpu [\nboolean\n]\n (default: \nfalse\n)\nIf GPU can't be used, rollback on the CPU.\n\n\n-fp16 [\nboolean\n]\n (default: \nfalse\n)\nUse half-precision float on GPU.\n\n\n-no_nccl [\nboolean\n]\n (default: \nfalse\n)\nDisable usage of nccl in parallel mode.\n\n\n\n\nLogger options\n\n\n\n\n-log_file \nstring\n (default: \n''\n)\nOutput logs to a file under this path instead of stdout.\n\n\n-disable_logs [\nboolean\n]\n (default: \nfalse\n)\nIf set, output nothing.\n\n\n-log_level \nstring\n (accepted: \nDEBUG\n, \nINFO\n, \nWARNING\n, \nERROR\n, \nNOERROR\n; default: \nINFO\n)\nOutput logs at this level and above.", 
            "title": "tools/release_model.lua"
        }, 
        {
            "location": "/options/release_model/#model-options", 
            "text": "-model  string  (required) Path to the trained model to release.  -output_model  string  (default:  '' ) Path the released model. If not set, the  release  suffix will be automatically added to the model filename.  -force [ boolean ]  (default:  false ) Force output model creation even if the target file exists.", 
            "title": "Model options"
        }, 
        {
            "location": "/options/release_model/#cuda-options", 
            "text": "-gpuid  table  (default:  0 ) List of GPU identifiers (1-indexed). CPU is used when set to 0.  -fallback_to_cpu [ boolean ]  (default:  false ) If GPU can't be used, rollback on the CPU.  -fp16 [ boolean ]  (default:  false ) Use half-precision float on GPU.  -no_nccl [ boolean ]  (default:  false ) Disable usage of nccl in parallel mode.", 
            "title": "Cuda options"
        }, 
        {
            "location": "/options/release_model/#logger-options", 
            "text": "-log_file  string  (default:  '' ) Output logs to a file under this path instead of stdout.  -disable_logs [ boolean ]  (default:  false ) If set, output nothing.  -log_level  string  (accepted:  DEBUG ,  INFO ,  WARNING ,  ERROR ,  NOERROR ; default:  INFO ) Output logs at this level and above.", 
            "title": "Logger options"
        }, 
        {
            "location": "/options/tokenize/", 
            "text": "tokenize.lua\n options:\n\n\n\n\n-h [\nboolean\n]\n (default: \nfalse\n)\nThis help.\n\n\n-md [\nboolean\n]\n (default: \nfalse\n)\nDump help in Markdown format.\n\n\n-config \nstring\n (default: \n''\n)\nLoad options from this file.\n\n\n-save_config \nstring\n (default: \n''\n)\nSave options to this file.\n\n\n\n\nTokenizer options\n\n\n\n\n-mode \nstring\n (accepted: \nspace\n, \nconservative\n, \naggressive\n; default: \nconservative\n)\nDefine how aggressive should the tokenization be. \naggressive\n only keeps sequences of letters/numbers, \nconservative\n allows a mix of alphanumeric as in: \"2,000\", \"E65\", \"soft-landing\", etc. \nspace\n is doing space tokenization.\n\n\n-joiner_annotate [\nboolean\n]\n (default: \nfalse\n)\nInclude joiner annotation using \n-joiner\n character.\n\n\n-joiner \nstring\n (default: \n\uffed\n)\nCharacter used to annotate joiners.\n\n\n-joiner_new [\nboolean\n]\n (default: \nfalse\n)\nIn \n-joiner_annotate\n mode, \n-joiner\n is an independent token.\n\n\n-case_feature [\nboolean\n]\n (default: \nfalse\n)\nGenerate case feature.\n\n\n-segment_case [\nboolean\n]\n (default: \nfalse\n)\nSegment case feature, splits AbC to Ab C to be able to restore case\n\n\n-segment_alphabet \ntable\n (accepted: \nBuhid\n, \nGreek\n, \nHangul\n, \nOgham\n, \nThaana\n, \nTamil\n, \nCherokee\n, \nKhmer\n, \nDevanagari\n, \nThai\n, \nOriya\n, \nArmenian\n, \nKatakana\n, \nLimbu\n, \nArabic\n, \nBengali\n, \nLatin\n, \nMongolian\n, \nCyrillic\n, \nHiragana\n, \nKangxi\n, \nYi\n, \nTagbanwa\n, \nGurmukhi\n, \nTagalog\n, \nSyriac\n, \nKanbun\n, \nHanunoo\n, \nGeorgian\n, \nKannada\n, \nMyanmar\n, \nTibetan\n, \nHebrew\n, \nHan\n, \nLao\n, \nTelugu\n, \nGujarati\n, \nSinhala\n, \nMalayalam\n, \nBopomofo\n, \nBraille\n, \nEthiopic\n)\nSegment all letters from indicated alphabet.\n\n\n-segment_numbers [\nboolean\n]\n (default: \nfalse\n)\nSegment numbers into single digits.\n\n\n-segment_alphabet_change [\nboolean\n]\n (default: \nfalse\n)\nSegment if alphabet change between 2 letters.\n\n\n-bpe_model \nstring\n (default: \n''\n)\nApply Byte Pair Encoding if the BPE model path is given. If the option is used, BPE related options will be overridden/set automatically if the BPE model specified by \n-bpe_model\n is learnt using \nlearn_bpe.lua\n.\n\n\n-bpe_EOT_marker \nstring\n (default: \n/w\n)\nMarker used to mark the End of Token while applying BPE in mode 'prefix' or 'both'.\n\n\n-bpe_BOT_marker \nstring\n (default: \nw\n)\nMarker used to mark the Beginning of Token while applying BPE in mode 'suffix' or 'both'.\n\n\n-bpe_case_insensitive [\nboolean\n]\n (default: \nfalse\n)\nApply BPE internally in lowercase, but still output the truecase units. This option will be overridden/set automatically if the BPE model specified by \n-bpe_model\n is learnt using \nlearn_bpe.lua\n.\n\n\n-bpe_mode \nstring\n (accepted: \nsuffix\n, \nprefix\n, \nboth\n, \nnone\n; default: \nsuffix\n)\nDefine the BPE mode. This option will be overridden/set automatically if the BPE model specified by \n-bpe_model\n is learnt using \nlearn_bpe.lua\n. \nprefix\n: append \n-bpe_BOT_marker\n to the begining of each word to learn prefix-oriented pair statistics; \nsuffix\n: append \n-bpe_EOT_marker\n to the end of each word to learn suffix-oriented pair statistics, as in the original Python script; \nboth\n: \nsuffix\n and \nprefix\n; \nnone\n: no \nsuffix\n nor \nprefix\n.\n\n\n-normalize_cmd \nstring\n (default: \n''\n)\nCommand for on-the-fly corpus normalization. It should work in 'pipeline' mode.\n\n\n\n\nOther options\n\n\n\n\n-nparallel \nnumber\n (default: \n1\n)\nNumber of parallel thread to run the tokenization\n\n\n-batchsize \nnumber\n (default: \n1000\n)\nSize of each parallel batch - you should not change except if low memory", 
            "title": "tools/tokenize.lua"
        }, 
        {
            "location": "/options/tokenize/#tokenizer-options", 
            "text": "-mode  string  (accepted:  space ,  conservative ,  aggressive ; default:  conservative ) Define how aggressive should the tokenization be.  aggressive  only keeps sequences of letters/numbers,  conservative  allows a mix of alphanumeric as in: \"2,000\", \"E65\", \"soft-landing\", etc.  space  is doing space tokenization.  -joiner_annotate [ boolean ]  (default:  false ) Include joiner annotation using  -joiner  character.  -joiner  string  (default:  \uffed ) Character used to annotate joiners.  -joiner_new [ boolean ]  (default:  false ) In  -joiner_annotate  mode,  -joiner  is an independent token.  -case_feature [ boolean ]  (default:  false ) Generate case feature.  -segment_case [ boolean ]  (default:  false ) Segment case feature, splits AbC to Ab C to be able to restore case  -segment_alphabet  table  (accepted:  Buhid ,  Greek ,  Hangul ,  Ogham ,  Thaana ,  Tamil ,  Cherokee ,  Khmer ,  Devanagari ,  Thai ,  Oriya ,  Armenian ,  Katakana ,  Limbu ,  Arabic ,  Bengali ,  Latin ,  Mongolian ,  Cyrillic ,  Hiragana ,  Kangxi ,  Yi ,  Tagbanwa ,  Gurmukhi ,  Tagalog ,  Syriac ,  Kanbun ,  Hanunoo ,  Georgian ,  Kannada ,  Myanmar ,  Tibetan ,  Hebrew ,  Han ,  Lao ,  Telugu ,  Gujarati ,  Sinhala ,  Malayalam ,  Bopomofo ,  Braille ,  Ethiopic ) Segment all letters from indicated alphabet.  -segment_numbers [ boolean ]  (default:  false ) Segment numbers into single digits.  -segment_alphabet_change [ boolean ]  (default:  false ) Segment if alphabet change between 2 letters.  -bpe_model  string  (default:  '' ) Apply Byte Pair Encoding if the BPE model path is given. If the option is used, BPE related options will be overridden/set automatically if the BPE model specified by  -bpe_model  is learnt using  learn_bpe.lua .  -bpe_EOT_marker  string  (default:  /w ) Marker used to mark the End of Token while applying BPE in mode 'prefix' or 'both'.  -bpe_BOT_marker  string  (default:  w ) Marker used to mark the Beginning of Token while applying BPE in mode 'suffix' or 'both'.  -bpe_case_insensitive [ boolean ]  (default:  false ) Apply BPE internally in lowercase, but still output the truecase units. This option will be overridden/set automatically if the BPE model specified by  -bpe_model  is learnt using  learn_bpe.lua .  -bpe_mode  string  (accepted:  suffix ,  prefix ,  both ,  none ; default:  suffix ) Define the BPE mode. This option will be overridden/set automatically if the BPE model specified by  -bpe_model  is learnt using  learn_bpe.lua .  prefix : append  -bpe_BOT_marker  to the begining of each word to learn prefix-oriented pair statistics;  suffix : append  -bpe_EOT_marker  to the end of each word to learn suffix-oriented pair statistics, as in the original Python script;  both :  suffix  and  prefix ;  none : no  suffix  nor  prefix .  -normalize_cmd  string  (default:  '' ) Command for on-the-fly corpus normalization. It should work in 'pipeline' mode.", 
            "title": "Tokenizer options"
        }, 
        {
            "location": "/options/tokenize/#other-options", 
            "text": "-nparallel  number  (default:  1 ) Number of parallel thread to run the tokenization  -batchsize  number  (default:  1000 ) Size of each parallel batch - you should not change except if low memory", 
            "title": "Other options"
        }, 
        {
            "location": "/options/learn_bpe/", 
            "text": "learn_bpe.lua\n options:\n\n\n\n\n-h [\nboolean\n]\n (default: \nfalse\n)\nThis help.\n\n\n-md [\nboolean\n]\n (default: \nfalse\n)\nDump help in Markdown format.\n\n\n-config \nstring\n (default: \n''\n)\nLoad options from this file.\n\n\n-save_config \nstring\n (default: \n''\n)\nSave options to this file.\n\n\n\n\nBPE options\n\n\n\n\n-size \nstring\n (default: \n30000\n)\nThe number of merge operations to learn.\n\n\n-bpe_mode \nstring\n (accepted: \nsuffix\n, \nprefix\n, \nboth\n, \nnone\n; default: \nsuffix\n)\nDefine the BPE mode. \nprefix\n: append \nw\n to the begining of each word to learn prefix-oriented pair statistics; \nsuffix\n: append \n/w\n to the end of each word to learn suffix-oriented pair statistics, as in the original Python script; \nboth\n: \nsuffix\n and \nprefix\n; \nnone\n: no \nsuffix\n nor \nprefix\n.\n\n\n-save_bpe \nstring\n (required)\nPath to save the output model.\n\n\n\n\nTokenizer options\n\n\n\n\n-tok_mode \nstring\n (accepted: \nconservative\n, \naggressive\n, \nspace\n; default: \nspace\n)\nDefine how aggressive should the tokenization be. \nspace\n is space-tokenization.\n\n\n-tok_joiner_annotate [\nboolean\n]\n (default: \nfalse\n)\nInclude joiner annotation using \n-joiner\n character.\n\n\n-tok_joiner \nstring\n (default: \n\uffed\n)\nCharacter used to annotate joiners.\n\n\n-tok_joiner_new [\nboolean\n]\n (default: \nfalse\n)\nIn \n-joiner_annotate\n mode, \n-joiner\n is an independent token.\n\n\n-tok_case_feature [\nboolean\n]\n (default: \nfalse\n)\nGenerate case feature.\n\n\n-tok_segment_case [\nboolean\n]\n (default: \nfalse\n)\nSegment case feature, splits AbC to Ab C to be able to restore case\n\n\n-tok_segment_alphabet \ntable\n (accepted: \nTagbanwa\n, \nGurmukhi\n, \nThaana\n, \nCyrillic\n, \nMongolian\n, \nHanunoo\n, \nHan\n, \nSinhala\n, \nGreek\n, \nKanbun\n, \nDevanagari\n, \nBengali\n, \nLao\n, \nSyriac\n, \nTamil\n, \nLimbu\n, \nKhmer\n, \nKangxi\n, \nTibetan\n, \nArabic\n, \nEthiopic\n, \nMalayalam\n, \nTagalog\n, \nOgham\n, \nHangul\n, \nGeorgian\n, \nMyanmar\n, \nBraille\n, \nYi\n, \nBuhid\n, \nThai\n, \nKannada\n, \nTelugu\n, \nKatakana\n, \nArmenian\n, \nLatin\n, \nCherokee\n, \nHiragana\n, \nHebrew\n, \nBopomofo\n, \nGujarati\n, \nOriya\n)\nSegment all letters from indicated alphabet.\n\n\n-tok_segment_numbers [\nboolean\n]\n (default: \nfalse\n)\nSegment numbers into single digits.\n\n\n-tok_segment_alphabet_change [\nboolean\n]\n (default: \nfalse\n)\nSegment if alphabet change between 2 letters.\n\n\n-tok_normalize_cmd \nstring\n (default: \n''\n)\nCommand for on-the-fly corpus normalization. It should work in 'pipeline' mode.\n\n\n\n\nLogger options\n\n\n\n\n-log_file \nstring\n (default: \n''\n)\nOutput logs to a file under this path instead of stdout.\n\n\n-disable_logs [\nboolean\n]\n (default: \nfalse\n)\nIf set, output nothing.\n\n\n-log_level \nstring\n (accepted: \nDEBUG\n, \nINFO\n, \nWARNING\n, \nERROR\n, \nNOERROR\n; default: \nINFO\n)\nOutput logs at this level and above.", 
            "title": "tools/learn_bpe.lua"
        }, 
        {
            "location": "/options/learn_bpe/#bpe-options", 
            "text": "-size  string  (default:  30000 ) The number of merge operations to learn.  -bpe_mode  string  (accepted:  suffix ,  prefix ,  both ,  none ; default:  suffix ) Define the BPE mode.  prefix : append  w  to the begining of each word to learn prefix-oriented pair statistics;  suffix : append  /w  to the end of each word to learn suffix-oriented pair statistics, as in the original Python script;  both :  suffix  and  prefix ;  none : no  suffix  nor  prefix .  -save_bpe  string  (required) Path to save the output model.", 
            "title": "BPE options"
        }, 
        {
            "location": "/options/learn_bpe/#tokenizer-options", 
            "text": "-tok_mode  string  (accepted:  conservative ,  aggressive ,  space ; default:  space ) Define how aggressive should the tokenization be.  space  is space-tokenization.  -tok_joiner_annotate [ boolean ]  (default:  false ) Include joiner annotation using  -joiner  character.  -tok_joiner  string  (default:  \uffed ) Character used to annotate joiners.  -tok_joiner_new [ boolean ]  (default:  false ) In  -joiner_annotate  mode,  -joiner  is an independent token.  -tok_case_feature [ boolean ]  (default:  false ) Generate case feature.  -tok_segment_case [ boolean ]  (default:  false ) Segment case feature, splits AbC to Ab C to be able to restore case  -tok_segment_alphabet  table  (accepted:  Tagbanwa ,  Gurmukhi ,  Thaana ,  Cyrillic ,  Mongolian ,  Hanunoo ,  Han ,  Sinhala ,  Greek ,  Kanbun ,  Devanagari ,  Bengali ,  Lao ,  Syriac ,  Tamil ,  Limbu ,  Khmer ,  Kangxi ,  Tibetan ,  Arabic ,  Ethiopic ,  Malayalam ,  Tagalog ,  Ogham ,  Hangul ,  Georgian ,  Myanmar ,  Braille ,  Yi ,  Buhid ,  Thai ,  Kannada ,  Telugu ,  Katakana ,  Armenian ,  Latin ,  Cherokee ,  Hiragana ,  Hebrew ,  Bopomofo ,  Gujarati ,  Oriya ) Segment all letters from indicated alphabet.  -tok_segment_numbers [ boolean ]  (default:  false ) Segment numbers into single digits.  -tok_segment_alphabet_change [ boolean ]  (default:  false ) Segment if alphabet change between 2 letters.  -tok_normalize_cmd  string  (default:  '' ) Command for on-the-fly corpus normalization. It should work in 'pipeline' mode.", 
            "title": "Tokenizer options"
        }, 
        {
            "location": "/options/learn_bpe/#logger-options", 
            "text": "-log_file  string  (default:  '' ) Output logs to a file under this path instead of stdout.  -disable_logs [ boolean ]  (default:  false ) If set, output nothing.  -log_level  string  (accepted:  DEBUG ,  INFO ,  WARNING ,  ERROR ,  NOERROR ; default:  INFO ) Output logs at this level and above.", 
            "title": "Logger options"
        }, 
        {
            "location": "/options/server/", 
            "text": "translation_server.lua\n options:\n\n\n\n\n-h [\nboolean\n]\n (default: \nfalse\n)\nThis help.\n\n\n-md [\nboolean\n]\n (default: \nfalse\n)\nDump help in Markdown format.\n\n\n-config \nstring\n (default: \n''\n)\nLoad options from this file.\n\n\n-save_config \nstring\n (default: \n''\n)\nSave options to this file.\n\n\n\n\nServer options\n\n\n\n\n-host \nstring\n (default: \n127.0.0.1\n)\nHost to run the server on.\n\n\n-port \nstring\n (default: \n5556\n)\nPort to run the server on.\n\n\n\n\nTranslator options\n\n\n\n\n-model \nstring\n (default: \n''\n)\nPath to the serialized model file.\n\n\n-lm_model \nstring\n (default: \n''\n)\nPath to serialized language model file.\n\n\n-lm_weight \nnumber\n (default: \n0.1\n)\nRelative weight of language model.\n\n\n-beam_size \nnumber\n (default: \n5\n)\nBeam size.\n\n\n-max_sent_length \nnumber\n (default: \n250\n)\nMaximum output sentence length.\n\n\n-replace_unk [\nboolean\n]\n (default: \nfalse\n)\nReplace the generated \n tokens with the source token that has the highest attention weight. If \n-phrase_table\n is provided, it will lookup the identified source token and give the corresponding target token. If it is not provided (or the identified source token does not exist in the table) then it will copy the source token\n\n\n-replace_unk_tagged [\nboolean\n]\n (default: \nfalse\n)\nThe same as -replace_unk, but wrap the replaced token in \uff5funk:xxxxx\uff60 if it is not found in the phrase table.\n\n\n-phrase_table \nstring\n (default: \n''\n)\nPath to source-target dictionary to replace \nunk\n tokens.\n\n\n-n_best \nnumber\n (default: \n1\n)\nIf \n 1, it will also output an n-best list of decoded sentences.\n\n\n-max_num_unks \nnumber\n (default: \ninf\n)\nAll sequences with more \nunk\ns than this will be ignored during beam search.\n\n\n-target_subdict \nstring\n (default: \n''\n)\nPath to target words dictionary corresponding to the source.\n\n\n-pre_filter_factor \nnumber\n (default: \n1\n)\nOptional, set this only if filter is being used. Before applying filters, hypotheses with top \nbeam_size * pre_filter_factor\n scores will be considered. If the returned hypotheses voilate filters, then set this to a larger value to consider more.\n\n\n-length_norm \nnumber\n (default: \n0\n)\nLength normalization coefficient (alpha). If set to 0, no length normalization.\n\n\n-coverage_norm \nnumber\n (default: \n0\n)\nCoverage normalization coefficient (beta). An extra coverage term multiplied by beta is added to hypotheses scores. If is set to 0, no coverage normalization.\n\n\n-eos_norm \nnumber\n (default: \n0\n)\nEnd of sentence normalization coefficient (gamma). If set to 0, no EOS normalization.\n\n\n-dump_input_encoding [\nboolean\n]\n (default: \nfalse\n)\nInstead of generating target tokens conditional on the source tokens, we print the representation (encoding/embedding) of the input.\n\n\n-save_beam_to \nstring\n (default: \n''\n)\nPath to a file where the beam search exploration will be saved in a JSON format. Requires the \ndkjson\n package.\n\n\n\n\nCuda options\n\n\n\n\n-gpuid \ntable\n (default: \n0\n)\nList of GPU identifiers (1-indexed). CPU is used when set to 0.\n\n\n-fallback_to_cpu [\nboolean\n]\n (default: \nfalse\n)\nIf GPU can't be used, rollback on the CPU.\n\n\n-fp16 [\nboolean\n]\n (default: \nfalse\n)\nUse half-precision float on GPU.\n\n\n-no_nccl [\nboolean\n]\n (default: \nfalse\n)\nDisable usage of nccl in parallel mode.\n\n\n\n\nLogger options\n\n\n\n\n-log_file \nstring\n (default: \n''\n)\nOutput logs to a file under this path instead of stdout.\n\n\n-disable_logs [\nboolean\n]\n (default: \nfalse\n)\nIf set, output nothing.\n\n\n-log_level \nstring\n (accepted: \nDEBUG\n, \nINFO\n, \nWARNING\n, \nERROR\n, \nNOERROR\n; default: \nINFO\n)\nOutput logs at this level and above.", 
            "title": "tools/translation_server.lua"
        }, 
        {
            "location": "/options/server/#server-options", 
            "text": "-host  string  (default:  127.0.0.1 ) Host to run the server on.  -port  string  (default:  5556 ) Port to run the server on.", 
            "title": "Server options"
        }, 
        {
            "location": "/options/server/#translator-options", 
            "text": "-model  string  (default:  '' ) Path to the serialized model file.  -lm_model  string  (default:  '' ) Path to serialized language model file.  -lm_weight  number  (default:  0.1 ) Relative weight of language model.  -beam_size  number  (default:  5 ) Beam size.  -max_sent_length  number  (default:  250 ) Maximum output sentence length.  -replace_unk [ boolean ]  (default:  false ) Replace the generated   tokens with the source token that has the highest attention weight. If  -phrase_table  is provided, it will lookup the identified source token and give the corresponding target token. If it is not provided (or the identified source token does not exist in the table) then it will copy the source token  -replace_unk_tagged [ boolean ]  (default:  false ) The same as -replace_unk, but wrap the replaced token in \uff5funk:xxxxx\uff60 if it is not found in the phrase table.  -phrase_table  string  (default:  '' ) Path to source-target dictionary to replace  unk  tokens.  -n_best  number  (default:  1 ) If   1, it will also output an n-best list of decoded sentences.  -max_num_unks  number  (default:  inf ) All sequences with more  unk s than this will be ignored during beam search.  -target_subdict  string  (default:  '' ) Path to target words dictionary corresponding to the source.  -pre_filter_factor  number  (default:  1 ) Optional, set this only if filter is being used. Before applying filters, hypotheses with top  beam_size * pre_filter_factor  scores will be considered. If the returned hypotheses voilate filters, then set this to a larger value to consider more.  -length_norm  number  (default:  0 ) Length normalization coefficient (alpha). If set to 0, no length normalization.  -coverage_norm  number  (default:  0 ) Coverage normalization coefficient (beta). An extra coverage term multiplied by beta is added to hypotheses scores. If is set to 0, no coverage normalization.  -eos_norm  number  (default:  0 ) End of sentence normalization coefficient (gamma). If set to 0, no EOS normalization.  -dump_input_encoding [ boolean ]  (default:  false ) Instead of generating target tokens conditional on the source tokens, we print the representation (encoding/embedding) of the input.  -save_beam_to  string  (default:  '' ) Path to a file where the beam search exploration will be saved in a JSON format. Requires the  dkjson  package.", 
            "title": "Translator options"
        }, 
        {
            "location": "/options/server/#cuda-options", 
            "text": "-gpuid  table  (default:  0 ) List of GPU identifiers (1-indexed). CPU is used when set to 0.  -fallback_to_cpu [ boolean ]  (default:  false ) If GPU can't be used, rollback on the CPU.  -fp16 [ boolean ]  (default:  false ) Use half-precision float on GPU.  -no_nccl [ boolean ]  (default:  false ) Disable usage of nccl in parallel mode.", 
            "title": "Cuda options"
        }, 
        {
            "location": "/options/server/#logger-options", 
            "text": "-log_file  string  (default:  '' ) Output logs to a file under this path instead of stdout.  -disable_logs [ boolean ]  (default:  false ) If set, output nothing.  -log_level  string  (accepted:  DEBUG ,  INFO ,  WARNING ,  ERROR ,  NOERROR ; default:  INFO ) Output logs at this level and above.", 
            "title": "Logger options"
        }, 
        {
            "location": "/options/rest_server/", 
            "text": "rest_translation_server.lua\n options:\n\n\n\n\n-h [\nboolean\n]\n (default: \nfalse\n)\nThis help.\n\n\n-md [\nboolean\n]\n (default: \nfalse\n)\nDump help in Markdown format.\n\n\n-config \nstring\n (default: \n''\n)\nLoad options from this file.\n\n\n-save_config \nstring\n (default: \n''\n)\nSave options to this file.\n\n\n\n\nServer options\n\n\n\n\n-host \nstring\n (default: \n127.0.0.1\n)\nHost to run the server on.\n\n\n-port \nstring\n (default: \n7784\n)\nPort to run the server on.\n\n\n-withAttn [\nboolean\n]\n (default: \nfalse\n)\nIf set returns by default attn vector.\n\n\n\n\nTranslator options\n\n\n\n\n-model \nstring\n (default: \n''\n)\nPath to the serialized model file.\n\n\n-lm_model \nstring\n (default: \n''\n)\nPath to serialized language model file.\n\n\n-lm_weight \nnumber\n (default: \n0.1\n)\nRelative weight of language model.\n\n\n-beam_size \nnumber\n (default: \n5\n)\nBeam size.\n\n\n-max_sent_length \nnumber\n (default: \n250\n)\nMaximum output sentence length.\n\n\n-replace_unk [\nboolean\n]\n (default: \nfalse\n)\nReplace the generated \n tokens with the source token that has the highest attention weight. If \n-phrase_table\n is provided, it will lookup the identified source token and give the corresponding target token. If it is not provided (or the identified source token does not exist in the table) then it will copy the source token\n\n\n-replace_unk_tagged [\nboolean\n]\n (default: \nfalse\n)\nThe same as -replace_unk, but wrap the replaced token in \uff5funk:xxxxx\uff60 if it is not found in the phrase table.\n\n\n-phrase_table \nstring\n (default: \n''\n)\nPath to source-target dictionary to replace \nunk\n tokens.\n\n\n-n_best \nnumber\n (default: \n1\n)\nIf \n 1, it will also output an n-best list of decoded sentences.\n\n\n-max_num_unks \nnumber\n (default: \ninf\n)\nAll sequences with more \nunk\ns than this will be ignored during beam search.\n\n\n-target_subdict \nstring\n (default: \n''\n)\nPath to target words dictionary corresponding to the source.\n\n\n-pre_filter_factor \nnumber\n (default: \n1\n)\nOptional, set this only if filter is being used. Before applying filters, hypotheses with top \nbeam_size * pre_filter_factor\n scores will be considered. If the returned hypotheses voilate filters, then set this to a larger value to consider more.\n\n\n-length_norm \nnumber\n (default: \n0\n)\nLength normalization coefficient (alpha). If set to 0, no length normalization.\n\n\n-coverage_norm \nnumber\n (default: \n0\n)\nCoverage normalization coefficient (beta). An extra coverage term multiplied by beta is added to hypotheses scores. If is set to 0, no coverage normalization.\n\n\n-eos_norm \nnumber\n (default: \n0\n)\nEnd of sentence normalization coefficient (gamma). If set to 0, no EOS normalization.\n\n\n-dump_input_encoding [\nboolean\n]\n (default: \nfalse\n)\nInstead of generating target tokens conditional on the source tokens, we print the representation (encoding/embedding) of the input.\n\n\n-save_beam_to \nstring\n (default: \n''\n)\nPath to a file where the beam search exploration will be saved in a JSON format. Requires the \ndkjson\n package.\n\n\n\n\nCuda options\n\n\n\n\n-gpuid \ntable\n (default: \n0\n)\nList of GPU identifiers (1-indexed). CPU is used when set to 0.\n\n\n-fallback_to_cpu [\nboolean\n]\n (default: \nfalse\n)\nIf GPU can't be used, rollback on the CPU.\n\n\n-fp16 [\nboolean\n]\n (default: \nfalse\n)\nUse half-precision float on GPU.\n\n\n-no_nccl [\nboolean\n]\n (default: \nfalse\n)\nDisable usage of nccl in parallel mode.\n\n\n\n\nLogger options\n\n\n\n\n-log_file \nstring\n (default: \n''\n)\nOutput logs to a file under this path instead of stdout.\n\n\n-disable_logs [\nboolean\n]\n (default: \nfalse\n)\nIf set, output nothing.\n\n\n-log_level \nstring\n (accepted: \nDEBUG\n, \nINFO\n, \nWARNING\n, \nERROR\n, \nNOERROR\n; default: \nINFO\n)\nOutput logs at this level and above.\n\n\n\n\nTokenizer options\n\n\n\n\n-mode \nstring\n (accepted: \nspace\n, \nconservative\n, \naggressive\n; default: \nconservative\n)\nDefine how aggressive should the tokenization be. \naggressive\n only keeps sequences of letters/numbers, \nconservative\n allows a mix of alphanumeric as in: \"2,000\", \"E65\", \"soft-landing\", etc. \nspace\n is doing space tokenization.\n\n\n-joiner_annotate [\nboolean\n]\n (default: \nfalse\n)\nInclude joiner annotation using \n-joiner\n character.\n\n\n-joiner \nstring\n (default: \n\uffed\n)\nCharacter used to annotate joiners.\n\n\n-joiner_new [\nboolean\n]\n (default: \nfalse\n)\nIn \n-joiner_annotate\n mode, \n-joiner\n is an independent token.\n\n\n-case_feature [\nboolean\n]\n (default: \nfalse\n)\nGenerate case feature.\n\n\n-segment_case [\nboolean\n]\n (default: \nfalse\n)\nSegment case feature, splits AbC to Ab C to be able to restore case\n\n\n-segment_alphabet \ntable\n (accepted: \nTagalog\n, \nHanunoo\n, \nLimbu\n, \nYi\n, \nHebrew\n, \nLatin\n, \nDevanagari\n, \nThaana\n, \nLao\n, \nSinhala\n, \nGeorgian\n, \nKannada\n, \nCherokee\n, \nKanbun\n, \nBuhid\n, \nMalayalam\n, \nHan\n, \nThai\n, \nKatakana\n, \nTelugu\n, \nGreek\n, \nMyanmar\n, \nArmenian\n, \nHangul\n, \nCyrillic\n, \nEthiopic\n, \nTagbanwa\n, \nGurmukhi\n, \nOgham\n, \nKhmer\n, \nArabic\n, \nOriya\n, \nHiragana\n, \nMongolian\n, \nKangxi\n, \nSyriac\n, \nGujarati\n, \nBraille\n, \nBengali\n, \nTamil\n, \nBopomofo\n, \nTibetan\n)\nSegment all letters from indicated alphabet.\n\n\n-segment_alphabet_change [\nboolean\n]\n (default: \nfalse\n)\nSegment if alphabet change between 2 letters.\n\n\n-bpe_model \nstring\n (default: \n''\n)\nApply Byte Pair Encoding if the BPE model path is given. If the option is used, BPE related options will be overridden/set automatically if the BPE model specified by \n-bpe_model\n is learnt using \nlearn_bpe.lua\n.\n\n\n-bpe_EOT_marker \nstring\n (default: \n/w\n)\nMarker used to mark the End of Token while applying BPE in mode 'prefix' or 'both'.\n\n\n-bpe_BOT_marker \nstring\n (default: \nw\n)\nMarker used to mark the Beginning of Token while applying BPE in mode 'suffix' or 'both'.\n\n\n-bpe_case_insensitive [\nboolean\n]\n (default: \nfalse\n)\nApply BPE internally in lowercase, but still output the truecase units. This option will be overridden/set automatically if the BPE model specified by \n-bpe_model\n is learnt using \nlearn_bpe.lua\n.\n\n\n-bpe_mode \nstring\n (accepted: \nsuffix\n, \nprefix\n, \nboth\n, \nnone\n; default: \nsuffix\n)\nDefine the BPE mode. This option will be overridden/set automatically if the BPE model specified by \n-bpe_model\n is learnt using \nlearn_bpe.lua\n. \nprefix\n: append \n-bpe_BOT_marker\n to the begining of each word to learn prefix-oriented pair statistics; \nsuffix\n: append \n-bpe_EOT_marker\n to the end of each word to learn suffix-oriented pair statistics, as in the original Python script; \nboth\n: \nsuffix\n and \nprefix\n; \nnone\n: no \nsuffix\n nor \nprefix\n.\n\n\n\n\nOther options\n\n\n\n\n-batchsize \nnumber\n (default: \n1000\n)\nSize of each parallel batch - you should not change except if low memory.", 
            "title": "tools/rest_translation_server.lua"
        }, 
        {
            "location": "/options/rest_server/#server-options", 
            "text": "-host  string  (default:  127.0.0.1 ) Host to run the server on.  -port  string  (default:  7784 ) Port to run the server on.  -withAttn [ boolean ]  (default:  false ) If set returns by default attn vector.", 
            "title": "Server options"
        }, 
        {
            "location": "/options/rest_server/#translator-options", 
            "text": "-model  string  (default:  '' ) Path to the serialized model file.  -lm_model  string  (default:  '' ) Path to serialized language model file.  -lm_weight  number  (default:  0.1 ) Relative weight of language model.  -beam_size  number  (default:  5 ) Beam size.  -max_sent_length  number  (default:  250 ) Maximum output sentence length.  -replace_unk [ boolean ]  (default:  false ) Replace the generated   tokens with the source token that has the highest attention weight. If  -phrase_table  is provided, it will lookup the identified source token and give the corresponding target token. If it is not provided (or the identified source token does not exist in the table) then it will copy the source token  -replace_unk_tagged [ boolean ]  (default:  false ) The same as -replace_unk, but wrap the replaced token in \uff5funk:xxxxx\uff60 if it is not found in the phrase table.  -phrase_table  string  (default:  '' ) Path to source-target dictionary to replace  unk  tokens.  -n_best  number  (default:  1 ) If   1, it will also output an n-best list of decoded sentences.  -max_num_unks  number  (default:  inf ) All sequences with more  unk s than this will be ignored during beam search.  -target_subdict  string  (default:  '' ) Path to target words dictionary corresponding to the source.  -pre_filter_factor  number  (default:  1 ) Optional, set this only if filter is being used. Before applying filters, hypotheses with top  beam_size * pre_filter_factor  scores will be considered. If the returned hypotheses voilate filters, then set this to a larger value to consider more.  -length_norm  number  (default:  0 ) Length normalization coefficient (alpha). If set to 0, no length normalization.  -coverage_norm  number  (default:  0 ) Coverage normalization coefficient (beta). An extra coverage term multiplied by beta is added to hypotheses scores. If is set to 0, no coverage normalization.  -eos_norm  number  (default:  0 ) End of sentence normalization coefficient (gamma). If set to 0, no EOS normalization.  -dump_input_encoding [ boolean ]  (default:  false ) Instead of generating target tokens conditional on the source tokens, we print the representation (encoding/embedding) of the input.  -save_beam_to  string  (default:  '' ) Path to a file where the beam search exploration will be saved in a JSON format. Requires the  dkjson  package.", 
            "title": "Translator options"
        }, 
        {
            "location": "/options/rest_server/#cuda-options", 
            "text": "-gpuid  table  (default:  0 ) List of GPU identifiers (1-indexed). CPU is used when set to 0.  -fallback_to_cpu [ boolean ]  (default:  false ) If GPU can't be used, rollback on the CPU.  -fp16 [ boolean ]  (default:  false ) Use half-precision float on GPU.  -no_nccl [ boolean ]  (default:  false ) Disable usage of nccl in parallel mode.", 
            "title": "Cuda options"
        }, 
        {
            "location": "/options/rest_server/#logger-options", 
            "text": "-log_file  string  (default:  '' ) Output logs to a file under this path instead of stdout.  -disable_logs [ boolean ]  (default:  false ) If set, output nothing.  -log_level  string  (accepted:  DEBUG ,  INFO ,  WARNING ,  ERROR ,  NOERROR ; default:  INFO ) Output logs at this level and above.", 
            "title": "Logger options"
        }, 
        {
            "location": "/options/rest_server/#tokenizer-options", 
            "text": "-mode  string  (accepted:  space ,  conservative ,  aggressive ; default:  conservative ) Define how aggressive should the tokenization be.  aggressive  only keeps sequences of letters/numbers,  conservative  allows a mix of alphanumeric as in: \"2,000\", \"E65\", \"soft-landing\", etc.  space  is doing space tokenization.  -joiner_annotate [ boolean ]  (default:  false ) Include joiner annotation using  -joiner  character.  -joiner  string  (default:  \uffed ) Character used to annotate joiners.  -joiner_new [ boolean ]  (default:  false ) In  -joiner_annotate  mode,  -joiner  is an independent token.  -case_feature [ boolean ]  (default:  false ) Generate case feature.  -segment_case [ boolean ]  (default:  false ) Segment case feature, splits AbC to Ab C to be able to restore case  -segment_alphabet  table  (accepted:  Tagalog ,  Hanunoo ,  Limbu ,  Yi ,  Hebrew ,  Latin ,  Devanagari ,  Thaana ,  Lao ,  Sinhala ,  Georgian ,  Kannada ,  Cherokee ,  Kanbun ,  Buhid ,  Malayalam ,  Han ,  Thai ,  Katakana ,  Telugu ,  Greek ,  Myanmar ,  Armenian ,  Hangul ,  Cyrillic ,  Ethiopic ,  Tagbanwa ,  Gurmukhi ,  Ogham ,  Khmer ,  Arabic ,  Oriya ,  Hiragana ,  Mongolian ,  Kangxi ,  Syriac ,  Gujarati ,  Braille ,  Bengali ,  Tamil ,  Bopomofo ,  Tibetan ) Segment all letters from indicated alphabet.  -segment_alphabet_change [ boolean ]  (default:  false ) Segment if alphabet change between 2 letters.  -bpe_model  string  (default:  '' ) Apply Byte Pair Encoding if the BPE model path is given. If the option is used, BPE related options will be overridden/set automatically if the BPE model specified by  -bpe_model  is learnt using  learn_bpe.lua .  -bpe_EOT_marker  string  (default:  /w ) Marker used to mark the End of Token while applying BPE in mode 'prefix' or 'both'.  -bpe_BOT_marker  string  (default:  w ) Marker used to mark the Beginning of Token while applying BPE in mode 'suffix' or 'both'.  -bpe_case_insensitive [ boolean ]  (default:  false ) Apply BPE internally in lowercase, but still output the truecase units. This option will be overridden/set automatically if the BPE model specified by  -bpe_model  is learnt using  learn_bpe.lua .  -bpe_mode  string  (accepted:  suffix ,  prefix ,  both ,  none ; default:  suffix ) Define the BPE mode. This option will be overridden/set automatically if the BPE model specified by  -bpe_model  is learnt using  learn_bpe.lua .  prefix : append  -bpe_BOT_marker  to the begining of each word to learn prefix-oriented pair statistics;  suffix : append  -bpe_EOT_marker  to the end of each word to learn suffix-oriented pair statistics, as in the original Python script;  both :  suffix  and  prefix ;  none : no  suffix  nor  prefix .", 
            "title": "Tokenizer options"
        }, 
        {
            "location": "/options/rest_server/#other-options", 
            "text": "-batchsize  number  (default:  1000 ) Size of each parallel batch - you should not change except if low memory.", 
            "title": "Other options"
        }, 
        {
            "location": "/options/embeddings/", 
            "text": "embeddings.lua\n options:\n\n\n\n\n-h [\nboolean\n]\n (default: \nfalse\n)\nThis help.\n\n\n-md [\nboolean\n]\n (default: \nfalse\n)\nDump help in Markdown format.\n\n\n-config \nstring\n (default: \n''\n)\nLoad options from this file.\n\n\n-save_config \nstring\n (default: \n''\n)\nSave options to this file.\n\n\n\n\nData options\n\n\n\n\n-dict_file \nstring\n (required)\nPath to outputted dict file from \npreprocess.lua\n.\n\n\n-embed_file \nstring\n (default: \n''\n)\nPath to the embedding file. Ignored if \n-lang\n is used.\n\n\n-save_data \nstring\n (required)\nOutput file path/label.\n\n\n-save_unknown_dict \nstring\n (default: \n''\n)\nPath to file for saving vocabs not found in embedding.\n\n\n\n\nEmbedding options\n\n\n\n\n-lang \nstring\n (default: \n''\n)\nWikipedia Language Code to autoload embeddings.\n\n\n-embed_type \nstring\n (accepted: \nword2vec-bin\n, \nword2vec-txt\n, \nglove\n; default: \nword2vec-bin\n)\nEmbeddings file origin. Ignored if \n-lang\n is used.\n\n\n-normalize [\nboolean\n]\n (default: \ntrue\n)\nBoolean to normalize the word vectors, or not.\n\n\n-approximate [\nboolean\n]\n (default: \nfalse\n)\nIf set, will also look for variants (case, joiner annotate) to match dictionary and word embedding.\n\n\n-report_every \nnumber\n (default: \n100000\n)\nPrint stats every this many lines read from embedding file.\n\n\n\n\nLogger options\n\n\n\n\n-log_file \nstring\n (default: \n''\n)\nOutput logs to a file under this path instead of stdout.\n\n\n-disable_logs [\nboolean\n]\n (default: \nfalse\n)\nIf set, output nothing.\n\n\n-log_level \nstring\n (accepted: \nDEBUG\n, \nINFO\n, \nWARNING\n, \nERROR\n, \nNOERROR\n; default: \nINFO\n)\nOutput logs at this level and above.", 
            "title": "tools/embeddings.lua"
        }, 
        {
            "location": "/options/embeddings/#data-options", 
            "text": "-dict_file  string  (required) Path to outputted dict file from  preprocess.lua .  -embed_file  string  (default:  '' ) Path to the embedding file. Ignored if  -lang  is used.  -save_data  string  (required) Output file path/label.  -save_unknown_dict  string  (default:  '' ) Path to file for saving vocabs not found in embedding.", 
            "title": "Data options"
        }, 
        {
            "location": "/options/embeddings/#embedding-options", 
            "text": "-lang  string  (default:  '' ) Wikipedia Language Code to autoload embeddings.  -embed_type  string  (accepted:  word2vec-bin ,  word2vec-txt ,  glove ; default:  word2vec-bin ) Embeddings file origin. Ignored if  -lang  is used.  -normalize [ boolean ]  (default:  true ) Boolean to normalize the word vectors, or not.  -approximate [ boolean ]  (default:  false ) If set, will also look for variants (case, joiner annotate) to match dictionary and word embedding.  -report_every  number  (default:  100000 ) Print stats every this many lines read from embedding file.", 
            "title": "Embedding options"
        }, 
        {
            "location": "/options/embeddings/#logger-options", 
            "text": "-log_file  string  (default:  '' ) Output logs to a file under this path instead of stdout.  -disable_logs [ boolean ]  (default:  false ) If set, output nothing.  -log_level  string  (accepted:  DEBUG ,  INFO ,  WARNING ,  ERROR ,  NOERROR ; default:  INFO ) Output logs at this level and above.", 
            "title": "Logger options"
        }, 
        {
            "location": "/options/average_models/", 
            "text": "average_models.lua\n options:\n\n\n\n\n-h [\nboolean\n]\n (default: \nfalse\n)\nThis help.\n\n\n-md [\nboolean\n]\n (default: \nfalse\n)\nDump help in Markdown format.\n\n\n-config \nstring\n (default: \n''\n)\nLoad options from this file.\n\n\n-save_config \nstring\n (default: \n''\n)\nSave options to this file.\n\n\n\n\nModel options\n\n\n\n\n-models \ntable\nPath to models to average.\n\n\n-output_model \nstring\n (default: \nfinal_averaged.t7\n)\nPath to the averaged model.\n\n\n-force [\nboolean\n]\n (default: \nfalse\n)\nForce output model creation even if the target file exists.\n\n\n\n\nCuda options\n\n\n\n\n-gpuid \ntable\n (default: \n0\n)\nList of GPU identifiers (1-indexed). CPU is used when set to 0.\n\n\n-fallback_to_cpu [\nboolean\n]\n (default: \nfalse\n)\nIf GPU can't be used, rollback on the CPU.\n\n\n-fp16 [\nboolean\n]\n (default: \nfalse\n)\nUse half-precision float on GPU.\n\n\n-no_nccl [\nboolean\n]\n (default: \nfalse\n)\nDisable usage of nccl in parallel mode.\n\n\n\n\nLogger options\n\n\n\n\n-log_file \nstring\n (default: \n''\n)\nOutput logs to a file under this path instead of stdout.\n\n\n-disable_logs [\nboolean\n]\n (default: \nfalse\n)\nIf set, output nothing.\n\n\n-log_level \nstring\n (accepted: \nDEBUG\n, \nINFO\n, \nWARNING\n, \nERROR\n, \nNOERROR\n; default: \nINFO\n)\nOutput logs at this level and above.", 
            "title": "tools/average_models.lua"
        }, 
        {
            "location": "/options/average_models/#model-options", 
            "text": "-models  table Path to models to average.  -output_model  string  (default:  final_averaged.t7 ) Path to the averaged model.  -force [ boolean ]  (default:  false ) Force output model creation even if the target file exists.", 
            "title": "Model options"
        }, 
        {
            "location": "/options/average_models/#cuda-options", 
            "text": "-gpuid  table  (default:  0 ) List of GPU identifiers (1-indexed). CPU is used when set to 0.  -fallback_to_cpu [ boolean ]  (default:  false ) If GPU can't be used, rollback on the CPU.  -fp16 [ boolean ]  (default:  false ) Use half-precision float on GPU.  -no_nccl [ boolean ]  (default:  false ) Disable usage of nccl in parallel mode.", 
            "title": "Cuda options"
        }, 
        {
            "location": "/options/average_models/#logger-options", 
            "text": "-log_file  string  (default:  '' ) Output logs to a file under this path instead of stdout.  -disable_logs [ boolean ]  (default:  false ) If set, output nothing.  -log_level  string  (accepted:  DEBUG ,  INFO ,  WARNING ,  ERROR ,  NOERROR ; default:  INFO ) Output logs at this level and above.", 
            "title": "Logger options"
        }, 
        {
            "location": "/lua_python_comparison/", 
            "text": "This page lists all the differences between commandline options of \nOpenNMT\n (lua version) and \nOpenNMT-py\n (python version). Identical options are not described.\n\n\ntokenize.lua\n, \nlearn_bpe.lua\n\n\nThere is no python version yet of tokenization/bpe scripts.\n\n\npreprocess.(lua|py)\n\n\nOpenNMT-lua reference option list for \npreprocess.lua\n is documented \nhere\n.\n\n\nCommandline options differences:\n\n\n\n\n\n\n\n\n\n\nOpenNMT-lua\n\n\nOpenNMT-py\n\n\n\n\n\n\n\n\n\n\nconfig\nsave_config\n\n\n\n\nnot supported\n\n\n\n\n\n\nsrc_img_dir\n\n\nnot supported\n\n\nused for \nsrc_type img\n\n\n\n\n\n\ndata_type\n\n\nbitext\n, \nmonotext\n, \nfeattext\n - used for ASR and LM models\n\n\ntext\n, \nimg\n (*)\n\n\n\n\n\n\nsrc_seq_length_trunc\n\n\nnot supported\n\n\nif sentence too long, truncate it\n\n\n\n\n\n\ntgt_seq_length_trunc\n\n\nnot supported\n\n\nif sentence too long, truncate it\n\n\n\n\n\n\ncheck_plength\n\n\ncheck alignment of source/target for sequence tagging\n\n\nnot supported\n\n\n\n\n\n\ntime_shift_features\n\n\nshift feature by one timestep\n\n\nnot supported\n\n\n\n\n\n\nkeep_frequency\n\n\nused by sampled softmax\n\n\nnot supported\n\n\n\n\n\n\nsort\n\n\n\n\nnot optional\n: can not be disabled\n\n\n\n\n\n\nidx_files\n\n\nfor \nfeattext\n - provide format to align source\ntarget alignment\n\n\nnot supported\n\n\n\n\n\n\nLogging Options\n\n\n\n\n\n\n\n\n\n\nlog_file\ndisable_logs\nlog_level\n\n\n\n\nnot supported\n\n\n\n\n\n\n\n\n(*) Lua implementation of \nim2text\n is in independent repository \nhere\n.\n\n\ntrain.(lua|py)\n\n\nOpenNMT-lua reference option list for \npreprocess.lua\n is documented \nhere\n.\n\n\nCommandline options differences:\n\n\n\n\n\n\n\n\n\n\nOpenNMT-lua\n\n\nOpenNMT-py\n\n\n\n\n\n\n\n\n\n\nconfig\nsave_config\n\n\n\n\nnot supported\n\n\n\n\n\n\nSampling Options\n\n\n\n\n\n\n\n\n\n\nsample\nsample_type\nsample_perplexity_init\nsample_perplexity_max\nsample_vocab\n\n\nSampled dataset options\n\n\nnot supported\n\n\n\n\n\n\nModel Options\n\n\n\n\n\n\n\n\n\n\nmodel_type\n\n\nlm\n, \nseq2seq\n, \nseqtagger\n\n\ntext\n, \nimg\n\n\n\n\n\n\nshare_decoder_embeddings\n\n\nnot supported\n\n\nshare the word and softmax embeddings for decoder\n\n\n\n\n\n\nuse_pos_emb\n\n\nadd positional embeddings to word embeddings\n\n\nnot supported\n\n\n\n\n\n\nmax_pos\n\n\nconnected to \nuse_pos_emb\n\n\nnot supported\n\n\n\n\n\n\nposition_encoding\n\n\nnot supported\n\n\nuse a sinusoid to mark relative words positions.\n\n\n\n\n\n\nfeat_merge\n\n\nconcat\n, \nsum\n\n\nconcat\n, \nsum\n, \nmlp\n\n\n\n\n\n\ndropout_input\n\n\nDropout applied to the input of the recurrent module\n\n\nnot supported\n\n\n\n\n\n\ndropout_words\n\n\nDropout applied to the full source sequence\n\n\nnot supported\n\n\n\n\n\n\ndropout_type\n\n\nnaive\n, \nvariational\n\n\nnot supported\n: dropout is \nnaive\n\n\n\n\n\n\nresidual\n\n\nAdd residual connections between recurrent layers\n\n\nnot supported\n\n\n\n\n\n\nbridge\n\n\ncopy\n, \ndense\n, \ndense_nonlinear\n\n\nnot supported\n\n\n\n\n\n\nencoder_type\n\n\nrnn\n, \nbrnn\n, \ndbrnn\n, \npdbrnn\n, \ngnmt\n, \ncnn\n\n\nrnn\n, \nmean\n, \ntransformer\n\n\n\n\n\n\ndecoder_layer\n\n\nnot supported\n\n\nrnn\n, \ntransformer\n\n\n\n\n\n\npdbrnn_reduction\npdbrnn_merge\n\n\nfor \nencoder_type\n set to \npdbrnn\n\n\nnot supported\n\n\n\n\n\n\ncnn_layers\ncnn_kernel\ncnn_size\n\n\nfor \nencoder_type\n set to \ncnn\n\n\nnot supported\n\n\n\n\n\n\ntruncated_decoder\n\n\nnot supported\n\n\ntruncated back propagation through time\n\n\n\n\n\n\nAttention Options\n\n\n\n\n\n\n\n\n\n\nattention\n\n\nnone\n, \nglobal\n\n\nnot supported\n: only global attention\n\n\n\n\n\n\nglobal_attention\n\n\ngeneral\n, \ndot\n, \nconcat\n\n\ndot\n, \ngeneral\n, \nmlp\n\n\n\n\n\n\ncopy_attn\n\n\nnot supported\n\n\ncopy attention layer\n\n\n\n\n\n\ncoverage_attn\nlambda_coverage\n\n\nnot supported\n\n\ncoverage attention layer\n\n\n\n\n\n\ncontext_gate\n\n\nnot supported\n\n\nsource\n, \ntarget\n, \nboth\n\n\n\n\n\n\nTraining Options\n\n\n\n\n\n\n\n\n\n\nasync_parallel\nasync_parallel_minbatch\n\n\nAsync multi-gpu training\n\n\nnot supported\n\n\n\n\n\n\nstart_iteration\n\n\n\n\nnot supported\n\n\n\n\n\n\nend_epoch\n\n\nfinal epoch of the training\n\n\nnot supported\n: see \nepochs\n\n\n\n\n\n\nepochs\n\n\nnot supported\n: see \nend_epoch\n\n\nnumber of training epochs\n\n\n\n\n\n\nvalidation_metric\n\n\nperplexity\n, \nloss\n, \nbleu\n, \nter\n, \ndlratio\n\n\nnot supported\n always perplexity\n\n\n\n\n\n\nsave_validation_translation_every\n\n\n\n\nnot supported\n\n\n\n\n\n\nOptim Options\n\n\n\n\n\n\n\n\n\n\nmax_batch_size\n\n\nmaximum batch size\n\n\nnot supported\n: see \nmax_generator_batches\n\n\n\n\n\n\nuneven_batches\n\n\n\n\n\n\n\n\n\n\nmax_generator_batches\n\n\nnot supported\n\n\n\n\n\n\n\n\nmin_learning_rate\n\n\ndo not continue the training past this learning rate value\n\n\nnot supported\n\n\n\n\n\n\nstart_decay_score_delta\n\n\n\n\nnot supported\n\n\n\n\n\n\ndecay\n\n\ndefault, epoch_only, score_only\n\n\nnot supported\n\n\n\n\n\n\ndecay_method\n\n\n\n\nuse a custom learning rate decay (?)\n\n\n\n\n\n\nwarmup_steps\n\n\nnot supported\n\n\nnumber of warmup steps for custom decay\n\n\n\n\n\n\nSaver Options\n\n\n\n\n\n\n\n\n\n\ncontinue\n\n\n\n\nnot supported\n\n\n\n\n\n\nstart_checkpoint_at\n\n\nnot supported\n\n\n\n\n\n\n\n\nsave_every\n\n\n\n\nnot supported\n\n\n\n\n\n\nsave_every_epochs\n\n\n\n\nnot supported\n\n\n\n\n\n\nLogging Options\n\n\n\n\n\n\n\n\n\n\nlog_file\ndisable_logs\nlog_level\n\n\n\n\nnot supported\n\n\n\n\n\n\nexp_port\n\n\nport of the Crayon server\n\n\nnot supported\n: default port 8889 is used\n\n\n\n\n\n\nGPU Options\n\n\n\n\n\n\n\n\n\n\nfp16\n\n\nhalf-float precision for GPU\n\n\nnot supported\n\n\n\n\n\n\nfallback_to_cpu\n\n\n\n\nnot supported\n\n\n\n\n\n\nno_nccl\n\n\n\n\nnot supported", 
            "title": "Lua and Python OpenNMT"
        }, 
        {
            "location": "/lua_python_comparison/#tokenizelua-learn_bpelua", 
            "text": "There is no python version yet of tokenization/bpe scripts.", 
            "title": "tokenize.lua, learn_bpe.lua"
        }, 
        {
            "location": "/lua_python_comparison/#preprocessluapy", 
            "text": "OpenNMT-lua reference option list for  preprocess.lua  is documented  here .  Commandline options differences:      OpenNMT-lua  OpenNMT-py      config save_config   not supported    src_img_dir  not supported  used for  src_type img    data_type  bitext ,  monotext ,  feattext  - used for ASR and LM models  text ,  img  (*)    src_seq_length_trunc  not supported  if sentence too long, truncate it    tgt_seq_length_trunc  not supported  if sentence too long, truncate it    check_plength  check alignment of source/target for sequence tagging  not supported    time_shift_features  shift feature by one timestep  not supported    keep_frequency  used by sampled softmax  not supported    sort   not optional : can not be disabled    idx_files  for  feattext  - provide format to align source target alignment  not supported    Logging Options      log_file disable_logs log_level   not supported     (*) Lua implementation of  im2text  is in independent repository  here .", 
            "title": "preprocess.(lua|py)"
        }, 
        {
            "location": "/lua_python_comparison/#trainluapy", 
            "text": "OpenNMT-lua reference option list for  preprocess.lua  is documented  here .  Commandline options differences:      OpenNMT-lua  OpenNMT-py      config save_config   not supported    Sampling Options      sample sample_type sample_perplexity_init sample_perplexity_max sample_vocab  Sampled dataset options  not supported    Model Options      model_type  lm ,  seq2seq ,  seqtagger  text ,  img    share_decoder_embeddings  not supported  share the word and softmax embeddings for decoder    use_pos_emb  add positional embeddings to word embeddings  not supported    max_pos  connected to  use_pos_emb  not supported    position_encoding  not supported  use a sinusoid to mark relative words positions.    feat_merge  concat ,  sum  concat ,  sum ,  mlp    dropout_input  Dropout applied to the input of the recurrent module  not supported    dropout_words  Dropout applied to the full source sequence  not supported    dropout_type  naive ,  variational  not supported : dropout is  naive    residual  Add residual connections between recurrent layers  not supported    bridge  copy ,  dense ,  dense_nonlinear  not supported    encoder_type  rnn ,  brnn ,  dbrnn ,  pdbrnn ,  gnmt ,  cnn  rnn ,  mean ,  transformer    decoder_layer  not supported  rnn ,  transformer    pdbrnn_reduction pdbrnn_merge  for  encoder_type  set to  pdbrnn  not supported    cnn_layers cnn_kernel cnn_size  for  encoder_type  set to  cnn  not supported    truncated_decoder  not supported  truncated back propagation through time    Attention Options      attention  none ,  global  not supported : only global attention    global_attention  general ,  dot ,  concat  dot ,  general ,  mlp    copy_attn  not supported  copy attention layer    coverage_attn lambda_coverage  not supported  coverage attention layer    context_gate  not supported  source ,  target ,  both    Training Options      async_parallel async_parallel_minbatch  Async multi-gpu training  not supported    start_iteration   not supported    end_epoch  final epoch of the training  not supported : see  epochs    epochs  not supported : see  end_epoch  number of training epochs    validation_metric  perplexity ,  loss ,  bleu ,  ter ,  dlratio  not supported  always perplexity    save_validation_translation_every   not supported    Optim Options      max_batch_size  maximum batch size  not supported : see  max_generator_batches    uneven_batches      max_generator_batches  not supported     min_learning_rate  do not continue the training past this learning rate value  not supported    start_decay_score_delta   not supported    decay  default, epoch_only, score_only  not supported    decay_method   use a custom learning rate decay (?)    warmup_steps  not supported  number of warmup steps for custom decay    Saver Options      continue   not supported    start_checkpoint_at  not supported     save_every   not supported    save_every_epochs   not supported    Logging Options      log_file disable_logs log_level   not supported    exp_port  port of the Crayon server  not supported : default port 8889 is used    GPU Options      fp16  half-float precision for GPU  not supported    fallback_to_cpu   not supported    no_nccl   not supported", 
            "title": "train.(lua|py)"
        }, 
        {
            "location": "/references/", 
            "text": "This is the list of papers, OpenNMT has been inspired on:\n\n\n\n\nLuong, M. T., Pham, H., \n Manning, C. D. (2015). \nEffective approaches to attention-based neural machine translation\n. arXiv preprint arXiv:1508.04025.\n\n\nSennrich, R., \n Haddow, B. (2016). \nLinguistic input features improve neural machine translation\n. arXiv preprint arXiv:1606.02892.\n\n\nSennrich, R., Haddow, B., \n Birch, A. (2015). \nNeural machine translation of rare words with subword units\n. arXiv preprint arXiv:1508.07909.\n\n\nWu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., ... \n Klingner, J. (2016). \nGoogle's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation\n. arXiv preprint arXiv:1609.08144.\n\n\nJean, S., Cho, K., Memisevic, R., Bengio, Y. (2015). \nOn Using Very Large Target Vocabulary for Neural Machine Translation\n. ACL 2015\n\n\nGehring, J., Auli, M., Grangier D., Dauphin Y. N. (2017). \nA Convolutional Encoder Model for Neural Machine Translation\n. arXiv preprint arXiv:1611.02344.\n\n\nBengio, S., Vinyals, O., Jaitly, N., \n Shazeer, N. (2015). \nScheduled sampling for sequence prediction with recurrent neural networks\n. In Advances in Neural Information Processing Systems (pp. 1171-1179).\n\n\nGulcehre, C., Firat, O., Xu, K., Cho, K., Barrault, L., Lin, H. C., ... \n Bengio, Y. (2015). \nOn using monolingual corpora in neural machine translation\n. arXiv preprint arXiv:1503.03535.", 
            "title": "References"
        }, 
        {
            "location": "/issues/", 
            "text": "luajit: out of memory\n\n\nThis most likely happened when training a model with long sequences and the LuaJIT memory limit was reached. You will need to switch to Lua 5.2 instead.\n\n\nTHCudaCheck FAIL [...]: out of memory\n\n\nThis means your model was too large to fit on the available GPU memory.\n\n\nTo work around this error during training, follow these steps in order and stop when the training no more fails:\n\n\n\n\nPrefix your command line with \nTHC_CACHING_ALLOCATOR=0\n\n\nReduce the \n-max_batch_size\n value (64 by default)\n\n\nReduce the \n-src_seq_length\n and \n-tgt_seq_length\n values during the preprocessing\n\n\nReduce your model size (\n-layers\n, \n-rnn_size\n, etc.)\n\n\n\n\nunknown Torch class \ntorch.CudaTensor>\n\n\nThis means you wanted to load a GPU model but did not use the \n-gpuid\n option to define which GPU to use.", 
            "title": "Common issues"
        }, 
        {
            "location": "/issues/#luajit-out-of-memory", 
            "text": "This most likely happened when training a model with long sequences and the LuaJIT memory limit was reached. You will need to switch to Lua 5.2 instead.", 
            "title": "luajit: out of memory"
        }, 
        {
            "location": "/issues/#thcudacheck-fail-out-of-memory", 
            "text": "This means your model was too large to fit on the available GPU memory.  To work around this error during training, follow these steps in order and stop when the training no more fails:   Prefix your command line with  THC_CACHING_ALLOCATOR=0  Reduce the  -max_batch_size  value (64 by default)  Reduce the  -src_seq_length  and  -tgt_seq_length  values during the preprocessing  Reduce your model size ( -layers ,  -rnn_size , etc.)", 
            "title": "THCudaCheck FAIL [...]: out of memory"
        }, 
        {
            "location": "/issues/#unknown-torch-class-torchcudatensor62", 
            "text": "This means you wanted to load a GPU model but did not use the  -gpuid  option to define which GPU to use.", 
            "title": "unknown Torch class &lt;torch.CudaTensor>"
        }
    ]
}