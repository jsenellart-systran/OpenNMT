<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../../img/favicon.ico">
  <title>Data sampling - OpenNMT</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../../css/highlight.css">
  <link href="../../css/extra.css" rel="stylesheet">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Data sampling";
    var mkdocs_page_input_path = "training/sampling.md";
    var mkdocs_page_url = "/training/sampling/";
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js"></script>
  <script src="../../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../../js/highlight.pack.js"></script> 
  
  <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-89222039-1', 'opennmt.net');
      ga('send', 'pageview');
  </script>
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> OpenNMT</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="../..">Overview</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../installation/">Installation</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../quickstart/">Quickstart</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../applications/">Applications</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Data</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../data/preparation/">Preparation</a>
                </li>
                <li class="">
                    
    <a class="" href="../../data/word_features/">Word features</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Training</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../models/">Models</a>
                </li>
                <li class="">
                    
    <a class="" href="../embeddings/">Embeddings</a>
                </li>
                <li class="">
                    
    <a class="" href="../logs/">Logs</a>
                </li>
                <li class="">
                    
    <a class="" href="../multi_gpu/">Multi GPU</a>
                </li>
                <li class="">
                    
    <a class="" href="../retraining/">Retraining</a>
                </li>
                <li class="">
                    
    <a class="" href="../regularization/">Regularization</a>
                </li>
                <li class="">
                    
    <a class="" href="../decay/">Decay strategies</a>
                </li>
                <li class=" current">
                    
    <a class="current" href="./">Data sampling</a>
    <ul class="subnav">
            
    <li class="toctree-l3"><a href="#importance-sampling">Importance Sampling</a></li>
    

    <li class="toctree-l3"><a href="#memory-sampling">Memory Sampling</a></li>
    
        <ul>
        
            <li><a class="toctree-l4" href="#uniform">Uniform</a></li>
        
            <li><a class="toctree-l4" href="#perplexity-based">Perplexity-based</a></li>
        
            <li><a class="toctree-l4" href="#partition">Partition</a></li>
        
        </ul>
    

    <li class="toctree-l3"><a href="#file-sampling">File Sampling</a></li>
    
        <ul>
        
            <li><a class="toctree-l4" href="#dynamic-dataset">Dynamic Dataset</a></li>
        
            <li><a class="toctree-l4" href="#sampling-distribution-rules">Sampling distribution rules</a></li>
        
        </ul>
    

    </ul>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Translation</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../translation/inference/">Inference</a>
                </li>
                <li class="">
                    
    <a class="" href="../../translation/beam_search/">Beam search</a>
                </li>
                <li class="">
                    
    <a class="" href="../../translation/unknowns/">Unknown words</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Tools</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../tools/tokenization/">Tokenization</a>
                </li>
                <li class="">
                    
    <a class="" href="../../tools/scorer/">Scorer</a>
                </li>
                <li class="">
                    
    <a class="" href="../../tools/servers/">Servers</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Reference: Options</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../options/usage/">Scripts usage</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/preprocess/">preprocess.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/train/">train.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/translate/">translate.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/tag/">tag.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/lm/">lm.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/build_vocab/">tools/build_vocab.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/release_model/">tools/release_model.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/tokenize/">tools/tokenize.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/learn_bpe/">tools/learn_bpe.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/server/">tools/translation_server.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/rest_server/">tools/rest_translation_server.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/embeddings/">tools/embeddings.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/average_models/">tools/average_models.lua</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../lua_python_comparison/">Lua and Python OpenNMT</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../references/">References</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../issues/">Common issues</a>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">OpenNMT</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
        
          <li>Training &raquo;</li>
        
      
    
    <li>Data sampling</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/OpenNMT/OpenNMT/edit/master/docs/training/sampling.md"> Edit on OpenNMT/OpenNMT</a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <p>Data sampling is a technique to select a subset of the training set at each epoch. This could be a way to make the epoch unit smaller or select relevant training sequences at each epoch. This is also necessary for working on very large dataset - where the full data does not need to be loaded in memory for each epoch. There are two implementations for sampling: <em>memory sampling</em> and <em>file sampling</em>.</p>
<p>Both implementations also support vocabulary sampling (also called <em>Importance Sampling</em>).</p>
<h2 id="importance-sampling">Importance Sampling<a class="headerlink" href="#importance-sampling" title="Permanent link">&para;</a></h2>
<p>When sampling, with the option <code>-sample_vocab</code> it is also possible to restrict the generated vocabulary to the current sample which gives an approximate of the full softmax as defined here <a href="http://www.aclweb.org/anthology/P15-1001">Jean et al, 2015</a> via an "importance sampling" approach.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Importance sampling is particularly useful when training systems with very large output vocabulary for faster computation.</p>
</div>
<h2 id="memory-sampling">Memory Sampling<a class="headerlink" href="#memory-sampling" title="Permanent link">&para;</a></h2>
<p><em>Memory Sampling</em> is enabled using <code>-sample N</code> option where (N) is the number of sequences to select at each epoch. There are different methods for selecting these (N) sentences corresponding to the <code>-sample_type</code> option: <code>uniform</code> (default), <code>perplexity</code> or <code>partition</code>.</p>
<h3 id="uniform">Uniform<a class="headerlink" href="#uniform" title="Permanent link">&para;</a></h3>
<p>The simplest data sampling is to uniformly select a subset of the training data. Using the <code>-sample N</code> option, the training will randomly choose (N) training sequences at each epoch.</p>
<p>A typical use case is to reduce the length of the epochs for more frequent learning rate updates and validation perplexity computation.</p>
<h3 id="perplexity-based">Perplexity-based<a class="headerlink" href="#perplexity-based" title="Permanent link">&para;</a></h3>
<p>This approach is an attempt to feed relevant training data at each epoch. When using the flag <code>-sample_type perplexity</code>, the perplexity of each sequence is used to generate a multinomial probability distribution over the training sequences. The higher the perplexity, the more likely the sequence is selected.</p>
<p>Alternatively, perplexity-based sampling can be enabled when an average training perplexity is met with the <code>-sample_perplexity_init</code> option.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This perplexity-based approach is experimental and effects are to be experimented. This also results in a ~10% slowdown as the perplexity of <strong>each</strong> sequence has to be independently computed.</p>
</div>
<h3 id="partition">Partition<a class="headerlink" href="#partition" title="Permanent link">&para;</a></h3>
<p>When using the flag <code>-sample_type partition</code>, samples are drawn without random, uniformally and incrementally from the corpus training. Use this mode for making sure all training sequences will be sent the same number of time.</p>
<h2 id="file-sampling">File Sampling<a class="headerlink" href="#file-sampling" title="Permanent link">&para;</a></h2>
<p><em>File Sampling</em> is enabled using <code>-gsample V</code> option: (V) is either an integer and in that case it represents the number of sentences to sample from the dataset, or a float values and in that case, it represents a relative size based on the full dataset size (e.g: 0.1 being 10%).</p>
<p>File Sampling can only be used with on-the-fly preprocessing and tokenization as an alternative to sequential tokenization, preprocessing, training - and this is refers as <em>Dynamic Dataset</em> below.</p>
<p>In <em>File Sampling</em>, the only available sampling method is uniform meaning that the sentences are selected uniformly in each corpus of the dataset. However, it is possible to modify the distribution of the sampling for the different files using sampling rule file as described below.</p>
<h3 id="dynamic-dataset">Dynamic Dataset<a class="headerlink" href="#dynamic-dataset" title="Permanent link">&para;</a></h3>
<p>It is possible to provide raw files directly to training script. For that, instead of using the <code>-data D</code> option, you have to use preprocessing data selection options (such as <code>-train_src</code>, <code>-train_tgt</code>, or <code>-training_dir</code> option). Note these modes are exclusive. Corpus can be pre-tokenized, or you can provide tokenization options for both source and target (or source only for language models) prefixing all tokenization options with <code>-tok_src_</code>, <code>-tok_tgt_</code> or <code>tok_</code>. For instance - the following commandlines use all the files from <code>baseline</code> directory with source <code>.en</code> suffix and target <code>.fr</code> suffix. The source is tokenized in aggressive mode, and is using case feature, while the target is tokenized in aggressive mode and limited to 30 words sequences.</p>
<div class="codehilite"><pre><span></span>th train.lua
  -train_dir baseline
  -src_suffix .en -tgt_suffix .fr
  -tok_src_mode aggressive -tok_src_case_feature
  -tok_tgt_mode aggressive
  -tgt_seq_length 30
  -save_model baseline
</pre></div>


<p>The available options are <code>preprocess.lua</code> options documented <a href="http://opennmt.net/OpenNMT/options/preprocess/">here</a> and <code>tokenize.lua</code> options documented <a href="http://opennmt.net/OpenNMT/options/tokenize/">here</a>.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>It is simpler (and faster) to use dynamic dataset associated with file sampling when working on corpus with more than 10 million sentences.</p>
</div>
<h3 id="sampling-distribution-rules">Sampling distribution rules<a class="headerlink" href="#sampling-distribution-rules" title="Permanent link">&para;</a></h3>
<p>When the set of training files is heterogenous, you can specify the proportion of each file using a distribution rule file specified with <code>-gsample_dist FILE</code> option.</p>
<p>The rule file is list of rule in each line applied. A rule is a <code>LuaPattern SPACE WEIGHT</code>. The first rule in the file matching (with <code>LuaPattern</code> a filename) is applied for the file. <code>LuaPattern</code> can be a lua regex (see <a href="https://www.lua.org/pil/20.2.html">https://www.lua.org/pil/20.2.html</a>) or <code>*</code> matching everything.</p>
<p>For instance, let us say you have the following files in your <code>train_dir</code> directory:</p>
<div class="codehilite"><pre><span></span>generic.src, generic.tgt
IT1.src, IT1.tgt
IT2.src, IT2.tgt
MSDN.src, MSDN.tgt
colloquial.src, colloquial.tgt
news.src, news.tgt
</pre></div>


<p>and using the following rules:</p>
<div class="codehilite"><pre><span></span>IT,MSDN 20
colloquial 10
generic 65
* 5
</pre></div>


<p>The following rules apply:</p>
<ul>
<li><code>generic 65</code> matches <code>generic.{src,tgt}</code></li>
<li><code>IT,MSDN 20</code> matches <code>IT{1,2}.{src,tgt}</code> and <code>MSDN.{src,tgt}</code></li>
<li><code>colloquial 10</code> matches <code>colloquial.{src,tgt}</code></li>
<li><code>* 5</code> maches <code>news.{src,tgt}</code></li>
</ul>
<p>The weights are dynamically normalized to 1. Here we will make sure that 65% of the sample will be composed of sentences from <code>generic.{src,tgt}</code> and only 20% from <code>IT{1,2}.{src,tgt}</code> and <code>MSDN.{src,tgt}</code>. To build the sample, the sampling preparation algorithm might oversample some of the corpus if is too small.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If one file could not be match by a rule, it would be completely excluded.</p>
</div>
<p>To test your distribution rules, it is possible to execute a dry run of the preprocessor:</p>
<div class="codehilite"><pre><span></span>th preprocess.lua -gsample_dist rules.txt -gsample <span class="m">100000</span> -train_dir data/ -dry_run
</pre></div>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../../translation/inference/" class="btn btn-neutral float-right" title="Inference">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../decay/" class="btn btn-neutral" title="Decay strategies"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>
    
  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../decay/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../../translation/inference/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script src="../../js/theme.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_HTMLorMML"></script>

</body>
</html>
