<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../../img/favicon.ico">
  <title>Tokenization - OpenNMT</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../../css/highlight.css">
  <link href="../../css/extra.css" rel="stylesheet">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Tokenization";
    var mkdocs_page_input_path = "tools/tokenization.md";
    var mkdocs_page_url = "/tools/tokenization/";
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js"></script>
  <script src="../../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../../js/highlight.pack.js"></script> 
  
  <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-89222039-1', 'opennmt.net');
      ga('send', 'pageview');
  </script>
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> OpenNMT</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="../..">Overview</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../installation/">Installation</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../quickstart/">Quickstart</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../applications/">Applications</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Data</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../data/preparation/">Preparation</a>
                </li>
                <li class="">
                    
    <a class="" href="../../data/word_features/">Word features</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Training</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../training/models/">Models</a>
                </li>
                <li class="">
                    
    <a class="" href="../../training/embeddings/">Embeddings</a>
                </li>
                <li class="">
                    
    <a class="" href="../../training/logs/">Logs</a>
                </li>
                <li class="">
                    
    <a class="" href="../../training/multi_gpu/">Multi GPU</a>
                </li>
                <li class="">
                    
    <a class="" href="../../training/retraining/">Retraining</a>
                </li>
                <li class="">
                    
    <a class="" href="../../training/regularization/">Regularization</a>
                </li>
                <li class="">
                    
    <a class="" href="../../training/decay/">Decay strategies</a>
                </li>
                <li class="">
                    
    <a class="" href="../../training/sampling/">Data sampling</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Translation</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../translation/inference/">Inference</a>
                </li>
                <li class="">
                    
    <a class="" href="../../translation/beam_search/">Beam search</a>
                </li>
                <li class="">
                    
    <a class="" href="../../translation/unknowns/">Unknown words</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Tools</span>
    <ul class="subnav">
                <li class=" current">
                    
    <a class="current" href="./">Tokenization</a>
    <ul class="subnav">
            
    <li class="toctree-l3"><a href="#normalization">Normalization</a></li>
    

    <li class="toctree-l3"><a href="#tokenization">Tokenization</a></li>
    

    <li class="toctree-l3"><a href="#detokenization">Detokenization</a></li>
    

    <li class="toctree-l3"><a href="#special-characters">Special characters</a></li>
    

    <li class="toctree-l3"><a href="#mixed-casing-words">Mixed casing words</a></li>
    

    <li class="toctree-l3"><a href="#alphabet-segmentation">Alphabet Segmentation</a></li>
    

    <li class="toctree-l3"><a href="#number-segmentation">Number Segmentation</a></li>
    

    <li class="toctree-l3"><a href="#bpe">BPE</a></li>
    

    </ul>
                </li>
                <li class="">
                    
    <a class="" href="../scorer/">Scorer</a>
                </li>
                <li class="">
                    
    <a class="" href="../servers/">Servers</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Reference: Options</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../options/usage/">Scripts usage</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/preprocess/">preprocess.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/train/">train.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/translate/">translate.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/tag/">tag.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/lm/">lm.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/build_vocab/">tools/build_vocab.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/release_model/">tools/release_model.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/tokenize/">tools/tokenize.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/learn_bpe/">tools/learn_bpe.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/server/">tools/translation_server.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/rest_server/">tools/rest_translation_server.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/embeddings/">tools/embeddings.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/average_models/">tools/average_models.lua</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../lua_python_comparison/">Lua and Python OpenNMT</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../references/">References</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../issues/">Common issues</a>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">OpenNMT</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
        
          <li>Tools &raquo;</li>
        
      
    
    <li>Tokenization</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/OpenNMT/OpenNMT/edit/master/docs/tools/tokenization.md"> Edit on OpenNMT/OpenNMT</a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <p>OpenNMT provides generic tokenization utilities to quickly process new training data. The goal of the tokenization is to convert raw sentences into sequences of tokens. In that process two main operations are performed in sequence:</p>
<ul>
<li>normalization - which applies some uniform transformation on the source sequences to identify and protect some specific sequences (for instance url), normalize characters (for instance all types of quotes, unicode variants) or even to normalize some variants (like dates) into unique representation simpler for the translation process</li>
<li>the tokenization itself - which transform the actual normalized sentence into a sequence of space-separated tokens together with possible features (case).</li>
</ul>
<h2 id="normalization">Normalization<a class="headerlink" href="#normalization" title="Permanent link">&para;</a></h2>
<p>Normalization is performed by user commandline tool which has to work in "pipeline" mode: sentences from standard input are normalized and produced on the standard output. For instance, the following python script is normalizing unicode representation (using NFC representation), turns French quotes <code>«»</code> into English quotes <code>“”</code>, and protect "hashtags" sequences:</p>
<div class="codehilite"><pre><span></span><span class="ch">#!/usr/bin/python</span>
<span class="c1"># -*- coding: utf-8 -*-</span>

<span class="kn">import</span> <span class="nn">unicodedata</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">sys</span>

<span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">sys</span><span class="o">.</span><span class="n">stdin</span><span class="p">:</span>
  <span class="n">line</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
  <span class="n">line</span> <span class="o">=</span> <span class="n">unicodedata</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="s1">&#39;NFC&#39;</span><span class="p">,</span> <span class="n">line</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s1">&#39;utf8&#39;</span><span class="p">))</span>
  <span class="n">line</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="sa">u</span><span class="s2">&quot;«&quot;</span><span class="p">,</span> <span class="sa">u</span><span class="s2">&quot;“&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="sa">u</span><span class="s2">&quot;»&quot;</span><span class="p">,</span> <span class="sa">u</span><span class="s2">&quot;”&quot;</span><span class="p">)</span>
  <span class="n">line</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s1">&#39;utf8&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;(^|[^S\w])#([A-Za-z0-9_]+)&#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\\</span><span class="s1">1｟#</span><span class="se">\\</span><span class="s1">2｠&#39;</span><span class="p">)</span>
  <span class="k">print</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
</pre></div>


<p>Normalization script is called as part of tokenization adding the option <code>-normalize_cmd "normalize.py"</code>.</p>
<h2 id="tokenization">Tokenization<a class="headerlink" href="#tokenization" title="Permanent link">&para;</a></h2>
<p>To tokenize a corpus:</p>
<div class="codehilite"><pre><span></span>th tools/tokenize.lua OPTIONS &lt; file &gt; file.tok
</pre></div>


<p>Available tokenization modes are defined <a href="http://opennmt.net/OpenNMT/options/tokenize/">here</a>.</p>
<p>In order to perform <code>detokenization</code>, tokenization can introduce a joiner annotation mark <code>￭</code>.</p>
<h2 id="detokenization">Detokenization<a class="headerlink" href="#detokenization" title="Permanent link">&para;</a></h2>
<p>If you activate <code>-joiner_annotate</code> marker, the tokenization is reversible. Just use:</p>
<div class="codehilite"><pre><span></span>th tools/detokenize.lua OPTIONS &lt; file.tok &gt; file.detok
</pre></div>


<h2 id="special-characters">Special characters<a class="headerlink" href="#special-characters" title="Permanent link">&para;</a></h2>
<ul>
<li><code>￨</code> (U+FFE8) is the feature separator symbol. If such character is used in source text, it is replaced by its non presentation form <code>│</code> (U+2502).</li>
<li><code>￭</code> (U+FFED) is the default joiner marker (generated in <code>-joiner_annotate marker</code> mode). If such character is used in source text, it is replaced by its non presentation form <code>■</code> (U+25A0)</li>
<li><code>｟...｠</code> (U+FF5F, U+FF60) are marking a sequence as protected - it won't be tokenized and its case feature is <code>N</code>.</li>
</ul>
<h2 id="mixed-casing-words">Mixed casing words<a class="headerlink" href="#mixed-casing-words" title="Permanent link">&para;</a></h2>
<p><code>-segment_case</code> feature enables tokenizer to segment words into subwords with one of 3 casing types (truecase ('House'), uppercase ('HOUSE') or lowercase ('house')), which helps  restore right casing during  detokenization. This feature is especially useful for texts with a signficant number of words with mixed casing ('WiFi' -&gt; 'Wi' and 'Fi').</p>
<div class="codehilite"><pre><span></span>WiFi --&gt; wi￨C fi￨C
TVs --&gt; tv￨U s￨L
</pre></div>


<h2 id="alphabet-segmentation">Alphabet Segmentation<a class="headerlink" href="#alphabet-segmentation" title="Permanent link">&para;</a></h2>
<p>Two options provide specific tokenization depending on alphabet:</p>
<ul>
<li><code>-segment_alphabet_change</code>: tokenize a sequence between two letters when their alphabets differ - for instance between a Latin alphabet character and a Han character.</li>
<li><code>-segment_alphabet Alphabet</code>: tokenize all words of the indicated alphabet into characters - for instance to split a chinese sentence into characters, use <code>-segment_alphabet Han</code>:</li>
</ul>
<div class="codehilite"><pre><span></span>君子之心不胜其小，而气量涵盖一世。 --&gt; 君 子 之 心 不 胜 其 小 ， 而 气 量 涵 盖 一 世 。
</pre></div>


<h2 id="number-segmentation">Number Segmentation<a class="headerlink" href="#number-segmentation" title="Permanent link">&para;</a></h2>
<p>The option <code>-segment_number</code> tokenizes numbers by digits. This option is interesting for full handling of numeric entities conversion/translation by neural networks.</p>
<div class="codehilite"><pre><span></span>1984 --&gt; 1 9 8 4
</pre></div>


<h2 id="bpe">BPE<a class="headerlink" href="#bpe" title="Permanent link">&para;</a></h2>
<p>OpenNMT's BPE module fully supports the <a href="https://github.com/rsennrich/subword-nmt">original BPE</a> as default mode:</p>
<div class="codehilite"><pre><span></span>tools/learn_bpe.lua -size <span class="m">30000</span> -save_bpe codes &lt; input_tokenized
tools/tokenize.lua -bpe_model codes &lt; input_tokenized
</pre></div>


<p>with three additional features:</p>
<p><strong>1. Accept raw text as input and use OpenNMT's tokenizer for pre-tokenization before BPE training</strong></p>
<div class="codehilite"><pre><span></span>tools/learn_bpe.lua -size <span class="m">30000</span> -save_bpe codes -tok_mode aggressive -tok_segment_alphabet_change <span class="o">[</span> OTHER_TOK_OPTIONS <span class="o">]</span> <span class="o">[</span> OTHER_BPE_TRAINING_OPTIONS <span class="o">]</span> &lt; input_raw
tools/tokenize.lua -bpe_model codes -mode aggressive -segment_alphabet_change <span class="o">[</span> SAME_TOK_OPTIONS <span class="o">]</span> <span class="o">[</span> OTHER_BPE_INFERENCE_OPTIONS <span class="o">]</span> &lt; input_raw
</pre></div>


<div class="admonition note">
<p class="admonition-title">Note</p>
<p>All TOK_OPTIONS for learn_bpe.lua have their equivalent for tokenize.lua without the prefix <code>tok_</code>
BPE_INFERENCE_OPTIONS for tokenize.lua are those of Tokenizer options with the prefix <code>bpe_</code></p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>When applying BPE for any data set, the same TOK_OPTIONS should be used for learn_bpe.lua and tokenize.lua</p>
</div>
<p><strong>2. Add BPE_TRAINING_OPTION for different modes of handling prefixes and/or suffixes: <code>-bpe_mode</code></strong></p>
<ul>
<li><code>suffix</code>: BPE merge operations are learnt to distinguish sub-tokens like "ent" in the middle of a word and "ent&lt;\w&gt;" at the end of a word. "&lt;\w&gt;" is an artificial marker appended to the end of each token input and treated as a single unit before doing statistics on bigrams. This is the default mode which is useful for most of the languages.</li>
<li><code>prefix</code>: BPE merge operations are learnt to distinguish sub-tokens like "ent" in the middle of a word and "&lt;w>ent" at the beginning of a word. "&lt;w>" is an artificial marker appended to the beginning of each token input and treated as a single unit before doing statistics on bigrams.</li>
<li><code>both</code>: <code>suffix</code> + <code>prefix</code></li>
<li><code>none</code>: No artificial marker is appended to input tokens, a sub-token is treated equally whether it is in the middle or at the beginning or at the end of a token.</li>
</ul>
<p><strong>3. Add BPE_INFERENCE_OPTION for BPE in addition to the case feature: <code>-bpe_case_insensitive</code></strong></p>
<p>OpenNMT's tokenization flow first applies BPE then add the case feature for each input token. With the standard BPE, "Constitution" and "constitution" may result in the different sequences of sub-tokens:</p>
<div class="codehilite"><pre><span></span>Constitution --&gt; con￨C sti￨l tu￨l tion￨l
constitution --&gt; consti￨l tu￨l tion￨l
</pre></div>


<p>If you want a <em>caseless</em> split so that you can take the best from using case feature, and you can achieve that with the following command lines:</p>
<div class="codehilite"><pre><span></span><span class="c1"># We don&#39;t need BPE to care about case</span>
tools/learn_bpe.lua -size <span class="m">30000</span> -save_bpe codes_lc -tok_case_feature <span class="o">[</span> OTHER_TOK_OPTIONS <span class="o">]</span> <span class="o">[</span> OTHER_BPE_TRAINING_OPTIONS <span class="o">]</span> &lt; input_raw

<span class="c1"># The case information is preserved in the true case input</span>
tools/tokenize.lua -bpe_model codes_lc -bpe_case_insensitive -case_feature <span class="o">[</span> SAME_TOK_OPTIONS <span class="o">]</span> <span class="o">[</span> OTHER_BPE_INFERENCE_OPTIONS <span class="o">]</span> &lt; input_raw
</pre></div>


<p>The output of the previous example would be:</p>
<div class="codehilite"><pre><span></span>Constitution --&gt; con￨C sti￨l tu￨l tion￨l
constitution --&gt; con￨l sti￨l tu￨l tion￨l
</pre></div>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../scorer/" class="btn btn-neutral float-right" title="Scorer">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../../translation/unknowns/" class="btn btn-neutral" title="Unknown words"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>
    
  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../../translation/unknowns/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../scorer/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script src="../../js/theme.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_HTMLorMML"></script>

</body>
</html>
