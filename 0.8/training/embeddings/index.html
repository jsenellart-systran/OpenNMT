<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../../img/favicon.ico">
  <title>Embeddings - OpenNMT</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../../css/highlight.css">
  <link href="../../css/extra.css" rel="stylesheet">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Embeddings";
    var mkdocs_page_input_path = "training/embeddings.md";
    var mkdocs_page_url = "/training/embeddings/";
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js"></script>
  <script src="../../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../../js/highlight.pack.js"></script> 
  
  <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-89222039-1', 'opennmt.net');
      ga('send', 'pageview');
  </script>
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> OpenNMT</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="../..">Overview</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../installation/">Installation</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../quickstart/">Quickstart</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../applications/">Applications</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Data</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../data/preparation/">Preparation</a>
                </li>
                <li class="">
                    
    <a class="" href="../../data/word_features/">Word features</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Training</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../models/">Models</a>
                </li>
                <li class=" current">
                    
    <a class="current" href="./">Embeddings</a>
    <ul class="subnav">
            
    <li class="toctree-l3"><a href="#pretrained">Pretrained</a></li>
    

    <li class="toctree-l3"><a href="#fixed">Fixed</a></li>
    

    <li class="toctree-l3"><a href="#extraction">Extraction</a></li>
    

    </ul>
                </li>
                <li class="">
                    
    <a class="" href="../logs/">Logs</a>
                </li>
                <li class="">
                    
    <a class="" href="../multi_gpu/">Multi GPU</a>
                </li>
                <li class="">
                    
    <a class="" href="../retraining/">Retraining</a>
                </li>
                <li class="">
                    
    <a class="" href="../regularization/">Regularization</a>
                </li>
                <li class="">
                    
    <a class="" href="../decay/">Decay strategies</a>
                </li>
                <li class="">
                    
    <a class="" href="../sampling/">Data sampling</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Translation</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../translation/inference/">Inference</a>
                </li>
                <li class="">
                    
    <a class="" href="../../translation/beam_search/">Beam search</a>
                </li>
                <li class="">
                    
    <a class="" href="../../translation/unknowns/">Unknown words</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Tools</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../tools/tokenization/">Tokenization</a>
                </li>
                <li class="">
                    
    <a class="" href="../../tools/scorer/">Scorer</a>
                </li>
                <li class="">
                    
    <a class="" href="../../tools/servers/">Servers</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Reference: Options</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../../options/usage/">Scripts usage</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/preprocess/">preprocess.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/train/">train.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/translate/">translate.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/tag/">tag.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/lm/">lm.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/build_vocab/">tools/build_vocab.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/release_model/">tools/release_model.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/tokenize/">tools/tokenize.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/learn_bpe/">tools/learn_bpe.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/server/">tools/translation_server.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/rest_server/">tools/rest_translation_server.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/embeddings/">tools/embeddings.lua</a>
                </li>
                <li class="">
                    
    <a class="" href="../../options/average_models/">tools/average_models.lua</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../lua_python_comparison/">Lua and Python OpenNMT</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../references/">References</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../issues/">Common issues</a>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">OpenNMT</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
        
          <li>Training &raquo;</li>
        
      
    
    <li>Embeddings</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/OpenNMT/OpenNMT/edit/master/docs/training/embeddings.md"> Edit on OpenNMT/OpenNMT</a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <p>Word embeddings are learned using a lookup table. Each word is assigned to a random vector within this table that is simply updated with the gradients coming from the network.</p>
<h2 id="pretrained">Pretrained<a class="headerlink" href="#pretrained" title="Permanent link">&para;</a></h2>
<p>When training with small amounts of data, performance can be improved by starting with pretrained embeddings. The arguments <code>-pre_word_vecs_dec</code> and <code>-pre_word_vecs_enc</code> can be used to specify these files.</p>
<p>The pretrained embeddings must be manually constructed Torch serialized tensors that correspond to the source and target dictionary files. For example:</p>
<div class="codehilite"><pre><span></span><span class="kd">local</span> <span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">50004</span>
<span class="kd">local</span> <span class="n">embedding_size</span> <span class="o">=</span> <span class="mi">500</span>

<span class="kd">local</span> <span class="n">embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">):</span><span class="n">uniform</span><span class="p">()</span>

<span class="n">torch</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;enc_embeddings.t7&#39;</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">)</span>
</pre></div>


<p>where <code>embeddings[i]</code> is the embedding of the (i)-th word in the vocabulary.</p>
<p>To automate this process, OpenNMT provides a script <code>tools/embeddings.lua</code> than can download pretrained embeddings from <a href="https://pypi.python.org/pypi/polyglot">Polyglot</a> or convert trained embeddings from <a href="https://github.com/dav/word2vec">word2vec</a>, <a href="https://nlp.stanford.edu/projects/glove/">GloVe</a> or <a href="https://github.com/facebookresearch/fastText">FastText</a> with regard to the word vocabularies generated by <code>preprocess.lua</code>. Supported format are:</p>
<ul>
<li><code>word2vec-bin</code> (default): binary format generated by word2vec.</li>
<li><code>word2vec-txt</code>: textual word2vec format - starts with header line containing number of words and embedding size, and is then followed by one line per embedding: the first token is the word, and following fields are the embeddings values.</li>
<li><code>glove</code>: text format - same format than <code>word2vec-txt</code> but without header line.</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The script requires the <code>lua-zlib</code> package.</p>
</div>
<p>For example, to generate pretrained English words embeddings:</p>
<div class="codehilite"><pre><span></span>th tools/embeddings.lua -lang en -dict_file data/demo.src.dict -save_data data/demo-src-emb
</pre></div>


<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Languages codes are Polygot's <a href="https://sites.google.com/site/rmyeid/projects/polyglot">Wikipedia Language Codes</a>.</p>
</div>
<p>Or to map pretrained <em>word2vec</em> vectors to the built vocabulary:</p>
<div class="codehilite"><pre><span></span>th tools/embeddings.lua -embed_type word2vec -embed_file data/GoogleNews-vectors-negative300.bin -dict_file data/demo.src.dict<span class="se">\</span>
                        -save_data data/demo-src-emb
</pre></div>


<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If vocabs as-is are not found in the embeddings file, you can use <code>-approximate</code> option to also look for uppercase variants and variants without possible joiner marks. You can dump the non found vocabs by setting <code>-save_unknown_dict</code> parameter.</p>
</div>
<h2 id="fixed">Fixed<a class="headerlink" href="#fixed" title="Permanent link">&para;</a></h2>
<p>By default these embeddings will be updated during training, but they can be held fixed using <code>-fix_word_vecs_enc</code> and <code>-fix_word_vecs_dec</code> options. These options can be enabled or disabled during a retraining.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>When using pretrained word embeddings, if you declare a larger <code>-word_vec_size</code> then the difference is uniformally initalized and you can use <code>-fix_word_vecs_enc pretrained</code> (or <code>-fix_word_vecs_dec pretrained</code>) to fix the pretrained part and optimize the remaining part.</p>
</div>
<h2 id="extraction">Extraction<a class="headerlink" href="#extraction" title="Permanent link">&para;</a></h2>
<p>The <code>tools/extract_embeddings.lua</code> script can be used to extract the model word embeddings into text files. They can then be easily transformed into another format for visualization or processing.</p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../logs/" class="btn btn-neutral float-right" title="Logs">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../models/" class="btn btn-neutral" title="Models"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>
    
  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../models/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../logs/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script src="../../js/theme.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_HTMLorMML"></script>

</body>
</html>
